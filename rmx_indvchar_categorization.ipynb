{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rmx_indvchar_categorization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vuchicago/Keras/blob/master/rmx_indvchar_categorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hCjDgcz3eSil",
        "colab_type": "code",
        "outputId": "00ba9429-383a-4bab-b367-dd5073b86858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') ##Mount Drive\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "df=pd.read_csv(\"rmxproducts.csv\")\n",
        "df=df.dropna()\n",
        "df = shuffle(df)\n",
        "train_size=np.int(df.shape[0]//(10/9))  ###Training size is 9/10 of dataset\n",
        "print(\"dset size =\",df.shape[0], \";training size = \",train_size, \"; test size= \",df.shape[0]-train_size)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "dset size = 5338 ;training size =  4804 ; test size=  534\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X9PbyZxGjDxq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "##Features is a series of descriptions \n",
        "features=list(df[\"Features\"])\n",
        "labels=df[\"Labels\"]\n",
        "\n",
        "cv=features[:train_size]\n",
        "test=features[train_size:]\n",
        "labels_cv=labels[:train_size]\n",
        "labels_test=labels[train_size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sc4gYEthZIRT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k9pr0hdCohO7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Split each sentence into words in a list\n",
        "word_dic=[]\n",
        "for lines in features:\n",
        "  for words in list(lines):\n",
        "    word_dic.append(words.replace(\" \",\"\")) #gets rid of all the spaces\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jrEJRpB-1eYk",
        "colab_type": "code",
        "outputId": "3e21c963-13f2-41ec-d57c-920640f7286e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "word_dic_unq=pd.Series(word_dic).unique() #get only the unique words in the list\n",
        "len(word_dic_unq)\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "xV_S9kGVZ--t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "####CREATES A DICTIONARY OF ALL THE UNIQUE words\n",
        "word2index={}\n",
        "index2word={}\n",
        "for index, word in enumerate(word_dic_unq):\n",
        "  word2index[word]=index\n",
        "  index2word[index]=word\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h2dRKE9uoRmD",
        "colab_type": "code",
        "outputId": "476d30e7-3a6a-4b38-8bc2-603a7da73d62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "##Converts each line to its character and sums up the amount of characters in the dictionary a line has if it has multiple\n",
        "def conv2inputs(dataset):\n",
        "  zeros_matrix=np.zeros([len(dataset),len(word2index)])\n",
        "  for num,lines in enumerate(dataset):\n",
        "    for words in list(lines):\n",
        "      indicator=word2index[words.replace(\" \",\"\")] #convert words to index number in dictionary above\n",
        "      zeros_matrix[num,indicator]+=1\n",
        "  return zeros_matrix\n",
        "dset=conv2inputs(features)   \n",
        "\n",
        "dset_cv=conv2inputs(cv)\n",
        "dset_test=conv2inputs(test)\n",
        "\n",
        "\n",
        "print(dset)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 2.  8.  2. ...  0.  0.  0.]\n",
            " [ 1. 13.  1. ...  0.  0.  0.]\n",
            " [ 0.  4.  0. ...  0.  0.  0.]\n",
            " ...\n",
            " [ 1.  6.  1. ...  0.  0.  0.]\n",
            " [ 1.  4.  1. ...  0.  0.  0.]\n",
            " [ 0.  3.  3. ...  0.  0.  0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rUw5xAsJvoQL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7138dbdd-c5d6-4d43-b772-ec32e92f2525"
      },
      "cell_type": "code",
      "source": [
        "###CREATE ONE HOT VECTOR FOR LABELS\n",
        "from keras.utils import to_categorical\n",
        "#labels_array=np.array(labels)\n",
        "#one_hot_labels = to_categorical(labels_array)\n",
        "\n",
        "label_dic_unq=pd.Series(labels).unique() #CREATE A DICTIONARY OF UNIQUE LABELS\n",
        "\n",
        "label2index={}\n",
        "index2label={}\n",
        "for index, word in enumerate(label_dic_unq):\n",
        "  label2index[word]=index\n",
        "  index2label[index]=word\n",
        "\n",
        "def label2inputs(dataset):\n",
        "  one_hot_labels=np.zeros([dataset.shape[0],len(label2index)])\n",
        "\n",
        "  for num,label in enumerate(dataset):\n",
        "    indicator=label2index[label]\n",
        "    one_hot_labels[num,indicator]=1\n",
        "  return one_hot_labels\n",
        "\n",
        "one_hot_labels_cv=label2inputs(labels_cv)\n",
        "one_hot_labels_test=label2inputs(labels_test)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "xWdNWkE4yWSE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9c2a083d-19e7-4f90-cca0-11a96c450dec"
      },
      "cell_type": "code",
      "source": [
        "from keras import Sequential,models\n",
        "from keras.layers import Dropout,Dense, Embedding\n",
        "from keras import optimizers, regularizers\n",
        "sgd=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "def keras_model():\n",
        "  model=models.Sequential()\n",
        "  model.add(Dense(256,kernel_regularizer=regularizers.l2(.001),input_shape=(len(word2index),), activation=\"relu\"))\n",
        "  model.add(Dropout(.2))\n",
        "  model.add(Dense(256, activation='tanh'))\n",
        "  model.add(Dropout(.2))\n",
        "  model.add(Dense(128, kernel_regularizer=regularizers.l2(.001),activation='relu'))\n",
        "  model.add(Dense(128,activation='relu'))\n",
        "  model.add(Dropout(.2))\n",
        "  model.add(Dense(128,activation='relu'))\n",
        "  model.add(Dense(26,activation=\"softmax\"))\n",
        "  model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yfFFmB556ryd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ciSWmpijz3j0",
        "colab_type": "code",
        "outputId": "efe448a5-9464-4d87-ab19-bf0e27805d16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105454
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import Sequential,models\n",
        "from keras.layers import Dropout,Dense\n",
        "from keras import optimizers, regularizers\n",
        "sgd=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fold=100\n",
        "epoch=30\n",
        "sample_size=len(dset_cv)//fold\n",
        "train_history=[]\n",
        "val_history=[]\n",
        "train_history_loss=[]\n",
        "val_history_loss=[]\n",
        "\n",
        "for i in range(fold):\n",
        "  print('processing fold #',i)\n",
        "  X_val=dset_cv[i*sample_size: (i+1)*sample_size]\n",
        "  y_val=one_hot_labels_cv[i*sample_size: (i+1)*sample_size]\n",
        "  \n",
        "  X_train=np.concatenate([dset_cv[:i*sample_size],dset_cv[(i+1)*sample_size:]],axis=0)\n",
        "  y_train=np.concatenate([one_hot_labels_cv[:i*sample_size],one_hot_labels_cv[(i+1)*sample_size:]],axis=0)\n",
        "  \n",
        "  history=keras_model().fit(X_train,y_train,epochs=epoch,batch_size=512,validation_data=[X_val,y_val])\n",
        "  \n",
        "  ###get the losses and accuracy for the train & validation for each epoch\n",
        "  train_acc=history.history[\"acc\"]\n",
        "  val_acc=history.history['val_acc']\n",
        "\n",
        "  train_loss=history.history['loss']\n",
        "  val_loss=history.history['val_loss']\n",
        "  \n",
        "  train_history.append(train_acc)\n",
        "  val_history.append(val_acc)\n",
        "  train_history_loss.append(train_loss)\n",
        "  val_history_loss.append(val_loss)\n",
        "\n",
        "#from sklearn.model_selection import KFold\n",
        "#kf = KFold(n_splits=5)\n",
        "#KFold(n_splits=5, random_state=None, shuffle=False)\n",
        "#accuracy=[]\n",
        "#for train_index, test_index in kf.split(dset_cv):\n",
        "  #X_train, X_val = dset_cv[train_index], dset_cv[test_index]\n",
        "  #y_train, y_val = one_hot_labels_cv[train_index], one_hot_labels_cv[test_index]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "  model=models.Sequential()\n",
        "  model.add(Dense(512,kernel_regularizer=regularizers.l2(.001), activation=\"relu\",input_shape=(len(word2index),)))\n",
        "  model.add(Dropout(.2))\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dropout(.1))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(128,kernel_regularizer=regularizers.l2(.001),activation='relu'))\n",
        "  model.add(Dense(26,activation=\"softmax\"))\n",
        "\n",
        "  model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])'''\n",
        "\n",
        "  \n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 4s 763us/step - loss: 3.0612 - acc: 0.1941 - val_loss: 2.8468 - val_acc: 0.2292\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.6892 - acc: 0.2813 - val_loss: 2.5660 - val_acc: 0.3333\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 2.4418 - acc: 0.3509 - val_loss: 2.3605 - val_acc: 0.2917\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 2.2588 - acc: 0.3993 - val_loss: 2.1938 - val_acc: 0.2917\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.1106 - acc: 0.4336 - val_loss: 2.1235 - val_acc: 0.3750\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.9951 - acc: 0.4575 - val_loss: 1.9990 - val_acc: 0.4792\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.9007 - acc: 0.4897 - val_loss: 1.8890 - val_acc: 0.4792\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.7990 - acc: 0.5191 - val_loss: 1.8241 - val_acc: 0.5208\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.7231 - acc: 0.5427 - val_loss: 1.8322 - val_acc: 0.5208\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.6473 - acc: 0.5610 - val_loss: 1.7552 - val_acc: 0.5625\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.5707 - acc: 0.5736 - val_loss: 1.6845 - val_acc: 0.5417\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.5205 - acc: 0.5988 - val_loss: 1.6786 - val_acc: 0.5208\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 17us/step - loss: 1.4468 - acc: 0.6171 - val_loss: 1.6367 - val_acc: 0.5625\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.4090 - acc: 0.6198 - val_loss: 1.6427 - val_acc: 0.5417\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.3564 - acc: 0.6358 - val_loss: 1.5717 - val_acc: 0.6042\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.3368 - acc: 0.6341 - val_loss: 1.5559 - val_acc: 0.5833\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.2874 - acc: 0.6569 - val_loss: 1.4866 - val_acc: 0.6042\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.2375 - acc: 0.6709 - val_loss: 1.4384 - val_acc: 0.6458\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.2001 - acc: 0.6884 - val_loss: 1.4317 - val_acc: 0.6458\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.1678 - acc: 0.6909 - val_loss: 1.4154 - val_acc: 0.6458\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 17us/step - loss: 1.1283 - acc: 0.7061 - val_loss: 1.3433 - val_acc: 0.6667\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.1212 - acc: 0.7025 - val_loss: 1.3687 - val_acc: 0.6667\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.0717 - acc: 0.7204 - val_loss: 1.3775 - val_acc: 0.6458\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0551 - acc: 0.7258 - val_loss: 1.4007 - val_acc: 0.6667\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.0384 - acc: 0.7355 - val_loss: 1.4075 - val_acc: 0.6458\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0169 - acc: 0.7380 - val_loss: 1.3749 - val_acc: 0.6458\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9937 - acc: 0.7424 - val_loss: 1.3266 - val_acc: 0.6875\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9999 - acc: 0.7420 - val_loss: 1.2787 - val_acc: 0.6875\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9581 - acc: 0.7559 - val_loss: 1.3217 - val_acc: 0.6875\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 17us/step - loss: 0.9425 - acc: 0.7666 - val_loss: 1.2400 - val_acc: 0.6875\n",
            "processing fold # 1\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 4s 803us/step - loss: 2.8712 - acc: 0.2653 - val_loss: 2.2976 - val_acc: 0.4167\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.3922 - acc: 0.3610 - val_loss: 1.9947 - val_acc: 0.5417\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 2.1544 - acc: 0.4115 - val_loss: 1.8472 - val_acc: 0.5000\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.9461 - acc: 0.4731 - val_loss: 1.6531 - val_acc: 0.5625\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.7719 - acc: 0.5168 - val_loss: 1.5567 - val_acc: 0.6042\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.6569 - acc: 0.5431 - val_loss: 1.4407 - val_acc: 0.5833\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.5463 - acc: 0.5849 - val_loss: 1.3074 - val_acc: 0.6667\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.4385 - acc: 0.6152 - val_loss: 1.2317 - val_acc: 0.6667\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.3780 - acc: 0.6320 - val_loss: 1.1498 - val_acc: 0.7292\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.3066 - acc: 0.6516 - val_loss: 1.0998 - val_acc: 0.7292\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2570 - acc: 0.6606 - val_loss: 1.0263 - val_acc: 0.7917\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.2011 - acc: 0.6812 - val_loss: 1.0001 - val_acc: 0.7708\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.1433 - acc: 0.6960 - val_loss: 0.9440 - val_acc: 0.7708\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.1047 - acc: 0.7086 - val_loss: 0.9381 - val_acc: 0.8125\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0729 - acc: 0.7206 - val_loss: 0.9161 - val_acc: 0.7917\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.0728 - acc: 0.7111 - val_loss: 0.8359 - val_acc: 0.8333\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0119 - acc: 0.7347 - val_loss: 0.8899 - val_acc: 0.8125\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.0016 - acc: 0.7340 - val_loss: 0.8399 - val_acc: 0.8125\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 17us/step - loss: 0.9714 - acc: 0.7414 - val_loss: 0.8337 - val_acc: 0.8125\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9348 - acc: 0.7538 - val_loss: 0.8769 - val_acc: 0.8125\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9296 - acc: 0.7641 - val_loss: 0.8036 - val_acc: 0.8125\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9094 - acc: 0.7653 - val_loss: 0.8065 - val_acc: 0.8333\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8820 - acc: 0.7729 - val_loss: 0.7678 - val_acc: 0.8542\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8695 - acc: 0.7668 - val_loss: 0.8516 - val_acc: 0.8333\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8625 - acc: 0.7773 - val_loss: 0.7423 - val_acc: 0.8333\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8418 - acc: 0.7813 - val_loss: 0.7702 - val_acc: 0.8333\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8197 - acc: 0.7805 - val_loss: 0.7954 - val_acc: 0.8542\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7913 - acc: 0.7937 - val_loss: 0.7886 - val_acc: 0.8125\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7907 - acc: 0.7954 - val_loss: 0.7363 - val_acc: 0.8333\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7748 - acc: 0.7952 - val_loss: 0.7688 - val_acc: 0.8125\n",
            "processing fold # 2\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 4s 848us/step - loss: 2.8733 - acc: 0.2359 - val_loss: 2.5081 - val_acc: 0.2500\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 2.3514 - acc: 0.3705 - val_loss: 2.0143 - val_acc: 0.3750\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 2.0495 - acc: 0.4413 - val_loss: 1.7768 - val_acc: 0.5417\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.8562 - acc: 0.4983 - val_loss: 1.5623 - val_acc: 0.5833\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.6866 - acc: 0.5376 - val_loss: 1.4623 - val_acc: 0.6250\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.5615 - acc: 0.5751 - val_loss: 1.3310 - val_acc: 0.6250\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4580 - acc: 0.6016 - val_loss: 1.2065 - val_acc: 0.6667\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.3767 - acc: 0.6312 - val_loss: 1.0956 - val_acc: 0.7083\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.3092 - acc: 0.6476 - val_loss: 1.1005 - val_acc: 0.6875\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.2286 - acc: 0.6709 - val_loss: 0.9904 - val_acc: 0.7500\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.1808 - acc: 0.6812 - val_loss: 0.9769 - val_acc: 0.7292\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.1313 - acc: 0.6939 - val_loss: 0.9318 - val_acc: 0.7500\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.0977 - acc: 0.7012 - val_loss: 0.9067 - val_acc: 0.7708\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0428 - acc: 0.7204 - val_loss: 0.8977 - val_acc: 0.7917\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.0158 - acc: 0.7349 - val_loss: 0.7718 - val_acc: 0.7500\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9825 - acc: 0.7464 - val_loss: 0.8418 - val_acc: 0.7917\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9686 - acc: 0.7506 - val_loss: 0.7596 - val_acc: 0.8125\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9221 - acc: 0.7601 - val_loss: 0.8439 - val_acc: 0.7917\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9142 - acc: 0.7571 - val_loss: 0.6754 - val_acc: 0.8542\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8894 - acc: 0.7677 - val_loss: 0.7363 - val_acc: 0.8333\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8645 - acc: 0.7830 - val_loss: 0.7212 - val_acc: 0.8542\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8370 - acc: 0.7820 - val_loss: 0.5976 - val_acc: 0.8542\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8166 - acc: 0.7918 - val_loss: 0.7033 - val_acc: 0.8542\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8031 - acc: 0.7918 - val_loss: 0.6870 - val_acc: 0.8542\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7933 - acc: 0.7950 - val_loss: 0.6707 - val_acc: 0.8958\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7841 - acc: 0.7939 - val_loss: 0.6705 - val_acc: 0.8542\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7593 - acc: 0.7971 - val_loss: 0.6235 - val_acc: 0.8333\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7420 - acc: 0.8099 - val_loss: 0.6793 - val_acc: 0.8542\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7314 - acc: 0.8074 - val_loss: 0.6283 - val_acc: 0.8542\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7134 - acc: 0.8118 - val_loss: 0.6663 - val_acc: 0.8542\n",
            "processing fold # 3\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 4s 914us/step - loss: 3.0148 - acc: 0.1989 - val_loss: 2.5292 - val_acc: 0.3125\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.4380 - acc: 0.3297 - val_loss: 2.1828 - val_acc: 0.3958\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 2.1644 - acc: 0.4037 - val_loss: 1.9780 - val_acc: 0.5208\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.9683 - acc: 0.4514 - val_loss: 1.7276 - val_acc: 0.5417\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.7837 - acc: 0.5042 - val_loss: 1.4858 - val_acc: 0.6250\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.6399 - acc: 0.5454 - val_loss: 1.4258 - val_acc: 0.5625\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.5223 - acc: 0.5820 - val_loss: 1.3764 - val_acc: 0.6250\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.4326 - acc: 0.6119 - val_loss: 1.2605 - val_acc: 0.6250\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 17us/step - loss: 1.3393 - acc: 0.6386 - val_loss: 1.2348 - val_acc: 0.6875\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.2880 - acc: 0.6484 - val_loss: 1.2082 - val_acc: 0.7292\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2315 - acc: 0.6636 - val_loss: 1.1461 - val_acc: 0.7292\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.1660 - acc: 0.6854 - val_loss: 1.1398 - val_acc: 0.7292\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.1168 - acc: 0.6991 - val_loss: 1.2310 - val_acc: 0.7292\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.0858 - acc: 0.7088 - val_loss: 1.2252 - val_acc: 0.7083\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0477 - acc: 0.7161 - val_loss: 1.1946 - val_acc: 0.7083\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0114 - acc: 0.7359 - val_loss: 1.1134 - val_acc: 0.8333\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.0033 - acc: 0.7300 - val_loss: 1.1315 - val_acc: 0.7083\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9755 - acc: 0.7351 - val_loss: 1.0927 - val_acc: 0.7292\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9460 - acc: 0.7500 - val_loss: 1.1528 - val_acc: 0.7500\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9054 - acc: 0.7664 - val_loss: 1.0820 - val_acc: 0.7708\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8779 - acc: 0.7712 - val_loss: 1.0655 - val_acc: 0.7708\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8558 - acc: 0.7794 - val_loss: 1.0694 - val_acc: 0.7292\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8312 - acc: 0.7845 - val_loss: 1.0330 - val_acc: 0.7708\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8243 - acc: 0.7864 - val_loss: 1.0712 - val_acc: 0.7500\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8087 - acc: 0.7845 - val_loss: 1.0363 - val_acc: 0.7708\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7958 - acc: 0.7912 - val_loss: 1.0308 - val_acc: 0.7708\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 17us/step - loss: 0.7728 - acc: 0.7967 - val_loss: 1.0603 - val_acc: 0.7917\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7603 - acc: 0.8015 - val_loss: 1.0362 - val_acc: 0.7917\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7545 - acc: 0.8017 - val_loss: 1.0863 - val_acc: 0.7500\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7410 - acc: 0.8055 - val_loss: 1.1124 - val_acc: 0.7500\n",
            "processing fold # 4\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 4s 924us/step - loss: 3.0593 - acc: 0.1831 - val_loss: 2.7519 - val_acc: 0.2917\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.5197 - acc: 0.3198 - val_loss: 2.4232 - val_acc: 0.3542\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.2389 - acc: 0.3900 - val_loss: 2.2172 - val_acc: 0.3125\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.0114 - acc: 0.4474 - val_loss: 1.9900 - val_acc: 0.4583\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.8157 - acc: 0.5004 - val_loss: 1.7116 - val_acc: 0.5625\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.6645 - acc: 0.5454 - val_loss: 1.6634 - val_acc: 0.5625\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.5721 - acc: 0.5711 - val_loss: 1.5338 - val_acc: 0.6042\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.4608 - acc: 0.6001 - val_loss: 1.5363 - val_acc: 0.6250\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.3642 - acc: 0.6327 - val_loss: 1.4671 - val_acc: 0.6250\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.3055 - acc: 0.6497 - val_loss: 1.1872 - val_acc: 0.7500\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2480 - acc: 0.6606 - val_loss: 1.1892 - val_acc: 0.7500\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1864 - acc: 0.6861 - val_loss: 1.0456 - val_acc: 0.7083\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1417 - acc: 0.6964 - val_loss: 1.0808 - val_acc: 0.6875\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.0912 - acc: 0.7098 - val_loss: 1.0100 - val_acc: 0.7292\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0584 - acc: 0.7183 - val_loss: 0.9387 - val_acc: 0.7708\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.0192 - acc: 0.7323 - val_loss: 0.8530 - val_acc: 0.8542\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9845 - acc: 0.7414 - val_loss: 0.7582 - val_acc: 0.8958\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9690 - acc: 0.7431 - val_loss: 0.7137 - val_acc: 0.8750\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9310 - acc: 0.7546 - val_loss: 0.7458 - val_acc: 0.8542\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8972 - acc: 0.7567 - val_loss: 0.7364 - val_acc: 0.8542\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8816 - acc: 0.7693 - val_loss: 0.7312 - val_acc: 0.8333\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8671 - acc: 0.7780 - val_loss: 0.7272 - val_acc: 0.8750\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8305 - acc: 0.7870 - val_loss: 0.7735 - val_acc: 0.8125\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8361 - acc: 0.7792 - val_loss: 0.7202 - val_acc: 0.8750\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8375 - acc: 0.7853 - val_loss: 0.7083 - val_acc: 0.8958\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8040 - acc: 0.7977 - val_loss: 0.7223 - val_acc: 0.8750\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7693 - acc: 0.7984 - val_loss: 0.6778 - val_acc: 0.8750\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7352 - acc: 0.8103 - val_loss: 0.6892 - val_acc: 0.8542\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7463 - acc: 0.8080 - val_loss: 0.6990 - val_acc: 0.8750\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7446 - acc: 0.8080 - val_loss: 0.7492 - val_acc: 0.8333\n",
            "processing fold # 5\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 5s 969us/step - loss: 2.9873 - acc: 0.1808 - val_loss: 2.7522 - val_acc: 0.2500\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.4190 - acc: 0.3331 - val_loss: 2.4351 - val_acc: 0.2708\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 2.1545 - acc: 0.4113 - val_loss: 1.9812 - val_acc: 0.4375\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.9299 - acc: 0.4651 - val_loss: 1.6071 - val_acc: 0.5417\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.7482 - acc: 0.5196 - val_loss: 1.5465 - val_acc: 0.5417\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.5872 - acc: 0.5641 - val_loss: 1.3063 - val_acc: 0.6667\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.4469 - acc: 0.6064 - val_loss: 1.2624 - val_acc: 0.6458\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.3545 - acc: 0.6358 - val_loss: 1.2581 - val_acc: 0.6667\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2716 - acc: 0.6535 - val_loss: 1.0822 - val_acc: 0.7292\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.2150 - acc: 0.6709 - val_loss: 1.1172 - val_acc: 0.7083\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.1709 - acc: 0.6888 - val_loss: 1.0824 - val_acc: 0.7708\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.1188 - acc: 0.6960 - val_loss: 1.0031 - val_acc: 0.8542\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0729 - acc: 0.7210 - val_loss: 0.9305 - val_acc: 0.8333\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0375 - acc: 0.7210 - val_loss: 0.9419 - val_acc: 0.8333\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.0008 - acc: 0.7330 - val_loss: 0.9413 - val_acc: 0.7917\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9495 - acc: 0.7542 - val_loss: 0.8866 - val_acc: 0.8125\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9289 - acc: 0.7525 - val_loss: 0.9478 - val_acc: 0.7917\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9255 - acc: 0.7620 - val_loss: 0.9480 - val_acc: 0.8542\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8773 - acc: 0.7740 - val_loss: 0.8728 - val_acc: 0.8125\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8581 - acc: 0.7744 - val_loss: 0.9097 - val_acc: 0.8125\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8529 - acc: 0.7796 - val_loss: 0.9578 - val_acc: 0.7708\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8176 - acc: 0.7794 - val_loss: 1.0011 - val_acc: 0.7917\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7883 - acc: 0.7923 - val_loss: 1.0459 - val_acc: 0.7917\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7963 - acc: 0.7933 - val_loss: 0.9784 - val_acc: 0.8125\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7488 - acc: 0.8063 - val_loss: 0.9273 - val_acc: 0.8125\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7487 - acc: 0.8019 - val_loss: 0.9725 - val_acc: 0.8125\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7276 - acc: 0.8089 - val_loss: 0.9299 - val_acc: 0.8125\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7272 - acc: 0.8078 - val_loss: 1.0358 - val_acc: 0.7917\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7196 - acc: 0.8099 - val_loss: 1.0056 - val_acc: 0.8125\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7155 - acc: 0.8150 - val_loss: 0.9443 - val_acc: 0.8125\n",
            "processing fold # 6\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 5s 1ms/step - loss: 2.9263 - acc: 0.2134 - val_loss: 2.4100 - val_acc: 0.3125\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 2.3677 - acc: 0.3488 - val_loss: 2.2698 - val_acc: 0.3333\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 2.1269 - acc: 0.4016 - val_loss: 2.2587 - val_acc: 0.3125\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.8739 - acc: 0.4664 - val_loss: 2.1327 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.7111 - acc: 0.5168 - val_loss: 1.8917 - val_acc: 0.5000\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.5652 - acc: 0.5614 - val_loss: 1.8518 - val_acc: 0.5625\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.4654 - acc: 0.5879 - val_loss: 1.7387 - val_acc: 0.6250\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.3746 - acc: 0.6207 - val_loss: 1.6989 - val_acc: 0.6042\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.2964 - acc: 0.6440 - val_loss: 1.7084 - val_acc: 0.6042\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.2265 - acc: 0.6707 - val_loss: 1.6046 - val_acc: 0.5833\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.1787 - acc: 0.6810 - val_loss: 1.6037 - val_acc: 0.6042\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.1136 - acc: 0.7012 - val_loss: 1.5249 - val_acc: 0.6875\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0636 - acc: 0.7151 - val_loss: 1.5909 - val_acc: 0.6458\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.0517 - acc: 0.7183 - val_loss: 1.4776 - val_acc: 0.6667\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0017 - acc: 0.7353 - val_loss: 1.5578 - val_acc: 0.6667\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9688 - acc: 0.7376 - val_loss: 1.4525 - val_acc: 0.6667\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9365 - acc: 0.7494 - val_loss: 1.5589 - val_acc: 0.7083\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9031 - acc: 0.7637 - val_loss: 1.4682 - val_acc: 0.7292\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8941 - acc: 0.7605 - val_loss: 1.5002 - val_acc: 0.6875\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8637 - acc: 0.7729 - val_loss: 1.5089 - val_acc: 0.6875\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8418 - acc: 0.7792 - val_loss: 1.5239 - val_acc: 0.7083\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8317 - acc: 0.7885 - val_loss: 1.5607 - val_acc: 0.6250\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8113 - acc: 0.7925 - val_loss: 1.5060 - val_acc: 0.6458\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8016 - acc: 0.7954 - val_loss: 1.4592 - val_acc: 0.6458\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7874 - acc: 0.7965 - val_loss: 1.4974 - val_acc: 0.6875\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7597 - acc: 0.8011 - val_loss: 1.4988 - val_acc: 0.7083\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7444 - acc: 0.8097 - val_loss: 1.5024 - val_acc: 0.7292\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7288 - acc: 0.8114 - val_loss: 1.4987 - val_acc: 0.7083\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7181 - acc: 0.8167 - val_loss: 1.4350 - val_acc: 0.6875\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.6998 - acc: 0.8253 - val_loss: 1.4977 - val_acc: 0.6875\n",
            "processing fold # 7\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 5s 1ms/step - loss: 2.9360 - acc: 0.2208 - val_loss: 2.5397 - val_acc: 0.3125\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.3908 - acc: 0.3541 - val_loss: 2.2215 - val_acc: 0.3333\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 2.1176 - acc: 0.4119 - val_loss: 1.9390 - val_acc: 0.3333\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.9064 - acc: 0.4758 - val_loss: 1.7361 - val_acc: 0.3958\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.7442 - acc: 0.5219 - val_loss: 1.5932 - val_acc: 0.5000\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.5866 - acc: 0.5622 - val_loss: 1.5922 - val_acc: 0.5000\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.4902 - acc: 0.5765 - val_loss: 1.3341 - val_acc: 0.5833\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.3913 - acc: 0.6159 - val_loss: 1.2721 - val_acc: 0.6250\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.3083 - acc: 0.6476 - val_loss: 1.3512 - val_acc: 0.6042\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.2490 - acc: 0.6548 - val_loss: 1.1715 - val_acc: 0.6250\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.1834 - acc: 0.6823 - val_loss: 1.2569 - val_acc: 0.5833\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.1458 - acc: 0.6897 - val_loss: 1.0325 - val_acc: 0.6250\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.1066 - acc: 0.7046 - val_loss: 1.0994 - val_acc: 0.6667\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.0555 - acc: 0.7199 - val_loss: 1.0144 - val_acc: 0.7083\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.0152 - acc: 0.7277 - val_loss: 1.0053 - val_acc: 0.6458\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9976 - acc: 0.7410 - val_loss: 0.9456 - val_acc: 0.6875\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9592 - acc: 0.7481 - val_loss: 0.8410 - val_acc: 0.7500\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9196 - acc: 0.7580 - val_loss: 1.0890 - val_acc: 0.6875\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9182 - acc: 0.7557 - val_loss: 0.9723 - val_acc: 0.7500\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8922 - acc: 0.7696 - val_loss: 0.8949 - val_acc: 0.7500\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8669 - acc: 0.7761 - val_loss: 0.7973 - val_acc: 0.7917\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8467 - acc: 0.7742 - val_loss: 0.9271 - val_acc: 0.7292\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8161 - acc: 0.7895 - val_loss: 0.9162 - val_acc: 0.7292\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7970 - acc: 0.7916 - val_loss: 0.8543 - val_acc: 0.7292\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7801 - acc: 0.7956 - val_loss: 0.8486 - val_acc: 0.6875\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7612 - acc: 0.7981 - val_loss: 0.8026 - val_acc: 0.8125\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7665 - acc: 0.7977 - val_loss: 0.9242 - val_acc: 0.7708\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7370 - acc: 0.8085 - val_loss: 0.8025 - val_acc: 0.7292\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7363 - acc: 0.8051 - val_loss: 0.7431 - val_acc: 0.7917\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7138 - acc: 0.8068 - val_loss: 0.8237 - val_acc: 0.7083\n",
            "processing fold # 8\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 5s 1ms/step - loss: 2.9698 - acc: 0.1968 - val_loss: 2.4737 - val_acc: 0.3542\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 2.4277 - acc: 0.3335 - val_loss: 1.9674 - val_acc: 0.4792\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 2.1350 - acc: 0.3938 - val_loss: 1.7586 - val_acc: 0.5417\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.9090 - acc: 0.4668 - val_loss: 1.5204 - val_acc: 0.6667\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.7243 - acc: 0.5231 - val_loss: 1.3706 - val_acc: 0.6875\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.5747 - acc: 0.5576 - val_loss: 1.2894 - val_acc: 0.7292\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.4798 - acc: 0.5976 - val_loss: 1.1841 - val_acc: 0.7292\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.3920 - acc: 0.6205 - val_loss: 1.1621 - val_acc: 0.7292\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.3027 - acc: 0.6396 - val_loss: 1.0067 - val_acc: 0.7708\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.2246 - acc: 0.6730 - val_loss: 0.9536 - val_acc: 0.7708\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.1759 - acc: 0.6869 - val_loss: 0.9742 - val_acc: 0.7708\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1314 - acc: 0.6987 - val_loss: 0.8858 - val_acc: 0.8125\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0878 - acc: 0.7134 - val_loss: 0.7978 - val_acc: 0.8125\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0632 - acc: 0.7151 - val_loss: 0.8255 - val_acc: 0.7917\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0117 - acc: 0.7275 - val_loss: 0.7655 - val_acc: 0.8125\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9638 - acc: 0.7511 - val_loss: 0.7466 - val_acc: 0.8333\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9377 - acc: 0.7544 - val_loss: 0.7789 - val_acc: 0.8125\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9064 - acc: 0.7607 - val_loss: 0.7721 - val_acc: 0.8333\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9117 - acc: 0.7567 - val_loss: 0.7292 - val_acc: 0.8333\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8728 - acc: 0.7744 - val_loss: 0.7080 - val_acc: 0.8542\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8501 - acc: 0.7717 - val_loss: 0.6828 - val_acc: 0.8333\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8327 - acc: 0.7799 - val_loss: 0.6757 - val_acc: 0.8750\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8146 - acc: 0.7874 - val_loss: 0.6579 - val_acc: 0.8958\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7974 - acc: 0.7906 - val_loss: 0.6411 - val_acc: 0.8958\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7638 - acc: 0.8003 - val_loss: 0.7025 - val_acc: 0.8750\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7604 - acc: 0.8017 - val_loss: 0.6733 - val_acc: 0.8958\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7652 - acc: 0.8040 - val_loss: 0.7019 - val_acc: 0.9167\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7424 - acc: 0.8122 - val_loss: 0.6334 - val_acc: 0.9167\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7309 - acc: 0.8135 - val_loss: 0.6124 - val_acc: 0.8958\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7142 - acc: 0.8167 - val_loss: 0.6826 - val_acc: 0.8958\n",
            "processing fold # 9\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 5s 1ms/step - loss: 2.9261 - acc: 0.2103 - val_loss: 2.7371 - val_acc: 0.2917\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.3683 - acc: 0.3541 - val_loss: 2.5298 - val_acc: 0.3542\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.0723 - acc: 0.4169 - val_loss: 2.4198 - val_acc: 0.3125\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.8605 - acc: 0.4817 - val_loss: 2.3250 - val_acc: 0.3750\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.7000 - acc: 0.5341 - val_loss: 2.0502 - val_acc: 0.4167\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.5812 - acc: 0.5627 - val_loss: 2.0928 - val_acc: 0.5208\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.4753 - acc: 0.5887 - val_loss: 1.9537 - val_acc: 0.5417\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.3593 - acc: 0.6287 - val_loss: 1.8730 - val_acc: 0.5208\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.2790 - acc: 0.6594 - val_loss: 1.7300 - val_acc: 0.5833\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.1930 - acc: 0.6819 - val_loss: 1.6176 - val_acc: 0.6875\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.1474 - acc: 0.6951 - val_loss: 1.7832 - val_acc: 0.6042\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.1227 - acc: 0.7023 - val_loss: 1.6038 - val_acc: 0.6458\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.0721 - acc: 0.7174 - val_loss: 1.5468 - val_acc: 0.6875\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0138 - acc: 0.7334 - val_loss: 1.5780 - val_acc: 0.7083\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9913 - acc: 0.7378 - val_loss: 1.5138 - val_acc: 0.7083\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9571 - acc: 0.7483 - val_loss: 1.5895 - val_acc: 0.6875\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9155 - acc: 0.7620 - val_loss: 1.6255 - val_acc: 0.7083\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9075 - acc: 0.7647 - val_loss: 1.5891 - val_acc: 0.6667\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8777 - acc: 0.7660 - val_loss: 1.4664 - val_acc: 0.7083\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8596 - acc: 0.7778 - val_loss: 1.6002 - val_acc: 0.6667\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8413 - acc: 0.7782 - val_loss: 1.5362 - val_acc: 0.6667\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8005 - acc: 0.7944 - val_loss: 1.5103 - val_acc: 0.7083\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7868 - acc: 0.7971 - val_loss: 1.4740 - val_acc: 0.7083\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7851 - acc: 0.7918 - val_loss: 1.5070 - val_acc: 0.7292\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7822 - acc: 0.7946 - val_loss: 1.5546 - val_acc: 0.6667\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7639 - acc: 0.7963 - val_loss: 1.4541 - val_acc: 0.7500\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7350 - acc: 0.8171 - val_loss: 1.5624 - val_acc: 0.6875\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7116 - acc: 0.8183 - val_loss: 1.4838 - val_acc: 0.6875\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.6977 - acc: 0.8167 - val_loss: 1.3816 - val_acc: 0.7500\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.6905 - acc: 0.8206 - val_loss: 1.3470 - val_acc: 0.7500\n",
            "processing fold # 10\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 5s 1ms/step - loss: 2.9508 - acc: 0.2035 - val_loss: 2.4626 - val_acc: 0.3958\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.4304 - acc: 0.3246 - val_loss: 2.0658 - val_acc: 0.4375\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 2.1609 - acc: 0.3980 - val_loss: 1.7805 - val_acc: 0.6042\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.8891 - acc: 0.4832 - val_loss: 1.5771 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.7551 - acc: 0.5250 - val_loss: 1.5848 - val_acc: 0.4375\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5773 - acc: 0.5635 - val_loss: 1.3521 - val_acc: 0.6042\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.4651 - acc: 0.6043 - val_loss: 1.3529 - val_acc: 0.6875\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.3493 - acc: 0.6438 - val_loss: 1.3506 - val_acc: 0.6250\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.2840 - acc: 0.6571 - val_loss: 1.3592 - val_acc: 0.6458\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.2351 - acc: 0.6699 - val_loss: 1.4640 - val_acc: 0.6875\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.1618 - acc: 0.6878 - val_loss: 1.3044 - val_acc: 0.6875\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.1490 - acc: 0.6876 - val_loss: 1.3477 - val_acc: 0.6458\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0965 - acc: 0.7090 - val_loss: 1.4714 - val_acc: 0.6667\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.0279 - acc: 0.7290 - val_loss: 1.4506 - val_acc: 0.6250\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0086 - acc: 0.7353 - val_loss: 1.4468 - val_acc: 0.6667\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9564 - acc: 0.7462 - val_loss: 1.4402 - val_acc: 0.6875\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9404 - acc: 0.7527 - val_loss: 1.4666 - val_acc: 0.6250\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9193 - acc: 0.7559 - val_loss: 1.4667 - val_acc: 0.6667\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9034 - acc: 0.7658 - val_loss: 1.4675 - val_acc: 0.6667\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8719 - acc: 0.7712 - val_loss: 1.4847 - val_acc: 0.7083\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8298 - acc: 0.7878 - val_loss: 1.4388 - val_acc: 0.6458\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8163 - acc: 0.7839 - val_loss: 1.4361 - val_acc: 0.7083\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8089 - acc: 0.7902 - val_loss: 1.4197 - val_acc: 0.7292\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7876 - acc: 0.7952 - val_loss: 1.4309 - val_acc: 0.6667\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7658 - acc: 0.7956 - val_loss: 1.3043 - val_acc: 0.7083\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7418 - acc: 0.8055 - val_loss: 1.4246 - val_acc: 0.6667\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7392 - acc: 0.8032 - val_loss: 1.3602 - val_acc: 0.7083\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7308 - acc: 0.8110 - val_loss: 1.3950 - val_acc: 0.7292\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7154 - acc: 0.8124 - val_loss: 1.3995 - val_acc: 0.6667\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.6919 - acc: 0.8192 - val_loss: 1.5116 - val_acc: 0.6875\n",
            "processing fold # 11\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 6s 1ms/step - loss: 2.8806 - acc: 0.2283 - val_loss: 2.5629 - val_acc: 0.2917\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.3917 - acc: 0.3568 - val_loss: 1.8395 - val_acc: 0.5000\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.0455 - acc: 0.4426 - val_loss: 1.6581 - val_acc: 0.5625\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.8100 - acc: 0.5034 - val_loss: 1.5871 - val_acc: 0.4583\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.6404 - acc: 0.5524 - val_loss: 1.3660 - val_acc: 0.6458\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.5403 - acc: 0.5908 - val_loss: 1.3978 - val_acc: 0.6042\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.4180 - acc: 0.6074 - val_loss: 1.3630 - val_acc: 0.6042\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.3212 - acc: 0.6346 - val_loss: 1.1938 - val_acc: 0.7292\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2473 - acc: 0.6613 - val_loss: 1.1117 - val_acc: 0.6875\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.1819 - acc: 0.6838 - val_loss: 1.0756 - val_acc: 0.6875\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1192 - acc: 0.7042 - val_loss: 1.1685 - val_acc: 0.7083\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0565 - acc: 0.7258 - val_loss: 1.1308 - val_acc: 0.7083\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0289 - acc: 0.7269 - val_loss: 1.0188 - val_acc: 0.7500\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9728 - acc: 0.7483 - val_loss: 1.0642 - val_acc: 0.7083\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9643 - acc: 0.7460 - val_loss: 0.9802 - val_acc: 0.7292\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9509 - acc: 0.7460 - val_loss: 0.9783 - val_acc: 0.7500\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9198 - acc: 0.7605 - val_loss: 1.0110 - val_acc: 0.7708\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8878 - acc: 0.7672 - val_loss: 0.9566 - val_acc: 0.7292\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8584 - acc: 0.7780 - val_loss: 1.0040 - val_acc: 0.7708\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8282 - acc: 0.7828 - val_loss: 0.8744 - val_acc: 0.7917\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7959 - acc: 0.7954 - val_loss: 0.8830 - val_acc: 0.7917\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7932 - acc: 0.7895 - val_loss: 0.9711 - val_acc: 0.7917\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7711 - acc: 0.7996 - val_loss: 0.8486 - val_acc: 0.7500\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7441 - acc: 0.8078 - val_loss: 0.7662 - val_acc: 0.8125\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7485 - acc: 0.8036 - val_loss: 0.8675 - val_acc: 0.8125\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7386 - acc: 0.8070 - val_loss: 0.8990 - val_acc: 0.7708\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7487 - acc: 0.8080 - val_loss: 0.8515 - val_acc: 0.7708\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7163 - acc: 0.8133 - val_loss: 0.8764 - val_acc: 0.7917\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7015 - acc: 0.8196 - val_loss: 0.9653 - val_acc: 0.7708\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.6768 - acc: 0.8253 - val_loss: 0.8932 - val_acc: 0.8333\n",
            "processing fold # 12\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 6s 1ms/step - loss: 2.9701 - acc: 0.1941 - val_loss: 2.5344 - val_acc: 0.3333\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.4537 - acc: 0.3246 - val_loss: 2.3673 - val_acc: 0.2917\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.1810 - acc: 0.3982 - val_loss: 2.0895 - val_acc: 0.2917\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.9516 - acc: 0.4523 - val_loss: 1.7775 - val_acc: 0.5208\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.7698 - acc: 0.5139 - val_loss: 1.5498 - val_acc: 0.6458\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.6299 - acc: 0.5513 - val_loss: 1.5575 - val_acc: 0.6042\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.4851 - acc: 0.5931 - val_loss: 1.3330 - val_acc: 0.7292\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.3975 - acc: 0.6112 - val_loss: 1.3094 - val_acc: 0.6250\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.3435 - acc: 0.6245 - val_loss: 1.1459 - val_acc: 0.7500\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.2605 - acc: 0.6581 - val_loss: 1.1501 - val_acc: 0.6875\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.2039 - acc: 0.6758 - val_loss: 1.1530 - val_acc: 0.6458\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.1427 - acc: 0.6949 - val_loss: 1.0723 - val_acc: 0.7708\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.1188 - acc: 0.6993 - val_loss: 1.0740 - val_acc: 0.7708\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.0567 - acc: 0.7113 - val_loss: 1.0789 - val_acc: 0.7292\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0344 - acc: 0.7216 - val_loss: 0.9850 - val_acc: 0.7500\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9807 - acc: 0.7397 - val_loss: 1.0668 - val_acc: 0.7083\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9610 - acc: 0.7473 - val_loss: 1.0641 - val_acc: 0.7500\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9399 - acc: 0.7460 - val_loss: 0.9971 - val_acc: 0.7708\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9116 - acc: 0.7614 - val_loss: 1.1202 - val_acc: 0.7708\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8995 - acc: 0.7555 - val_loss: 1.0840 - val_acc: 0.7500\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8703 - acc: 0.7689 - val_loss: 1.1063 - val_acc: 0.6875\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8738 - acc: 0.7620 - val_loss: 1.0140 - val_acc: 0.7083\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8351 - acc: 0.7780 - val_loss: 0.9666 - val_acc: 0.7708\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8265 - acc: 0.7855 - val_loss: 0.9650 - val_acc: 0.7500\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7991 - acc: 0.7874 - val_loss: 1.0462 - val_acc: 0.7708\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7766 - acc: 0.7952 - val_loss: 0.9323 - val_acc: 0.7500\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7718 - acc: 0.8007 - val_loss: 1.1117 - val_acc: 0.7708\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7350 - acc: 0.8078 - val_loss: 1.1002 - val_acc: 0.7292\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7328 - acc: 0.8017 - val_loss: 1.1478 - val_acc: 0.7083\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7251 - acc: 0.8066 - val_loss: 1.0701 - val_acc: 0.7500\n",
            "processing fold # 13\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 6s 1ms/step - loss: 3.0150 - acc: 0.1918 - val_loss: 2.7640 - val_acc: 0.2500\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 2.4458 - acc: 0.3337 - val_loss: 2.2919 - val_acc: 0.2917\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.1837 - acc: 0.3991 - val_loss: 2.0045 - val_acc: 0.4375\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.9582 - acc: 0.4603 - val_loss: 1.6641 - val_acc: 0.5417\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.7904 - acc: 0.5078 - val_loss: 1.4511 - val_acc: 0.6875\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.6754 - acc: 0.5429 - val_loss: 1.3802 - val_acc: 0.6667\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.5370 - acc: 0.5763 - val_loss: 1.2237 - val_acc: 0.7083\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.4432 - acc: 0.6047 - val_loss: 1.1891 - val_acc: 0.6667\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.3647 - acc: 0.6194 - val_loss: 1.1900 - val_acc: 0.6875\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.2990 - acc: 0.6432 - val_loss: 1.1281 - val_acc: 0.7083\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.2240 - acc: 0.6686 - val_loss: 1.0757 - val_acc: 0.7708\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.1690 - acc: 0.6894 - val_loss: 0.9861 - val_acc: 0.8125\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.1252 - acc: 0.7012 - val_loss: 0.9829 - val_acc: 0.7500\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0682 - acc: 0.7130 - val_loss: 0.9594 - val_acc: 0.7708\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0211 - acc: 0.7281 - val_loss: 0.9493 - val_acc: 0.7708\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0126 - acc: 0.7332 - val_loss: 0.9751 - val_acc: 0.7500\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9609 - acc: 0.7498 - val_loss: 0.9756 - val_acc: 0.7500\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9309 - acc: 0.7590 - val_loss: 0.9799 - val_acc: 0.7708\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9075 - acc: 0.7637 - val_loss: 0.9781 - val_acc: 0.7708\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8855 - acc: 0.7704 - val_loss: 0.9732 - val_acc: 0.7708\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8944 - acc: 0.7605 - val_loss: 0.9644 - val_acc: 0.7917\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8315 - acc: 0.7860 - val_loss: 0.9525 - val_acc: 0.8125\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8228 - acc: 0.7807 - val_loss: 0.9724 - val_acc: 0.7917\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8115 - acc: 0.7851 - val_loss: 1.0059 - val_acc: 0.7917\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7958 - acc: 0.7881 - val_loss: 0.9327 - val_acc: 0.7708\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 26us/step - loss: 0.7961 - acc: 0.7904 - val_loss: 0.9722 - val_acc: 0.7708\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7816 - acc: 0.7908 - val_loss: 0.9028 - val_acc: 0.7917\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7607 - acc: 0.8021 - val_loss: 1.0148 - val_acc: 0.7708\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7486 - acc: 0.8061 - val_loss: 0.9624 - val_acc: 0.7917\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7098 - acc: 0.8198 - val_loss: 0.9573 - val_acc: 0.8125\n",
            "processing fold # 14\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 6s 1ms/step - loss: 2.9801 - acc: 0.2058 - val_loss: 2.4423 - val_acc: 0.3125\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.3789 - acc: 0.3585 - val_loss: 2.0578 - val_acc: 0.5417\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.0901 - acc: 0.4304 - val_loss: 1.8513 - val_acc: 0.4792\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.8951 - acc: 0.4777 - val_loss: 1.7874 - val_acc: 0.5417\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.7407 - acc: 0.5244 - val_loss: 1.6194 - val_acc: 0.5417\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.5518 - acc: 0.5698 - val_loss: 1.4648 - val_acc: 0.6042\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.4342 - acc: 0.6045 - val_loss: 1.2663 - val_acc: 0.6875\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3550 - acc: 0.6316 - val_loss: 1.1419 - val_acc: 0.7500\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2800 - acc: 0.6583 - val_loss: 1.1542 - val_acc: 0.7500\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2200 - acc: 0.6642 - val_loss: 1.0705 - val_acc: 0.7500\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1657 - acc: 0.6812 - val_loss: 1.0460 - val_acc: 0.7083\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1129 - acc: 0.7046 - val_loss: 0.9934 - val_acc: 0.7500\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0740 - acc: 0.7098 - val_loss: 0.9610 - val_acc: 0.7500\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0222 - acc: 0.7286 - val_loss: 0.8867 - val_acc: 0.7292\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9865 - acc: 0.7395 - val_loss: 0.9453 - val_acc: 0.7708\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9694 - acc: 0.7437 - val_loss: 0.8426 - val_acc: 0.7917\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9189 - acc: 0.7624 - val_loss: 0.8039 - val_acc: 0.7917\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8999 - acc: 0.7630 - val_loss: 0.8045 - val_acc: 0.8333\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8644 - acc: 0.7824 - val_loss: 0.7817 - val_acc: 0.8333\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8497 - acc: 0.7778 - val_loss: 0.8789 - val_acc: 0.7708\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8304 - acc: 0.7782 - val_loss: 0.8434 - val_acc: 0.8125\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8334 - acc: 0.7799 - val_loss: 0.8246 - val_acc: 0.7917\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7966 - acc: 0.7918 - val_loss: 0.8792 - val_acc: 0.7917\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7875 - acc: 0.8007 - val_loss: 0.7899 - val_acc: 0.8333\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7908 - acc: 0.7929 - val_loss: 0.7190 - val_acc: 0.8542\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7771 - acc: 0.7952 - val_loss: 0.7790 - val_acc: 0.8333\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7548 - acc: 0.8070 - val_loss: 0.6786 - val_acc: 0.8125\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7188 - acc: 0.8099 - val_loss: 0.6777 - val_acc: 0.7917\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7036 - acc: 0.8179 - val_loss: 0.7631 - val_acc: 0.7500\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7066 - acc: 0.8089 - val_loss: 0.8514 - val_acc: 0.7708\n",
            "processing fold # 15\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 7s 1ms/step - loss: 3.0356 - acc: 0.2079 - val_loss: 2.6805 - val_acc: 0.2292\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.4682 - acc: 0.3244 - val_loss: 2.3331 - val_acc: 0.3542\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.1916 - acc: 0.3835 - val_loss: 2.0734 - val_acc: 0.4167\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.9935 - acc: 0.4354 - val_loss: 1.7426 - val_acc: 0.5208\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.8065 - acc: 0.5002 - val_loss: 1.6037 - val_acc: 0.5417\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.6501 - acc: 0.5404 - val_loss: 1.4500 - val_acc: 0.6875\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.5586 - acc: 0.5723 - val_loss: 1.3297 - val_acc: 0.7083\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.4601 - acc: 0.6018 - val_loss: 1.2831 - val_acc: 0.6875\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.3830 - acc: 0.6228 - val_loss: 1.2019 - val_acc: 0.6458\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.2906 - acc: 0.6508 - val_loss: 1.1241 - val_acc: 0.6875\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.2288 - acc: 0.6663 - val_loss: 1.0189 - val_acc: 0.7708\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.1596 - acc: 0.6939 - val_loss: 1.0197 - val_acc: 0.6458\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1401 - acc: 0.6943 - val_loss: 0.9985 - val_acc: 0.6875\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0782 - acc: 0.7185 - val_loss: 0.9814 - val_acc: 0.7083\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0125 - acc: 0.7311 - val_loss: 0.9316 - val_acc: 0.7083\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0120 - acc: 0.7368 - val_loss: 0.9093 - val_acc: 0.7292\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9930 - acc: 0.7422 - val_loss: 0.9747 - val_acc: 0.6875\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9355 - acc: 0.7557 - val_loss: 0.8519 - val_acc: 0.7500\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9308 - acc: 0.7559 - val_loss: 0.8900 - val_acc: 0.7083\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8981 - acc: 0.7616 - val_loss: 0.8237 - val_acc: 0.7708\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8778 - acc: 0.7635 - val_loss: 0.8352 - val_acc: 0.7292\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8632 - acc: 0.7708 - val_loss: 0.8167 - val_acc: 0.7292\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8585 - acc: 0.7735 - val_loss: 0.8404 - val_acc: 0.7292\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8181 - acc: 0.7815 - val_loss: 0.8059 - val_acc: 0.6875\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8089 - acc: 0.7929 - val_loss: 0.7937 - val_acc: 0.7292\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7919 - acc: 0.7942 - val_loss: 0.8253 - val_acc: 0.7292\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7765 - acc: 0.8013 - val_loss: 0.8153 - val_acc: 0.7708\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7591 - acc: 0.8074 - val_loss: 0.7400 - val_acc: 0.7500\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7358 - acc: 0.8089 - val_loss: 0.6589 - val_acc: 0.8125\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7296 - acc: 0.8156 - val_loss: 0.8307 - val_acc: 0.7708\n",
            "processing fold # 16\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 7s 1ms/step - loss: 2.9192 - acc: 0.2237 - val_loss: 2.8272 - val_acc: 0.2708\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.3657 - acc: 0.3663 - val_loss: 2.4311 - val_acc: 0.3750\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.0552 - acc: 0.4510 - val_loss: 1.9412 - val_acc: 0.4792\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.8254 - acc: 0.5023 - val_loss: 1.8892 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.6471 - acc: 0.5488 - val_loss: 1.7107 - val_acc: 0.5000\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.5252 - acc: 0.5877 - val_loss: 1.6804 - val_acc: 0.5208\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.4167 - acc: 0.6203 - val_loss: 1.6314 - val_acc: 0.5833\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.3117 - acc: 0.6377 - val_loss: 1.4485 - val_acc: 0.5625\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.2560 - acc: 0.6667 - val_loss: 1.4775 - val_acc: 0.6042\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.1857 - acc: 0.6827 - val_loss: 1.4800 - val_acc: 0.5000\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.1371 - acc: 0.6947 - val_loss: 1.3012 - val_acc: 0.6667\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1068 - acc: 0.6991 - val_loss: 1.3144 - val_acc: 0.6250\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0632 - acc: 0.7185 - val_loss: 1.2890 - val_acc: 0.6250\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0077 - acc: 0.7426 - val_loss: 1.3425 - val_acc: 0.6458\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9847 - acc: 0.7426 - val_loss: 1.2943 - val_acc: 0.7083\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9526 - acc: 0.7466 - val_loss: 1.3022 - val_acc: 0.6458\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9234 - acc: 0.7597 - val_loss: 1.2552 - val_acc: 0.6875\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8813 - acc: 0.7706 - val_loss: 1.3233 - val_acc: 0.6042\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8875 - acc: 0.7681 - val_loss: 1.2075 - val_acc: 0.6250\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8515 - acc: 0.7761 - val_loss: 1.2570 - val_acc: 0.6875\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8071 - acc: 0.7899 - val_loss: 1.2276 - val_acc: 0.7292\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8175 - acc: 0.7902 - val_loss: 1.2837 - val_acc: 0.6458\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 29us/step - loss: 0.7902 - acc: 0.7969 - val_loss: 1.2688 - val_acc: 0.6458\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7595 - acc: 0.8032 - val_loss: 1.2383 - val_acc: 0.6667\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7562 - acc: 0.8061 - val_loss: 1.2948 - val_acc: 0.6250\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7538 - acc: 0.8066 - val_loss: 1.1965 - val_acc: 0.6667\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7238 - acc: 0.8129 - val_loss: 1.3001 - val_acc: 0.6250\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7148 - acc: 0.8150 - val_loss: 1.3037 - val_acc: 0.6458\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7103 - acc: 0.8124 - val_loss: 1.2340 - val_acc: 0.6667\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.6972 - acc: 0.8219 - val_loss: 1.3557 - val_acc: 0.6667\n",
            "processing fold # 17\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 7s 1ms/step - loss: 2.9762 - acc: 0.1985 - val_loss: 2.4528 - val_acc: 0.3542\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.4208 - acc: 0.3457 - val_loss: 2.1426 - val_acc: 0.3125\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.1557 - acc: 0.4060 - val_loss: 1.7093 - val_acc: 0.5417\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.9059 - acc: 0.4832 - val_loss: 1.4338 - val_acc: 0.5625\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.7333 - acc: 0.5206 - val_loss: 1.2858 - val_acc: 0.6667\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.5952 - acc: 0.5620 - val_loss: 1.1902 - val_acc: 0.6875\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.5046 - acc: 0.5807 - val_loss: 1.1305 - val_acc: 0.7083\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4192 - acc: 0.6163 - val_loss: 1.1736 - val_acc: 0.7292\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3239 - acc: 0.6461 - val_loss: 1.0322 - val_acc: 0.7708\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2648 - acc: 0.6619 - val_loss: 1.0527 - val_acc: 0.7292\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.1974 - acc: 0.6766 - val_loss: 0.9751 - val_acc: 0.7500\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 26us/step - loss: 1.1379 - acc: 0.7044 - val_loss: 0.9312 - val_acc: 0.7500\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0883 - acc: 0.7088 - val_loss: 0.8650 - val_acc: 0.7917\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.0505 - acc: 0.7273 - val_loss: 0.9734 - val_acc: 0.7500\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0240 - acc: 0.7271 - val_loss: 0.8570 - val_acc: 0.8125\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9982 - acc: 0.7410 - val_loss: 0.8617 - val_acc: 0.7708\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9578 - acc: 0.7508 - val_loss: 0.8138 - val_acc: 0.8333\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9548 - acc: 0.7492 - val_loss: 0.8306 - val_acc: 0.8125\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.9330 - acc: 0.7590 - val_loss: 0.7669 - val_acc: 0.8125\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8973 - acc: 0.7683 - val_loss: 0.7875 - val_acc: 0.7708\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8624 - acc: 0.7773 - val_loss: 0.7896 - val_acc: 0.8333\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8506 - acc: 0.7742 - val_loss: 0.6896 - val_acc: 0.8542\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8334 - acc: 0.7862 - val_loss: 0.8839 - val_acc: 0.7917\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.8125 - acc: 0.7916 - val_loss: 0.7431 - val_acc: 0.8542\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7674 - acc: 0.8070 - val_loss: 0.7089 - val_acc: 0.8333\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7671 - acc: 0.8032 - val_loss: 0.7542 - val_acc: 0.8125\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7570 - acc: 0.8072 - val_loss: 0.7490 - val_acc: 0.8542\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7418 - acc: 0.8101 - val_loss: 0.6551 - val_acc: 0.7917\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7353 - acc: 0.8154 - val_loss: 0.7889 - val_acc: 0.8333\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 0.7218 - acc: 0.8164 - val_loss: 0.6839 - val_acc: 0.8333\n",
            "processing fold # 18\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 7s 1ms/step - loss: 2.9638 - acc: 0.2109 - val_loss: 2.6371 - val_acc: 0.2500\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.3806 - acc: 0.3501 - val_loss: 2.1546 - val_acc: 0.3333\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.1055 - acc: 0.4123 - val_loss: 1.8146 - val_acc: 0.4167\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.8669 - acc: 0.4792 - val_loss: 1.5255 - val_acc: 0.5833\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.7083 - acc: 0.5339 - val_loss: 1.4401 - val_acc: 0.6042\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.6069 - acc: 0.5477 - val_loss: 1.2721 - val_acc: 0.6042\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.4900 - acc: 0.5801 - val_loss: 1.1163 - val_acc: 0.7708\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.3981 - acc: 0.6152 - val_loss: 1.1676 - val_acc: 0.6875\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.3074 - acc: 0.6405 - val_loss: 1.0491 - val_acc: 0.7500\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2430 - acc: 0.6655 - val_loss: 1.0524 - val_acc: 0.7292\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1995 - acc: 0.6775 - val_loss: 1.0120 - val_acc: 0.7083\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.1152 - acc: 0.7025 - val_loss: 0.9910 - val_acc: 0.7500\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0732 - acc: 0.7195 - val_loss: 0.9696 - val_acc: 0.7083\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0443 - acc: 0.7172 - val_loss: 0.9207 - val_acc: 0.7083\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0101 - acc: 0.7342 - val_loss: 0.9497 - val_acc: 0.7500\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9635 - acc: 0.7525 - val_loss: 0.9235 - val_acc: 0.7708\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9531 - acc: 0.7529 - val_loss: 0.8434 - val_acc: 0.6667\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9136 - acc: 0.7647 - val_loss: 0.8596 - val_acc: 0.7292\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8897 - acc: 0.7675 - val_loss: 0.9047 - val_acc: 0.7708\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8701 - acc: 0.7712 - val_loss: 0.8183 - val_acc: 0.7500\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8463 - acc: 0.7853 - val_loss: 0.8682 - val_acc: 0.8125\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8327 - acc: 0.7820 - val_loss: 0.8889 - val_acc: 0.7500\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8111 - acc: 0.7929 - val_loss: 0.8071 - val_acc: 0.7708\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7888 - acc: 0.7958 - val_loss: 1.0109 - val_acc: 0.7500\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7722 - acc: 0.7990 - val_loss: 0.9375 - val_acc: 0.8125\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7572 - acc: 0.8042 - val_loss: 0.7627 - val_acc: 0.8125\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7380 - acc: 0.8108 - val_loss: 0.8618 - val_acc: 0.7292\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7315 - acc: 0.8127 - val_loss: 0.9024 - val_acc: 0.7708\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.6954 - acc: 0.8204 - val_loss: 0.9463 - val_acc: 0.7292\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.6994 - acc: 0.8194 - val_loss: 0.9756 - val_acc: 0.7500\n",
            "processing fold # 19\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 7s 2ms/step - loss: 2.8710 - acc: 0.2265 - val_loss: 2.3486 - val_acc: 0.3125\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.3727 - acc: 0.3484 - val_loss: 2.0097 - val_acc: 0.4583\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.0717 - acc: 0.4285 - val_loss: 1.8372 - val_acc: 0.4792\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.8517 - acc: 0.4903 - val_loss: 1.8661 - val_acc: 0.3958\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.7273 - acc: 0.5143 - val_loss: 1.5317 - val_acc: 0.5625\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5511 - acc: 0.5740 - val_loss: 1.4641 - val_acc: 0.5625\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.4273 - acc: 0.6053 - val_loss: 1.4269 - val_acc: 0.6667\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.3630 - acc: 0.6230 - val_loss: 1.3862 - val_acc: 0.5833\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.2724 - acc: 0.6569 - val_loss: 1.1641 - val_acc: 0.6667\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.1958 - acc: 0.6722 - val_loss: 1.3149 - val_acc: 0.6250\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.1393 - acc: 0.6968 - val_loss: 1.1479 - val_acc: 0.6458\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.1140 - acc: 0.7019 - val_loss: 1.2244 - val_acc: 0.6875\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0559 - acc: 0.7229 - val_loss: 1.2244 - val_acc: 0.6042\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0213 - acc: 0.7332 - val_loss: 1.1225 - val_acc: 0.6458\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9974 - acc: 0.7368 - val_loss: 1.0078 - val_acc: 0.7292\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9656 - acc: 0.7555 - val_loss: 1.1140 - val_acc: 0.7083\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9456 - acc: 0.7517 - val_loss: 1.0235 - val_acc: 0.7500\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9274 - acc: 0.7567 - val_loss: 1.0222 - val_acc: 0.7083\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8670 - acc: 0.7733 - val_loss: 1.0210 - val_acc: 0.7500\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8727 - acc: 0.7723 - val_loss: 1.0347 - val_acc: 0.7500\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8312 - acc: 0.7805 - val_loss: 0.9483 - val_acc: 0.7292\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8283 - acc: 0.7811 - val_loss: 0.9678 - val_acc: 0.7708\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7990 - acc: 0.7937 - val_loss: 0.9754 - val_acc: 0.7292\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7859 - acc: 0.7969 - val_loss: 0.9196 - val_acc: 0.7917\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7607 - acc: 0.8070 - val_loss: 0.9949 - val_acc: 0.7708\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7527 - acc: 0.8026 - val_loss: 0.9020 - val_acc: 0.7708\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7559 - acc: 0.8080 - val_loss: 0.9355 - val_acc: 0.7708\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7163 - acc: 0.8167 - val_loss: 1.0148 - val_acc: 0.7708\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7237 - acc: 0.8146 - val_loss: 0.8915 - val_acc: 0.7708\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7257 - acc: 0.8072 - val_loss: 0.8329 - val_acc: 0.7708\n",
            "processing fold # 20\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 8s 2ms/step - loss: 2.8892 - acc: 0.2161 - val_loss: 2.3560 - val_acc: 0.3333\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.4115 - acc: 0.3373 - val_loss: 2.0367 - val_acc: 0.4583\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.0724 - acc: 0.4342 - val_loss: 1.9050 - val_acc: 0.4167\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.8559 - acc: 0.4945 - val_loss: 1.7460 - val_acc: 0.3750\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.6997 - acc: 0.5292 - val_loss: 1.6556 - val_acc: 0.4792\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.5633 - acc: 0.5702 - val_loss: 1.4836 - val_acc: 0.5417\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.4576 - acc: 0.6005 - val_loss: 1.5683 - val_acc: 0.5625\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.3848 - acc: 0.6182 - val_loss: 1.3905 - val_acc: 0.5833\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.2927 - acc: 0.6579 - val_loss: 1.3786 - val_acc: 0.6875\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2183 - acc: 0.6674 - val_loss: 1.3624 - val_acc: 0.6458\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1622 - acc: 0.6878 - val_loss: 1.3451 - val_acc: 0.6875\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1400 - acc: 0.6968 - val_loss: 1.4044 - val_acc: 0.7083\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0676 - acc: 0.7153 - val_loss: 1.4218 - val_acc: 0.7083\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0177 - acc: 0.7256 - val_loss: 1.3829 - val_acc: 0.6875\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9737 - acc: 0.7401 - val_loss: 1.5024 - val_acc: 0.6042\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9775 - acc: 0.7378 - val_loss: 1.4357 - val_acc: 0.6250\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9476 - acc: 0.7458 - val_loss: 1.3854 - val_acc: 0.6458\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9145 - acc: 0.7550 - val_loss: 1.3800 - val_acc: 0.6458\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8909 - acc: 0.7645 - val_loss: 1.4882 - val_acc: 0.5833\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 29us/step - loss: 0.8658 - acc: 0.7698 - val_loss: 1.3891 - val_acc: 0.6250\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8345 - acc: 0.7815 - val_loss: 1.4534 - val_acc: 0.6667\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8230 - acc: 0.7813 - val_loss: 1.4620 - val_acc: 0.6250\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8101 - acc: 0.7921 - val_loss: 1.4291 - val_acc: 0.6458\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7666 - acc: 0.7975 - val_loss: 1.3865 - val_acc: 0.6875\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7542 - acc: 0.8034 - val_loss: 1.4808 - val_acc: 0.6458\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7649 - acc: 0.8032 - val_loss: 1.4104 - val_acc: 0.7083\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7565 - acc: 0.8045 - val_loss: 1.4826 - val_acc: 0.6042\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7485 - acc: 0.8059 - val_loss: 1.4033 - val_acc: 0.6458\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7227 - acc: 0.8154 - val_loss: 1.4733 - val_acc: 0.6042\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7050 - acc: 0.8190 - val_loss: 1.5013 - val_acc: 0.6250\n",
            "processing fold # 21\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 8s 2ms/step - loss: 2.8919 - acc: 0.2319 - val_loss: 2.6033 - val_acc: 0.2917\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.4518 - acc: 0.3356 - val_loss: 2.2461 - val_acc: 0.3333\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.1219 - acc: 0.4169 - val_loss: 1.8632 - val_acc: 0.4792\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.8951 - acc: 0.4765 - val_loss: 1.7181 - val_acc: 0.6042\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 18us/step - loss: 1.7108 - acc: 0.5280 - val_loss: 1.5786 - val_acc: 0.5625\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5670 - acc: 0.5690 - val_loss: 1.5124 - val_acc: 0.5625\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.4383 - acc: 0.6070 - val_loss: 1.4982 - val_acc: 0.5417\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.3726 - acc: 0.6192 - val_loss: 1.3400 - val_acc: 0.5833\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.2860 - acc: 0.6575 - val_loss: 1.2645 - val_acc: 0.6250\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.2111 - acc: 0.6707 - val_loss: 1.2692 - val_acc: 0.6458\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.1621 - acc: 0.6899 - val_loss: 1.2505 - val_acc: 0.6042\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.1228 - acc: 0.7002 - val_loss: 1.3253 - val_acc: 0.5833\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0617 - acc: 0.7197 - val_loss: 1.1579 - val_acc: 0.6875\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0342 - acc: 0.7281 - val_loss: 1.1521 - val_acc: 0.6458\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9770 - acc: 0.7418 - val_loss: 1.0850 - val_acc: 0.6667\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9566 - acc: 0.7462 - val_loss: 1.1039 - val_acc: 0.6875\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9192 - acc: 0.7601 - val_loss: 1.1034 - val_acc: 0.7292\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8854 - acc: 0.7706 - val_loss: 1.0725 - val_acc: 0.7083\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8594 - acc: 0.7780 - val_loss: 1.1185 - val_acc: 0.6875\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8302 - acc: 0.7866 - val_loss: 1.1254 - val_acc: 0.7083\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8445 - acc: 0.7700 - val_loss: 1.0364 - val_acc: 0.6875\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8198 - acc: 0.7883 - val_loss: 1.1083 - val_acc: 0.7292\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8092 - acc: 0.7876 - val_loss: 1.0429 - val_acc: 0.7083\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7775 - acc: 0.7956 - val_loss: 1.1133 - val_acc: 0.7292\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7875 - acc: 0.7921 - val_loss: 1.0937 - val_acc: 0.6875\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7512 - acc: 0.8091 - val_loss: 1.0153 - val_acc: 0.7500\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7425 - acc: 0.8110 - val_loss: 0.9904 - val_acc: 0.7292\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7073 - acc: 0.8158 - val_loss: 1.0637 - val_acc: 0.7083\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7194 - acc: 0.8133 - val_loss: 1.0950 - val_acc: 0.7500\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7006 - acc: 0.8179 - val_loss: 1.0883 - val_acc: 0.7292\n",
            "processing fold # 22\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 8s 2ms/step - loss: 2.8404 - acc: 0.2452 - val_loss: 2.0136 - val_acc: 0.4792\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.3151 - acc: 0.3831 - val_loss: 1.7662 - val_acc: 0.4167\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.0114 - acc: 0.4577 - val_loss: 1.7625 - val_acc: 0.4792\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.8077 - acc: 0.4971 - val_loss: 1.6608 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.6398 - acc: 0.5515 - val_loss: 1.4857 - val_acc: 0.5833\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.5029 - acc: 0.5904 - val_loss: 1.4578 - val_acc: 0.5625\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.3822 - acc: 0.6257 - val_loss: 1.4716 - val_acc: 0.6042\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.2879 - acc: 0.6468 - val_loss: 1.3564 - val_acc: 0.6250\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2245 - acc: 0.6659 - val_loss: 1.4523 - val_acc: 0.6042\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.1694 - acc: 0.6857 - val_loss: 1.3662 - val_acc: 0.6458\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1115 - acc: 0.7000 - val_loss: 1.3780 - val_acc: 0.6042\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0583 - acc: 0.7218 - val_loss: 1.3560 - val_acc: 0.6042\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0413 - acc: 0.7271 - val_loss: 1.5034 - val_acc: 0.6042\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0282 - acc: 0.7286 - val_loss: 1.3269 - val_acc: 0.6250\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9641 - acc: 0.7443 - val_loss: 1.3764 - val_acc: 0.6250\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9558 - acc: 0.7489 - val_loss: 1.3650 - val_acc: 0.6042\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8986 - acc: 0.7624 - val_loss: 1.4103 - val_acc: 0.5417\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8841 - acc: 0.7639 - val_loss: 1.3022 - val_acc: 0.6458\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8586 - acc: 0.7685 - val_loss: 1.3005 - val_acc: 0.5833\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8307 - acc: 0.7817 - val_loss: 1.2716 - val_acc: 0.6667\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8309 - acc: 0.7849 - val_loss: 1.2813 - val_acc: 0.6458\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8240 - acc: 0.7799 - val_loss: 1.2848 - val_acc: 0.6667\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8038 - acc: 0.7906 - val_loss: 1.2569 - val_acc: 0.6458\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7656 - acc: 0.7986 - val_loss: 1.1731 - val_acc: 0.6250\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7552 - acc: 0.8005 - val_loss: 1.0588 - val_acc: 0.6875\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7462 - acc: 0.8047 - val_loss: 1.1077 - val_acc: 0.7083\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7356 - acc: 0.8066 - val_loss: 1.2970 - val_acc: 0.6250\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7242 - acc: 0.8150 - val_loss: 1.1775 - val_acc: 0.6458\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7012 - acc: 0.8211 - val_loss: 1.1512 - val_acc: 0.6667\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.6846 - acc: 0.8244 - val_loss: 1.2336 - val_acc: 0.6458\n",
            "processing fold # 23\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 8s 2ms/step - loss: 2.9379 - acc: 0.2279 - val_loss: 2.5925 - val_acc: 0.2917\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.4839 - acc: 0.2990 - val_loss: 2.1586 - val_acc: 0.3333\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.1662 - acc: 0.3869 - val_loss: 1.8459 - val_acc: 0.4792\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.9540 - acc: 0.4540 - val_loss: 1.5332 - val_acc: 0.5833\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.7456 - acc: 0.5189 - val_loss: 1.3273 - val_acc: 0.6250\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5989 - acc: 0.5574 - val_loss: 1.2329 - val_acc: 0.6667\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4805 - acc: 0.5866 - val_loss: 1.2131 - val_acc: 0.6875\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4078 - acc: 0.6186 - val_loss: 1.0282 - val_acc: 0.7500\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3133 - acc: 0.6409 - val_loss: 1.0593 - val_acc: 0.7292\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2268 - acc: 0.6756 - val_loss: 0.9788 - val_acc: 0.6875\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.1870 - acc: 0.6798 - val_loss: 0.9005 - val_acc: 0.7500\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1277 - acc: 0.7012 - val_loss: 0.8449 - val_acc: 0.7917\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1035 - acc: 0.7101 - val_loss: 0.7555 - val_acc: 0.7708\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0518 - acc: 0.7197 - val_loss: 0.8144 - val_acc: 0.7708\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9922 - acc: 0.7359 - val_loss: 0.7780 - val_acc: 0.7500\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9692 - acc: 0.7435 - val_loss: 0.8440 - val_acc: 0.7500\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9480 - acc: 0.7494 - val_loss: 0.7883 - val_acc: 0.7708\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9254 - acc: 0.7546 - val_loss: 0.6898 - val_acc: 0.8125\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8985 - acc: 0.7609 - val_loss: 0.6740 - val_acc: 0.7917\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8690 - acc: 0.7702 - val_loss: 0.6607 - val_acc: 0.7917\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8194 - acc: 0.7849 - val_loss: 0.6506 - val_acc: 0.7917\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8193 - acc: 0.7895 - val_loss: 0.6585 - val_acc: 0.7917\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8075 - acc: 0.7893 - val_loss: 0.6829 - val_acc: 0.8125\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7750 - acc: 0.7933 - val_loss: 0.7148 - val_acc: 0.7500\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7693 - acc: 0.7988 - val_loss: 0.7415 - val_acc: 0.7708\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7703 - acc: 0.7969 - val_loss: 0.6412 - val_acc: 0.8542\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7500 - acc: 0.8061 - val_loss: 0.6889 - val_acc: 0.8125\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7439 - acc: 0.8063 - val_loss: 0.7235 - val_acc: 0.8125\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7230 - acc: 0.8150 - val_loss: 0.7224 - val_acc: 0.8125\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.6953 - acc: 0.8280 - val_loss: 0.6756 - val_acc: 0.8125\n",
            "processing fold # 24\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 9s 2ms/step - loss: 3.0693 - acc: 0.1966 - val_loss: 2.2721 - val_acc: 0.4375\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.4991 - acc: 0.3312 - val_loss: 1.8998 - val_acc: 0.4583\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.1778 - acc: 0.4031 - val_loss: 1.7469 - val_acc: 0.5625\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.9581 - acc: 0.4634 - val_loss: 1.7245 - val_acc: 0.5208\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.8087 - acc: 0.5090 - val_loss: 1.4819 - val_acc: 0.6042\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.6704 - acc: 0.5368 - val_loss: 1.3729 - val_acc: 0.6458\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.5631 - acc: 0.5742 - val_loss: 1.3033 - val_acc: 0.6667\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.4493 - acc: 0.6079 - val_loss: 1.2919 - val_acc: 0.6875\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3807 - acc: 0.6249 - val_loss: 1.3288 - val_acc: 0.6250\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2975 - acc: 0.6436 - val_loss: 1.3153 - val_acc: 0.6667\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2386 - acc: 0.6665 - val_loss: 1.2993 - val_acc: 0.6458\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.1715 - acc: 0.6876 - val_loss: 1.3009 - val_acc: 0.6667\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1404 - acc: 0.6960 - val_loss: 1.2998 - val_acc: 0.6875\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0916 - acc: 0.7092 - val_loss: 1.2468 - val_acc: 0.6875\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0480 - acc: 0.7210 - val_loss: 1.2827 - val_acc: 0.6667\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0198 - acc: 0.7361 - val_loss: 1.2725 - val_acc: 0.7083\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0029 - acc: 0.7363 - val_loss: 1.2279 - val_acc: 0.7292\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9695 - acc: 0.7435 - val_loss: 1.2128 - val_acc: 0.7292\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9474 - acc: 0.7502 - val_loss: 1.2258 - val_acc: 0.6875\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9117 - acc: 0.7626 - val_loss: 1.2282 - val_acc: 0.7083\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8913 - acc: 0.7691 - val_loss: 1.1632 - val_acc: 0.7083\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8626 - acc: 0.7738 - val_loss: 1.2591 - val_acc: 0.6458\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.8298 - acc: 0.7881 - val_loss: 1.2511 - val_acc: 0.6667\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8351 - acc: 0.7805 - val_loss: 1.2401 - val_acc: 0.6875\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8188 - acc: 0.7914 - val_loss: 1.1326 - val_acc: 0.6875\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7764 - acc: 0.8026 - val_loss: 1.1779 - val_acc: 0.7083\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7736 - acc: 0.8011 - val_loss: 1.2002 - val_acc: 0.7500\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7721 - acc: 0.8030 - val_loss: 1.1753 - val_acc: 0.7292\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7391 - acc: 0.8143 - val_loss: 1.1579 - val_acc: 0.7292\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7237 - acc: 0.8225 - val_loss: 1.1491 - val_acc: 0.7083\n",
            "processing fold # 25\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 8s 2ms/step - loss: 2.9646 - acc: 0.2031 - val_loss: 2.5650 - val_acc: 0.2708\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.4715 - acc: 0.3173 - val_loss: 1.9968 - val_acc: 0.5417\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.1903 - acc: 0.3898 - val_loss: 1.8760 - val_acc: 0.4792\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.9663 - acc: 0.4500 - val_loss: 1.6081 - val_acc: 0.5833\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.7954 - acc: 0.5046 - val_loss: 1.5213 - val_acc: 0.5625\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.6340 - acc: 0.5488 - val_loss: 1.5018 - val_acc: 0.5000\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5167 - acc: 0.5807 - val_loss: 1.3297 - val_acc: 0.6250\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.4180 - acc: 0.6186 - val_loss: 1.3574 - val_acc: 0.5833\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3481 - acc: 0.6333 - val_loss: 1.3882 - val_acc: 0.6250\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2715 - acc: 0.6526 - val_loss: 1.3155 - val_acc: 0.7083\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2064 - acc: 0.6789 - val_loss: 1.2448 - val_acc: 0.6875\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.1276 - acc: 0.7021 - val_loss: 1.2404 - val_acc: 0.7083\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0882 - acc: 0.7128 - val_loss: 1.1898 - val_acc: 0.6458\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0395 - acc: 0.7256 - val_loss: 1.3468 - val_acc: 0.6458\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0356 - acc: 0.7283 - val_loss: 1.3186 - val_acc: 0.6667\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9908 - acc: 0.7464 - val_loss: 1.1617 - val_acc: 0.6875\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9549 - acc: 0.7506 - val_loss: 1.2907 - val_acc: 0.6875\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9030 - acc: 0.7681 - val_loss: 1.3020 - val_acc: 0.6250\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8863 - acc: 0.7738 - val_loss: 1.1443 - val_acc: 0.6458\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8787 - acc: 0.7704 - val_loss: 1.1528 - val_acc: 0.6458\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8585 - acc: 0.7733 - val_loss: 1.2303 - val_acc: 0.7083\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8239 - acc: 0.7849 - val_loss: 1.2159 - val_acc: 0.6250\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8197 - acc: 0.7872 - val_loss: 1.2059 - val_acc: 0.6875\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7905 - acc: 0.7946 - val_loss: 1.2569 - val_acc: 0.7083\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7807 - acc: 0.8040 - val_loss: 1.2485 - val_acc: 0.6458\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7857 - acc: 0.7960 - val_loss: 1.1962 - val_acc: 0.6667\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7520 - acc: 0.8036 - val_loss: 1.2756 - val_acc: 0.7083\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7371 - acc: 0.8106 - val_loss: 1.2539 - val_acc: 0.7083\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7128 - acc: 0.8133 - val_loss: 1.3668 - val_acc: 0.7083\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7097 - acc: 0.8169 - val_loss: 1.3124 - val_acc: 0.6875\n",
            "processing fold # 26\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 9s 2ms/step - loss: 2.8830 - acc: 0.2313 - val_loss: 2.5843 - val_acc: 0.2917\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.3434 - acc: 0.3579 - val_loss: 2.1183 - val_acc: 0.3750\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.0369 - acc: 0.4298 - val_loss: 1.7882 - val_acc: 0.4375\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.8369 - acc: 0.4905 - val_loss: 1.5548 - val_acc: 0.6042\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.6776 - acc: 0.5364 - val_loss: 1.3440 - val_acc: 0.5833\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.5379 - acc: 0.5732 - val_loss: 1.2401 - val_acc: 0.6875\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.4142 - acc: 0.6068 - val_loss: 1.2221 - val_acc: 0.7292\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.3324 - acc: 0.6371 - val_loss: 1.1586 - val_acc: 0.7708\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2640 - acc: 0.6516 - val_loss: 1.1248 - val_acc: 0.7292\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.1887 - acc: 0.6756 - val_loss: 1.2963 - val_acc: 0.7708\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1522 - acc: 0.6861 - val_loss: 1.0541 - val_acc: 0.7292\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0964 - acc: 0.7082 - val_loss: 1.1747 - val_acc: 0.7708\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0305 - acc: 0.7216 - val_loss: 1.1476 - val_acc: 0.7500\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0139 - acc: 0.7283 - val_loss: 1.2007 - val_acc: 0.7500\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9769 - acc: 0.7435 - val_loss: 1.0846 - val_acc: 0.7917\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9215 - acc: 0.7584 - val_loss: 1.2041 - val_acc: 0.7500\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9100 - acc: 0.7571 - val_loss: 1.1111 - val_acc: 0.8125\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9111 - acc: 0.7511 - val_loss: 1.1862 - val_acc: 0.8333\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8655 - acc: 0.7708 - val_loss: 1.1507 - val_acc: 0.7500\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8442 - acc: 0.7784 - val_loss: 1.1525 - val_acc: 0.7917\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8230 - acc: 0.7839 - val_loss: 1.0657 - val_acc: 0.7500\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8172 - acc: 0.7857 - val_loss: 1.0023 - val_acc: 0.7500\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8088 - acc: 0.7885 - val_loss: 1.1037 - val_acc: 0.8125\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7768 - acc: 0.7967 - val_loss: 1.1211 - val_acc: 0.7917\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7611 - acc: 0.8124 - val_loss: 1.1422 - val_acc: 0.7708\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7438 - acc: 0.8026 - val_loss: 1.1713 - val_acc: 0.8333\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7036 - acc: 0.8211 - val_loss: 1.1043 - val_acc: 0.7708\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7165 - acc: 0.8196 - val_loss: 1.1575 - val_acc: 0.7500\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.6920 - acc: 0.8200 - val_loss: 1.1399 - val_acc: 0.7917\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.6927 - acc: 0.8177 - val_loss: 1.1433 - val_acc: 0.7917\n",
            "processing fold # 27\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 9s 2ms/step - loss: 2.9910 - acc: 0.2117 - val_loss: 2.8121 - val_acc: 0.1875\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.4632 - acc: 0.3278 - val_loss: 2.4297 - val_acc: 0.2500\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.1645 - acc: 0.4016 - val_loss: 2.0540 - val_acc: 0.3958\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.9449 - acc: 0.4746 - val_loss: 2.0137 - val_acc: 0.4583\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.7687 - acc: 0.5193 - val_loss: 1.7378 - val_acc: 0.5000\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.6006 - acc: 0.5578 - val_loss: 1.6238 - val_acc: 0.4792\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.5120 - acc: 0.5810 - val_loss: 1.6267 - val_acc: 0.5000\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4311 - acc: 0.6095 - val_loss: 1.5566 - val_acc: 0.5000\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.3274 - acc: 0.6417 - val_loss: 1.5320 - val_acc: 0.5625\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2571 - acc: 0.6566 - val_loss: 1.4333 - val_acc: 0.5625\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2032 - acc: 0.6787 - val_loss: 1.3885 - val_acc: 0.5833\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.1254 - acc: 0.6974 - val_loss: 1.4463 - val_acc: 0.6042\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0721 - acc: 0.7218 - val_loss: 1.2345 - val_acc: 0.6458\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0389 - acc: 0.7235 - val_loss: 1.2499 - val_acc: 0.6458\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0239 - acc: 0.7338 - val_loss: 1.3901 - val_acc: 0.6458\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9781 - acc: 0.7401 - val_loss: 1.2974 - val_acc: 0.6667\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9434 - acc: 0.7590 - val_loss: 1.3274 - val_acc: 0.6250\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9086 - acc: 0.7647 - val_loss: 1.2496 - val_acc: 0.6458\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9102 - acc: 0.7635 - val_loss: 1.2439 - val_acc: 0.6875\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8575 - acc: 0.7727 - val_loss: 1.3185 - val_acc: 0.6458\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8590 - acc: 0.7794 - val_loss: 1.1920 - val_acc: 0.7083\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8379 - acc: 0.7845 - val_loss: 1.1931 - val_acc: 0.6458\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8170 - acc: 0.7923 - val_loss: 1.2555 - val_acc: 0.6875\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7775 - acc: 0.7948 - val_loss: 1.2772 - val_acc: 0.6667\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7771 - acc: 0.7939 - val_loss: 1.1826 - val_acc: 0.6458\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7555 - acc: 0.8032 - val_loss: 1.2314 - val_acc: 0.7083\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7631 - acc: 0.8059 - val_loss: 1.2522 - val_acc: 0.6458\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7391 - acc: 0.8122 - val_loss: 1.2248 - val_acc: 0.6250\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7280 - acc: 0.8074 - val_loss: 1.1650 - val_acc: 0.6667\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.6979 - acc: 0.8196 - val_loss: 1.3042 - val_acc: 0.6667\n",
            "processing fold # 28\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 10s 2ms/step - loss: 2.8900 - acc: 0.2212 - val_loss: 2.4422 - val_acc: 0.3542\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.3790 - acc: 0.3595 - val_loss: 2.0934 - val_acc: 0.5000\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.1196 - acc: 0.4012 - val_loss: 2.0493 - val_acc: 0.4583\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.9002 - acc: 0.4767 - val_loss: 1.6935 - val_acc: 0.6250\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.7067 - acc: 0.5328 - val_loss: 1.5140 - val_acc: 0.6458\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.5822 - acc: 0.5662 - val_loss: 1.4524 - val_acc: 0.6667\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.4801 - acc: 0.5959 - val_loss: 1.4348 - val_acc: 0.6458\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.3769 - acc: 0.6198 - val_loss: 1.3266 - val_acc: 0.6875\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2806 - acc: 0.6423 - val_loss: 1.3004 - val_acc: 0.7083\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2169 - acc: 0.6737 - val_loss: 1.2561 - val_acc: 0.6667\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.1744 - acc: 0.6873 - val_loss: 1.2411 - val_acc: 0.7708\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.1207 - acc: 0.7012 - val_loss: 1.1881 - val_acc: 0.7500\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0710 - acc: 0.7117 - val_loss: 1.1625 - val_acc: 0.7708\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0296 - acc: 0.7277 - val_loss: 1.1374 - val_acc: 0.7500\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9776 - acc: 0.7466 - val_loss: 1.1401 - val_acc: 0.7500\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9601 - acc: 0.7468 - val_loss: 1.1434 - val_acc: 0.7292\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9396 - acc: 0.7561 - val_loss: 1.0649 - val_acc: 0.7708\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9182 - acc: 0.7593 - val_loss: 1.0089 - val_acc: 0.7708\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8836 - acc: 0.7685 - val_loss: 1.0978 - val_acc: 0.7708\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8520 - acc: 0.7786 - val_loss: 1.0514 - val_acc: 0.7708\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8392 - acc: 0.7813 - val_loss: 1.0650 - val_acc: 0.7500\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8196 - acc: 0.7860 - val_loss: 1.0418 - val_acc: 0.7917\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8080 - acc: 0.7927 - val_loss: 0.9784 - val_acc: 0.7917\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7800 - acc: 0.7956 - val_loss: 1.0058 - val_acc: 0.7708\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7829 - acc: 0.8003 - val_loss: 1.0516 - val_acc: 0.7708\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7682 - acc: 0.8059 - val_loss: 1.0013 - val_acc: 0.7708\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7193 - acc: 0.8089 - val_loss: 0.9935 - val_acc: 0.7917\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7392 - acc: 0.8045 - val_loss: 1.0107 - val_acc: 0.7917\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7128 - acc: 0.8173 - val_loss: 1.0600 - val_acc: 0.7917\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.6901 - acc: 0.8206 - val_loss: 0.9862 - val_acc: 0.7917\n",
            "processing fold # 29\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 10s 2ms/step - loss: 2.9964 - acc: 0.1962 - val_loss: 2.5871 - val_acc: 0.3333\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.4522 - acc: 0.3259 - val_loss: 2.2532 - val_acc: 0.4583\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.2015 - acc: 0.3982 - val_loss: 2.0325 - val_acc: 0.4167\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.9857 - acc: 0.4529 - val_loss: 1.8690 - val_acc: 0.5417\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.8086 - acc: 0.5053 - val_loss: 1.8821 - val_acc: 0.5208\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.6539 - acc: 0.5328 - val_loss: 1.6539 - val_acc: 0.6042\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.5454 - acc: 0.5692 - val_loss: 1.6155 - val_acc: 0.6042\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.4319 - acc: 0.6007 - val_loss: 1.6146 - val_acc: 0.6042\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3262 - acc: 0.6381 - val_loss: 1.5216 - val_acc: 0.6250\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2496 - acc: 0.6562 - val_loss: 1.4778 - val_acc: 0.6042\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2024 - acc: 0.6730 - val_loss: 1.4608 - val_acc: 0.6042\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.1634 - acc: 0.6749 - val_loss: 1.4248 - val_acc: 0.6667\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.1180 - acc: 0.6997 - val_loss: 1.3919 - val_acc: 0.6667\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0594 - acc: 0.7115 - val_loss: 1.4690 - val_acc: 0.6250\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0301 - acc: 0.7233 - val_loss: 1.3352 - val_acc: 0.7500\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9974 - acc: 0.7267 - val_loss: 1.3274 - val_acc: 0.6667\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9620 - acc: 0.7405 - val_loss: 1.3562 - val_acc: 0.6458\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9316 - acc: 0.7519 - val_loss: 1.3151 - val_acc: 0.6250\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9202 - acc: 0.7553 - val_loss: 1.2743 - val_acc: 0.6042\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8780 - acc: 0.7653 - val_loss: 1.3032 - val_acc: 0.6458\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8571 - acc: 0.7729 - val_loss: 1.2883 - val_acc: 0.6458\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8331 - acc: 0.7775 - val_loss: 1.2783 - val_acc: 0.6458\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8179 - acc: 0.7809 - val_loss: 1.2232 - val_acc: 0.6875\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7939 - acc: 0.7990 - val_loss: 1.2046 - val_acc: 0.7083\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7755 - acc: 0.7990 - val_loss: 1.3291 - val_acc: 0.6250\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7809 - acc: 0.7925 - val_loss: 1.2117 - val_acc: 0.6875\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7720 - acc: 0.7939 - val_loss: 1.2294 - val_acc: 0.6667\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7410 - acc: 0.8070 - val_loss: 1.2070 - val_acc: 0.6667\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7418 - acc: 0.8051 - val_loss: 1.2292 - val_acc: 0.6458\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7206 - acc: 0.8146 - val_loss: 1.2850 - val_acc: 0.6875\n",
            "processing fold # 30\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 10s 2ms/step - loss: 2.9525 - acc: 0.2319 - val_loss: 2.7048 - val_acc: 0.2083\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.4080 - acc: 0.3471 - val_loss: 2.2990 - val_acc: 0.3125\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.1164 - acc: 0.4142 - val_loss: 2.1015 - val_acc: 0.4167\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.9490 - acc: 0.4689 - val_loss: 1.9540 - val_acc: 0.4583\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.7582 - acc: 0.5116 - val_loss: 1.7206 - val_acc: 0.4375\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.6074 - acc: 0.5519 - val_loss: 1.6274 - val_acc: 0.4792\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.4954 - acc: 0.5898 - val_loss: 1.5822 - val_acc: 0.4792\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.4007 - acc: 0.6182 - val_loss: 1.4061 - val_acc: 0.5208\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3370 - acc: 0.6346 - val_loss: 1.5153 - val_acc: 0.5208\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2466 - acc: 0.6602 - val_loss: 1.4704 - val_acc: 0.5833\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.1808 - acc: 0.6859 - val_loss: 1.3439 - val_acc: 0.5417\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1280 - acc: 0.7004 - val_loss: 1.2603 - val_acc: 0.5208\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0980 - acc: 0.7044 - val_loss: 1.2976 - val_acc: 0.5417\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0328 - acc: 0.7166 - val_loss: 1.1813 - val_acc: 0.5833\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.0093 - acc: 0.7248 - val_loss: 1.1717 - val_acc: 0.6250\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9640 - acc: 0.7418 - val_loss: 1.2226 - val_acc: 0.6875\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9425 - acc: 0.7555 - val_loss: 1.2246 - val_acc: 0.5833\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9528 - acc: 0.7466 - val_loss: 1.1544 - val_acc: 0.6667\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9103 - acc: 0.7571 - val_loss: 1.1289 - val_acc: 0.6042\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8597 - acc: 0.7815 - val_loss: 1.0832 - val_acc: 0.6667\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8423 - acc: 0.7817 - val_loss: 1.0943 - val_acc: 0.6667\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8301 - acc: 0.7906 - val_loss: 1.0699 - val_acc: 0.6667\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8166 - acc: 0.7813 - val_loss: 1.1391 - val_acc: 0.6458\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7840 - acc: 0.7929 - val_loss: 1.1117 - val_acc: 0.6667\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7831 - acc: 0.7971 - val_loss: 1.1064 - val_acc: 0.6250\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7797 - acc: 0.7969 - val_loss: 1.0713 - val_acc: 0.7083\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7501 - acc: 0.8038 - val_loss: 1.0081 - val_acc: 0.6250\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7174 - acc: 0.8192 - val_loss: 1.0066 - val_acc: 0.7292\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7100 - acc: 0.8185 - val_loss: 1.0118 - val_acc: 0.7292\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.6925 - acc: 0.8244 - val_loss: 1.0003 - val_acc: 0.6667\n",
            "processing fold # 31\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 10s 2ms/step - loss: 2.9430 - acc: 0.1960 - val_loss: 2.7096 - val_acc: 0.3333\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.3973 - acc: 0.3457 - val_loss: 2.4555 - val_acc: 0.3333\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.1073 - acc: 0.4163 - val_loss: 2.1755 - val_acc: 0.5000\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.8744 - acc: 0.4798 - val_loss: 1.9374 - val_acc: 0.5833\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 1.6708 - acc: 0.5360 - val_loss: 1.7949 - val_acc: 0.5625\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.5612 - acc: 0.5761 - val_loss: 1.6244 - val_acc: 0.6458\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.4595 - acc: 0.6083 - val_loss: 1.4278 - val_acc: 0.7292\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3130 - acc: 0.6510 - val_loss: 1.3138 - val_acc: 0.8125\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2508 - acc: 0.6617 - val_loss: 1.2419 - val_acc: 0.7708\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.1924 - acc: 0.6794 - val_loss: 1.1007 - val_acc: 0.7708\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.1532 - acc: 0.6989 - val_loss: 1.0939 - val_acc: 0.7708\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0977 - acc: 0.7077 - val_loss: 1.1067 - val_acc: 0.7500\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0508 - acc: 0.7222 - val_loss: 0.9610 - val_acc: 0.7708\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0211 - acc: 0.7227 - val_loss: 0.8881 - val_acc: 0.8125\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9858 - acc: 0.7494 - val_loss: 0.8293 - val_acc: 0.8125\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9373 - acc: 0.7597 - val_loss: 0.8840 - val_acc: 0.7500\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9294 - acc: 0.7586 - val_loss: 0.8494 - val_acc: 0.8333\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8841 - acc: 0.7729 - val_loss: 0.8189 - val_acc: 0.7917\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8651 - acc: 0.7769 - val_loss: 0.7959 - val_acc: 0.7917\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8552 - acc: 0.7765 - val_loss: 0.7839 - val_acc: 0.7708\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8402 - acc: 0.7832 - val_loss: 0.7673 - val_acc: 0.7917\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8118 - acc: 0.7876 - val_loss: 0.7027 - val_acc: 0.8333\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7712 - acc: 0.8028 - val_loss: 0.7063 - val_acc: 0.7917\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7732 - acc: 0.7954 - val_loss: 0.6223 - val_acc: 0.8125\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7539 - acc: 0.8015 - val_loss: 0.6792 - val_acc: 0.7917\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7629 - acc: 0.8032 - val_loss: 0.6344 - val_acc: 0.8333\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7483 - acc: 0.8095 - val_loss: 0.6217 - val_acc: 0.8333\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7071 - acc: 0.8141 - val_loss: 0.5757 - val_acc: 0.8333\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7128 - acc: 0.8188 - val_loss: 0.5621 - val_acc: 0.8750\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.6874 - acc: 0.8217 - val_loss: 0.5740 - val_acc: 0.8750\n",
            "processing fold # 32\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 10s 2ms/step - loss: 2.8514 - acc: 0.2489 - val_loss: 2.5842 - val_acc: 0.2500\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.3483 - acc: 0.3486 - val_loss: 2.0452 - val_acc: 0.4583\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.0930 - acc: 0.4243 - val_loss: 1.6848 - val_acc: 0.5625\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.8586 - acc: 0.4929 - val_loss: 1.4869 - val_acc: 0.6458\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.6695 - acc: 0.5360 - val_loss: 1.3848 - val_acc: 0.6458\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.5297 - acc: 0.5824 - val_loss: 1.3218 - val_acc: 0.6042\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4461 - acc: 0.6020 - val_loss: 1.1065 - val_acc: 0.6875\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.3580 - acc: 0.6299 - val_loss: 1.0875 - val_acc: 0.7083\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2785 - acc: 0.6514 - val_loss: 1.0135 - val_acc: 0.6875\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2234 - acc: 0.6741 - val_loss: 0.9703 - val_acc: 0.7500\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1578 - acc: 0.6972 - val_loss: 1.0093 - val_acc: 0.7500\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1344 - acc: 0.6979 - val_loss: 0.9458 - val_acc: 0.7500\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0575 - acc: 0.7168 - val_loss: 0.9084 - val_acc: 0.7292\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0456 - acc: 0.7199 - val_loss: 0.9531 - val_acc: 0.7500\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9956 - acc: 0.7403 - val_loss: 0.9732 - val_acc: 0.7500\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9920 - acc: 0.7376 - val_loss: 0.9463 - val_acc: 0.7500\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9578 - acc: 0.7441 - val_loss: 0.8588 - val_acc: 0.8125\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.9165 - acc: 0.7643 - val_loss: 0.8804 - val_acc: 0.7917\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8754 - acc: 0.7721 - val_loss: 0.7928 - val_acc: 0.7500\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8756 - acc: 0.7668 - val_loss: 0.7899 - val_acc: 0.7917\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8430 - acc: 0.7809 - val_loss: 0.8050 - val_acc: 0.7917\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8168 - acc: 0.7908 - val_loss: 0.7571 - val_acc: 0.7708\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8056 - acc: 0.7971 - val_loss: 0.7392 - val_acc: 0.7917\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7787 - acc: 0.7986 - val_loss: 0.7200 - val_acc: 0.8542\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7601 - acc: 0.8017 - val_loss: 0.8079 - val_acc: 0.8125\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7414 - acc: 0.8055 - val_loss: 0.7811 - val_acc: 0.7917\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7339 - acc: 0.8131 - val_loss: 0.7469 - val_acc: 0.8333\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7063 - acc: 0.8192 - val_loss: 0.7902 - val_acc: 0.7917\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.6946 - acc: 0.8234 - val_loss: 0.7147 - val_acc: 0.8333\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.6962 - acc: 0.8215 - val_loss: 0.7167 - val_acc: 0.8125\n",
            "processing fold # 33\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 10s 2ms/step - loss: 2.9503 - acc: 0.2334 - val_loss: 2.8292 - val_acc: 0.1667\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.3971 - acc: 0.3629 - val_loss: 2.4531 - val_acc: 0.2292\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.1158 - acc: 0.4180 - val_loss: 2.4036 - val_acc: 0.2292\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.9080 - acc: 0.4760 - val_loss: 2.2292 - val_acc: 0.3542\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.7466 - acc: 0.5143 - val_loss: 2.0813 - val_acc: 0.4375\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.6187 - acc: 0.5572 - val_loss: 2.0669 - val_acc: 0.4583\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5184 - acc: 0.5795 - val_loss: 1.9439 - val_acc: 0.5000\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4420 - acc: 0.6028 - val_loss: 1.8651 - val_acc: 0.4792\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3486 - acc: 0.6310 - val_loss: 1.8299 - val_acc: 0.5625\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2678 - acc: 0.6554 - val_loss: 1.7259 - val_acc: 0.5625\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2339 - acc: 0.6642 - val_loss: 1.6550 - val_acc: 0.6458\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.1727 - acc: 0.6886 - val_loss: 1.7497 - val_acc: 0.5208\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.1178 - acc: 0.6972 - val_loss: 1.6386 - val_acc: 0.6250\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0906 - acc: 0.7054 - val_loss: 1.6854 - val_acc: 0.6458\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0403 - acc: 0.7218 - val_loss: 1.7339 - val_acc: 0.5833\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9946 - acc: 0.7355 - val_loss: 1.5995 - val_acc: 0.6875\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9620 - acc: 0.7445 - val_loss: 1.5350 - val_acc: 0.6667\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9393 - acc: 0.7466 - val_loss: 1.7246 - val_acc: 0.7083\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9259 - acc: 0.7578 - val_loss: 1.7804 - val_acc: 0.5625\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9142 - acc: 0.7508 - val_loss: 1.6409 - val_acc: 0.6875\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8746 - acc: 0.7721 - val_loss: 1.6415 - val_acc: 0.6667\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8638 - acc: 0.7668 - val_loss: 1.4945 - val_acc: 0.6875\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 26us/step - loss: 0.8318 - acc: 0.7828 - val_loss: 1.4944 - val_acc: 0.6875\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8208 - acc: 0.7855 - val_loss: 1.5782 - val_acc: 0.6875\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8001 - acc: 0.7954 - val_loss: 1.5529 - val_acc: 0.7083\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7904 - acc: 0.7918 - val_loss: 1.5896 - val_acc: 0.6875\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7572 - acc: 0.8049 - val_loss: 1.4940 - val_acc: 0.6875\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7435 - acc: 0.8047 - val_loss: 1.5488 - val_acc: 0.6875\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7233 - acc: 0.8053 - val_loss: 1.5259 - val_acc: 0.7083\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 33us/step - loss: 0.7211 - acc: 0.8103 - val_loss: 1.4010 - val_acc: 0.7708\n",
            "processing fold # 34\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 10s 2ms/step - loss: 3.0217 - acc: 0.2019 - val_loss: 2.4186 - val_acc: 0.3333\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.5348 - acc: 0.3032 - val_loss: 2.1538 - val_acc: 0.4375\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 2.2289 - acc: 0.3684 - val_loss: 1.9621 - val_acc: 0.5000\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.9864 - acc: 0.4407 - val_loss: 1.7969 - val_acc: 0.6042\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.7929 - acc: 0.4947 - val_loss: 1.8182 - val_acc: 0.6042\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.6651 - acc: 0.5412 - val_loss: 1.6059 - val_acc: 0.7083\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.5438 - acc: 0.5637 - val_loss: 1.5789 - val_acc: 0.6667\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4387 - acc: 0.6037 - val_loss: 1.4494 - val_acc: 0.6875\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3789 - acc: 0.6203 - val_loss: 1.3661 - val_acc: 0.7292\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2899 - acc: 0.6571 - val_loss: 1.4695 - val_acc: 0.6875\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2518 - acc: 0.6619 - val_loss: 1.3515 - val_acc: 0.7500\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.1767 - acc: 0.6867 - val_loss: 1.3520 - val_acc: 0.7708\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1284 - acc: 0.6991 - val_loss: 1.3804 - val_acc: 0.7500\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0947 - acc: 0.7069 - val_loss: 1.3839 - val_acc: 0.7708\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0587 - acc: 0.7178 - val_loss: 1.2882 - val_acc: 0.7708\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0268 - acc: 0.7300 - val_loss: 1.3399 - val_acc: 0.7500\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9872 - acc: 0.7429 - val_loss: 1.3338 - val_acc: 0.7917\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9436 - acc: 0.7508 - val_loss: 1.3551 - val_acc: 0.7708\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9245 - acc: 0.7540 - val_loss: 1.3508 - val_acc: 0.7708\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8983 - acc: 0.7643 - val_loss: 1.3064 - val_acc: 0.8125\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8799 - acc: 0.7643 - val_loss: 1.2876 - val_acc: 0.7917\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8544 - acc: 0.7742 - val_loss: 1.3440 - val_acc: 0.7917\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8401 - acc: 0.7832 - val_loss: 1.2466 - val_acc: 0.7917\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8200 - acc: 0.7834 - val_loss: 1.1989 - val_acc: 0.8125\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8055 - acc: 0.7937 - val_loss: 1.2298 - val_acc: 0.7917\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7658 - acc: 0.7967 - val_loss: 1.2199 - val_acc: 0.8125\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7636 - acc: 0.8032 - val_loss: 1.2667 - val_acc: 0.8125\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7474 - acc: 0.8091 - val_loss: 1.2473 - val_acc: 0.8125\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7281 - acc: 0.8087 - val_loss: 1.1912 - val_acc: 0.8333\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7148 - acc: 0.8114 - val_loss: 1.2422 - val_acc: 0.8333\n",
            "processing fold # 35\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 11s 2ms/step - loss: 2.9840 - acc: 0.2241 - val_loss: 2.5623 - val_acc: 0.3125\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.4550 - acc: 0.3370 - val_loss: 2.1371 - val_acc: 0.3333\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.1498 - acc: 0.4012 - val_loss: 1.9455 - val_acc: 0.5208\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.9386 - acc: 0.4695 - val_loss: 1.7430 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.7654 - acc: 0.5139 - val_loss: 1.5629 - val_acc: 0.5000\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.6038 - acc: 0.5624 - val_loss: 1.4864 - val_acc: 0.5417\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5150 - acc: 0.5900 - val_loss: 1.4144 - val_acc: 0.6667\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.4120 - acc: 0.6226 - val_loss: 1.2400 - val_acc: 0.7500\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3509 - acc: 0.6405 - val_loss: 1.2426 - val_acc: 0.7500\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2786 - acc: 0.6619 - val_loss: 1.2783 - val_acc: 0.6875\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2245 - acc: 0.6703 - val_loss: 1.1890 - val_acc: 0.7500\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1587 - acc: 0.6955 - val_loss: 1.1708 - val_acc: 0.7708\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1151 - acc: 0.7086 - val_loss: 1.0871 - val_acc: 0.8125\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0655 - acc: 0.7149 - val_loss: 1.0352 - val_acc: 0.8542\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0220 - acc: 0.7330 - val_loss: 1.0447 - val_acc: 0.7708\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0017 - acc: 0.7424 - val_loss: 1.0430 - val_acc: 0.8333\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9697 - acc: 0.7424 - val_loss: 0.9975 - val_acc: 0.8750\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9451 - acc: 0.7429 - val_loss: 0.9735 - val_acc: 0.8542\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9078 - acc: 0.7651 - val_loss: 1.0281 - val_acc: 0.8542\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9040 - acc: 0.7630 - val_loss: 0.9386 - val_acc: 0.8750\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8791 - acc: 0.7702 - val_loss: 0.8780 - val_acc: 0.8333\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8265 - acc: 0.7885 - val_loss: 1.0010 - val_acc: 0.8333\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8396 - acc: 0.7836 - val_loss: 0.9448 - val_acc: 0.8333\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8197 - acc: 0.7790 - val_loss: 0.9786 - val_acc: 0.8333\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7955 - acc: 0.7904 - val_loss: 0.9806 - val_acc: 0.8542\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7902 - acc: 0.7992 - val_loss: 0.8920 - val_acc: 0.8333\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 19us/step - loss: 0.7709 - acc: 0.7971 - val_loss: 1.0205 - val_acc: 0.8125\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7566 - acc: 0.8061 - val_loss: 0.9226 - val_acc: 0.8542\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7502 - acc: 0.8057 - val_loss: 0.9742 - val_acc: 0.8333\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7300 - acc: 0.8047 - val_loss: 1.0079 - val_acc: 0.8333\n",
            "processing fold # 36\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 11s 2ms/step - loss: 2.9561 - acc: 0.2161 - val_loss: 2.4411 - val_acc: 0.3750\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.3312 - acc: 0.3684 - val_loss: 2.1989 - val_acc: 0.3333\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.1044 - acc: 0.4172 - val_loss: 1.9669 - val_acc: 0.4167\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.8706 - acc: 0.4804 - val_loss: 1.6369 - val_acc: 0.5208\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.7264 - acc: 0.5267 - val_loss: 1.6664 - val_acc: 0.5833\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.6078 - acc: 0.5471 - val_loss: 1.5813 - val_acc: 0.5625\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4710 - acc: 0.5982 - val_loss: 1.5155 - val_acc: 0.5625\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3902 - acc: 0.6201 - val_loss: 1.4750 - val_acc: 0.5833\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2928 - acc: 0.6482 - val_loss: 1.4538 - val_acc: 0.6458\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2213 - acc: 0.6749 - val_loss: 1.4011 - val_acc: 0.6250\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1914 - acc: 0.6806 - val_loss: 1.3740 - val_acc: 0.6875\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1186 - acc: 0.7006 - val_loss: 1.4406 - val_acc: 0.6667\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0715 - acc: 0.7096 - val_loss: 1.4044 - val_acc: 0.7083\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0220 - acc: 0.7347 - val_loss: 1.4648 - val_acc: 0.7500\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9906 - acc: 0.7347 - val_loss: 1.3196 - val_acc: 0.7292\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9548 - acc: 0.7496 - val_loss: 1.3877 - val_acc: 0.7500\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9196 - acc: 0.7601 - val_loss: 1.3387 - val_acc: 0.7500\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8902 - acc: 0.7700 - val_loss: 1.3787 - val_acc: 0.7083\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8755 - acc: 0.7765 - val_loss: 1.3797 - val_acc: 0.7083\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8564 - acc: 0.7763 - val_loss: 1.3153 - val_acc: 0.7500\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8373 - acc: 0.7767 - val_loss: 1.2804 - val_acc: 0.7083\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8110 - acc: 0.7843 - val_loss: 1.3350 - val_acc: 0.7083\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7960 - acc: 0.7965 - val_loss: 1.3307 - val_acc: 0.7083\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7852 - acc: 0.7986 - val_loss: 1.4751 - val_acc: 0.6875\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7743 - acc: 0.8066 - val_loss: 1.3974 - val_acc: 0.7708\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7531 - acc: 0.8017 - val_loss: 1.3214 - val_acc: 0.7292\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7357 - acc: 0.8135 - val_loss: 1.2469 - val_acc: 0.7708\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7057 - acc: 0.8148 - val_loss: 1.3711 - val_acc: 0.6875\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7123 - acc: 0.8148 - val_loss: 1.3757 - val_acc: 0.7708\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7036 - acc: 0.8148 - val_loss: 1.4464 - val_acc: 0.7083\n",
            "processing fold # 37\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 11s 2ms/step - loss: 2.9657 - acc: 0.2250 - val_loss: 2.5227 - val_acc: 0.2292\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.4151 - acc: 0.3188 - val_loss: 2.0726 - val_acc: 0.4792\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.0927 - acc: 0.4161 - val_loss: 1.7553 - val_acc: 0.5833\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.8841 - acc: 0.4767 - val_loss: 1.6200 - val_acc: 0.5417\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.6824 - acc: 0.5343 - val_loss: 1.5505 - val_acc: 0.5625\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.5625 - acc: 0.5673 - val_loss: 1.4341 - val_acc: 0.6458\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4493 - acc: 0.6016 - val_loss: 1.3428 - val_acc: 0.6875\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.3877 - acc: 0.6133 - val_loss: 1.3781 - val_acc: 0.6042\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2986 - acc: 0.6390 - val_loss: 1.2520 - val_acc: 0.6458\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2165 - acc: 0.6726 - val_loss: 1.2062 - val_acc: 0.7083\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1625 - acc: 0.6892 - val_loss: 1.1382 - val_acc: 0.7292\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1275 - acc: 0.7006 - val_loss: 1.0677 - val_acc: 0.7292\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0941 - acc: 0.7048 - val_loss: 1.1831 - val_acc: 0.7500\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0533 - acc: 0.7069 - val_loss: 1.1440 - val_acc: 0.7708\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0056 - acc: 0.7407 - val_loss: 1.1200 - val_acc: 0.7500\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9793 - acc: 0.7344 - val_loss: 1.0859 - val_acc: 0.7917\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9467 - acc: 0.7542 - val_loss: 1.0661 - val_acc: 0.7083\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9079 - acc: 0.7643 - val_loss: 0.9505 - val_acc: 0.7917\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8942 - acc: 0.7668 - val_loss: 1.0401 - val_acc: 0.7708\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8531 - acc: 0.7778 - val_loss: 1.0657 - val_acc: 0.7500\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8411 - acc: 0.7801 - val_loss: 0.9854 - val_acc: 0.7292\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8473 - acc: 0.7799 - val_loss: 0.9997 - val_acc: 0.7917\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8240 - acc: 0.7864 - val_loss: 0.9486 - val_acc: 0.7917\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7838 - acc: 0.7965 - val_loss: 1.0128 - val_acc: 0.7500\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7742 - acc: 0.7981 - val_loss: 0.9799 - val_acc: 0.8333\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7672 - acc: 0.8024 - val_loss: 1.0612 - val_acc: 0.7500\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7609 - acc: 0.7992 - val_loss: 0.8991 - val_acc: 0.8125\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7294 - acc: 0.8158 - val_loss: 0.9259 - val_acc: 0.7708\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7308 - acc: 0.8169 - val_loss: 0.9120 - val_acc: 0.7917\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.6919 - acc: 0.8242 - val_loss: 0.8985 - val_acc: 0.7500\n",
            "processing fold # 38\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 11s 2ms/step - loss: 2.8927 - acc: 0.2180 - val_loss: 2.1271 - val_acc: 0.5000\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.3281 - acc: 0.3543 - val_loss: 1.6533 - val_acc: 0.5625\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.0607 - acc: 0.4247 - val_loss: 1.4843 - val_acc: 0.6250\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.8449 - acc: 0.4933 - val_loss: 1.4205 - val_acc: 0.5625\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.6631 - acc: 0.5395 - val_loss: 1.2651 - val_acc: 0.6250\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5259 - acc: 0.5778 - val_loss: 1.2067 - val_acc: 0.6250\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4081 - acc: 0.6083 - val_loss: 1.0808 - val_acc: 0.6458\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3235 - acc: 0.6362 - val_loss: 1.0901 - val_acc: 0.6667\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2756 - acc: 0.6489 - val_loss: 1.0732 - val_acc: 0.7083\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.2189 - acc: 0.6754 - val_loss: 1.0814 - val_acc: 0.6667\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1594 - acc: 0.6808 - val_loss: 0.9455 - val_acc: 0.7083\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0923 - acc: 0.7092 - val_loss: 0.8706 - val_acc: 0.7292\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0528 - acc: 0.7204 - val_loss: 0.8562 - val_acc: 0.7917\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0132 - acc: 0.7277 - val_loss: 0.8204 - val_acc: 0.7500\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9934 - acc: 0.7376 - val_loss: 0.9376 - val_acc: 0.6875\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9571 - acc: 0.7502 - val_loss: 0.8472 - val_acc: 0.7292\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9290 - acc: 0.7534 - val_loss: 0.7586 - val_acc: 0.7917\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8899 - acc: 0.7719 - val_loss: 0.8040 - val_acc: 0.7500\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8804 - acc: 0.7649 - val_loss: 0.8131 - val_acc: 0.7917\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8538 - acc: 0.7771 - val_loss: 0.6778 - val_acc: 0.7917\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8476 - acc: 0.7735 - val_loss: 0.7360 - val_acc: 0.8333\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8301 - acc: 0.7696 - val_loss: 0.7302 - val_acc: 0.7917\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7893 - acc: 0.7954 - val_loss: 0.6653 - val_acc: 0.7917\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7759 - acc: 0.7998 - val_loss: 0.6616 - val_acc: 0.7917\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7433 - acc: 0.8089 - val_loss: 0.8009 - val_acc: 0.7708\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7481 - acc: 0.8118 - val_loss: 0.6788 - val_acc: 0.8125\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7321 - acc: 0.8141 - val_loss: 0.6593 - val_acc: 0.8333\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7254 - acc: 0.8101 - val_loss: 0.6662 - val_acc: 0.8333\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7155 - acc: 0.8185 - val_loss: 0.6511 - val_acc: 0.8125\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7090 - acc: 0.8179 - val_loss: 0.6451 - val_acc: 0.7917\n",
            "processing fold # 39\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 12s 2ms/step - loss: 3.0608 - acc: 0.1941 - val_loss: 2.4317 - val_acc: 0.3333\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.4932 - acc: 0.3177 - val_loss: 2.1779 - val_acc: 0.4167\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.2358 - acc: 0.3949 - val_loss: 1.8441 - val_acc: 0.5417\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.0106 - acc: 0.4531 - val_loss: 1.7167 - val_acc: 0.5417\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.8454 - acc: 0.4958 - val_loss: 1.4623 - val_acc: 0.6458\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.6770 - acc: 0.5427 - val_loss: 1.3431 - val_acc: 0.6667\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5412 - acc: 0.5795 - val_loss: 1.1982 - val_acc: 0.6875\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.4723 - acc: 0.5950 - val_loss: 1.0701 - val_acc: 0.6875\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3597 - acc: 0.6312 - val_loss: 1.1397 - val_acc: 0.7083\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2965 - acc: 0.6550 - val_loss: 1.0609 - val_acc: 0.7292\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.2285 - acc: 0.6730 - val_loss: 0.9771 - val_acc: 0.7292\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1606 - acc: 0.6894 - val_loss: 1.0335 - val_acc: 0.6875\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1092 - acc: 0.7027 - val_loss: 0.9226 - val_acc: 0.7500\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0848 - acc: 0.7140 - val_loss: 0.9545 - val_acc: 0.7708\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0294 - acc: 0.7269 - val_loss: 0.9466 - val_acc: 0.7500\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0154 - acc: 0.7334 - val_loss: 0.9673 - val_acc: 0.7708\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9790 - acc: 0.7477 - val_loss: 0.9225 - val_acc: 0.7708\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9393 - acc: 0.7561 - val_loss: 0.8987 - val_acc: 0.7292\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9270 - acc: 0.7574 - val_loss: 0.9253 - val_acc: 0.7500\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9021 - acc: 0.7630 - val_loss: 0.8684 - val_acc: 0.7500\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8840 - acc: 0.7649 - val_loss: 0.9298 - val_acc: 0.7083\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8502 - acc: 0.7811 - val_loss: 0.8669 - val_acc: 0.7708\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8237 - acc: 0.7828 - val_loss: 0.8210 - val_acc: 0.7083\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7949 - acc: 0.7910 - val_loss: 0.8189 - val_acc: 0.7917\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8020 - acc: 0.7948 - val_loss: 0.8901 - val_acc: 0.7708\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7825 - acc: 0.7958 - val_loss: 0.8519 - val_acc: 0.7500\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7900 - acc: 0.7954 - val_loss: 0.8926 - val_acc: 0.7917\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7578 - acc: 0.8005 - val_loss: 0.8762 - val_acc: 0.7500\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7437 - acc: 0.8095 - val_loss: 0.8085 - val_acc: 0.7708\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7187 - acc: 0.8167 - val_loss: 0.8492 - val_acc: 0.7500\n",
            "processing fold # 40\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 12s 3ms/step - loss: 2.8960 - acc: 0.2187 - val_loss: 2.8087 - val_acc: 0.2292\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.3452 - acc: 0.3520 - val_loss: 2.4382 - val_acc: 0.3333\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.0680 - acc: 0.4254 - val_loss: 2.0998 - val_acc: 0.4167\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.8215 - acc: 0.4998 - val_loss: 1.9692 - val_acc: 0.4583\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.6510 - acc: 0.5414 - val_loss: 1.8806 - val_acc: 0.5417\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.5378 - acc: 0.5700 - val_loss: 1.7282 - val_acc: 0.5417\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4330 - acc: 0.6093 - val_loss: 1.6417 - val_acc: 0.5625\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3413 - acc: 0.6386 - val_loss: 1.5990 - val_acc: 0.5833\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2622 - acc: 0.6493 - val_loss: 1.5398 - val_acc: 0.5625\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2036 - acc: 0.6722 - val_loss: 1.4930 - val_acc: 0.6250\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1475 - acc: 0.6909 - val_loss: 1.2882 - val_acc: 0.5833\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0918 - acc: 0.7035 - val_loss: 1.4666 - val_acc: 0.6667\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0364 - acc: 0.7286 - val_loss: 1.4670 - val_acc: 0.6458\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9878 - acc: 0.7399 - val_loss: 1.3253 - val_acc: 0.6458\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9904 - acc: 0.7328 - val_loss: 1.4779 - val_acc: 0.6042\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9707 - acc: 0.7433 - val_loss: 1.2965 - val_acc: 0.7083\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9312 - acc: 0.7519 - val_loss: 1.2725 - val_acc: 0.6875\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8870 - acc: 0.7677 - val_loss: 1.4977 - val_acc: 0.6250\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8783 - acc: 0.7714 - val_loss: 1.4453 - val_acc: 0.6250\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8275 - acc: 0.7794 - val_loss: 1.3961 - val_acc: 0.6458\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8144 - acc: 0.7916 - val_loss: 1.3871 - val_acc: 0.6042\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7877 - acc: 0.7893 - val_loss: 1.4830 - val_acc: 0.6250\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8027 - acc: 0.7916 - val_loss: 1.4987 - val_acc: 0.5833\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7711 - acc: 0.8047 - val_loss: 1.4403 - val_acc: 0.6458\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7596 - acc: 0.8055 - val_loss: 1.3962 - val_acc: 0.6875\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7486 - acc: 0.8063 - val_loss: 1.4304 - val_acc: 0.6250\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7272 - acc: 0.8061 - val_loss: 1.4012 - val_acc: 0.6875\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7255 - acc: 0.8156 - val_loss: 1.4111 - val_acc: 0.6458\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7262 - acc: 0.8053 - val_loss: 1.4888 - val_acc: 0.6458\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.6962 - acc: 0.8213 - val_loss: 1.4601 - val_acc: 0.6042\n",
            "processing fold # 41\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 12s 3ms/step - loss: 3.0040 - acc: 0.2088 - val_loss: 2.7170 - val_acc: 0.2917\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.4447 - acc: 0.3242 - val_loss: 2.4716 - val_acc: 0.2917\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.1946 - acc: 0.3858 - val_loss: 2.3482 - val_acc: 0.2917\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.9931 - acc: 0.4432 - val_loss: 2.0935 - val_acc: 0.3750\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.8276 - acc: 0.4914 - val_loss: 1.9578 - val_acc: 0.4375\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.6783 - acc: 0.5393 - val_loss: 1.9541 - val_acc: 0.4167\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.5547 - acc: 0.5740 - val_loss: 1.8103 - val_acc: 0.4792\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4509 - acc: 0.5927 - val_loss: 1.7431 - val_acc: 0.5625\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3797 - acc: 0.6188 - val_loss: 1.6858 - val_acc: 0.5417\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2882 - acc: 0.6480 - val_loss: 1.6964 - val_acc: 0.5833\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2372 - acc: 0.6592 - val_loss: 1.6157 - val_acc: 0.6042\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1742 - acc: 0.6833 - val_loss: 1.5618 - val_acc: 0.6458\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1293 - acc: 0.6941 - val_loss: 1.5067 - val_acc: 0.6667\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0769 - acc: 0.7109 - val_loss: 1.4870 - val_acc: 0.6458\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0468 - acc: 0.7208 - val_loss: 1.5116 - val_acc: 0.7083\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0133 - acc: 0.7313 - val_loss: 1.4629 - val_acc: 0.6667\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9786 - acc: 0.7420 - val_loss: 1.3994 - val_acc: 0.7292\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9307 - acc: 0.7569 - val_loss: 1.3911 - val_acc: 0.7083\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9207 - acc: 0.7595 - val_loss: 1.3744 - val_acc: 0.7500\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9203 - acc: 0.7624 - val_loss: 1.3885 - val_acc: 0.7292\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8687 - acc: 0.7714 - val_loss: 1.4523 - val_acc: 0.7292\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8628 - acc: 0.7750 - val_loss: 1.3925 - val_acc: 0.7500\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8633 - acc: 0.7708 - val_loss: 1.3343 - val_acc: 0.7500\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8258 - acc: 0.7780 - val_loss: 1.3965 - val_acc: 0.6875\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8059 - acc: 0.7902 - val_loss: 1.2989 - val_acc: 0.7292\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 30us/step - loss: 0.8022 - acc: 0.7906 - val_loss: 1.3787 - val_acc: 0.6458\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 26us/step - loss: 0.7593 - acc: 0.8066 - val_loss: 1.3968 - val_acc: 0.7292\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 26us/step - loss: 0.7544 - acc: 0.8030 - val_loss: 1.3792 - val_acc: 0.7083\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7381 - acc: 0.8036 - val_loss: 1.3764 - val_acc: 0.6875\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7334 - acc: 0.8116 - val_loss: 1.2548 - val_acc: 0.7292\n",
            "processing fold # 42\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 12s 3ms/step - loss: 3.0143 - acc: 0.2101 - val_loss: 2.6413 - val_acc: 0.2500\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.5244 - acc: 0.3122 - val_loss: 2.2988 - val_acc: 0.3958\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.2233 - acc: 0.3953 - val_loss: 2.1155 - val_acc: 0.3542\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.0368 - acc: 0.4405 - val_loss: 2.0156 - val_acc: 0.4375\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.8818 - acc: 0.4828 - val_loss: 1.8311 - val_acc: 0.5417\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.7225 - acc: 0.5370 - val_loss: 1.6809 - val_acc: 0.5417\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.6044 - acc: 0.5593 - val_loss: 1.5412 - val_acc: 0.5833\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5068 - acc: 0.5833 - val_loss: 1.4976 - val_acc: 0.6250\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.3986 - acc: 0.6171 - val_loss: 1.5599 - val_acc: 0.6042\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3379 - acc: 0.6337 - val_loss: 1.5215 - val_acc: 0.6250\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2590 - acc: 0.6672 - val_loss: 1.3992 - val_acc: 0.6250\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1944 - acc: 0.6852 - val_loss: 1.3322 - val_acc: 0.6667\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.1474 - acc: 0.6932 - val_loss: 1.3261 - val_acc: 0.6458\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1175 - acc: 0.7088 - val_loss: 1.3081 - val_acc: 0.6667\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0464 - acc: 0.7237 - val_loss: 1.2852 - val_acc: 0.7083\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0270 - acc: 0.7365 - val_loss: 1.1944 - val_acc: 0.7292\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0069 - acc: 0.7351 - val_loss: 1.2657 - val_acc: 0.6667\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9590 - acc: 0.7511 - val_loss: 1.2988 - val_acc: 0.7083\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9277 - acc: 0.7677 - val_loss: 1.2726 - val_acc: 0.7292\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9193 - acc: 0.7588 - val_loss: 1.2995 - val_acc: 0.6458\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8976 - acc: 0.7658 - val_loss: 1.2418 - val_acc: 0.7292\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8634 - acc: 0.7775 - val_loss: 1.2591 - val_acc: 0.7083\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8465 - acc: 0.7794 - val_loss: 1.2893 - val_acc: 0.6875\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8387 - acc: 0.7830 - val_loss: 1.2830 - val_acc: 0.7083\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8178 - acc: 0.7836 - val_loss: 1.2778 - val_acc: 0.7083\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8050 - acc: 0.7889 - val_loss: 1.2586 - val_acc: 0.7083\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7855 - acc: 0.7927 - val_loss: 1.2873 - val_acc: 0.6667\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7533 - acc: 0.8042 - val_loss: 1.2732 - val_acc: 0.6875\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7445 - acc: 0.8053 - val_loss: 1.1961 - val_acc: 0.7083\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7343 - acc: 0.8051 - val_loss: 1.1818 - val_acc: 0.7292\n",
            "processing fold # 43\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 13s 3ms/step - loss: 2.9969 - acc: 0.1930 - val_loss: 2.3399 - val_acc: 0.3958\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.4131 - acc: 0.3511 - val_loss: 2.1500 - val_acc: 0.4375\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.1258 - acc: 0.4251 - val_loss: 2.0547 - val_acc: 0.4375\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.8991 - acc: 0.4790 - val_loss: 1.8659 - val_acc: 0.4167\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.7136 - acc: 0.5290 - val_loss: 1.6266 - val_acc: 0.5417\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.5838 - acc: 0.5673 - val_loss: 1.5497 - val_acc: 0.6042\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4837 - acc: 0.5929 - val_loss: 1.4620 - val_acc: 0.6042\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3954 - acc: 0.6207 - val_loss: 1.4223 - val_acc: 0.5833\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2975 - acc: 0.6495 - val_loss: 1.3667 - val_acc: 0.5625\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2242 - acc: 0.6766 - val_loss: 1.3977 - val_acc: 0.5833\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1728 - acc: 0.6876 - val_loss: 1.3337 - val_acc: 0.6458\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1574 - acc: 0.6949 - val_loss: 1.1806 - val_acc: 0.6875\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0892 - acc: 0.7157 - val_loss: 1.2606 - val_acc: 0.6250\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0442 - acc: 0.7187 - val_loss: 1.2037 - val_acc: 0.6458\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0057 - acc: 0.7391 - val_loss: 1.1273 - val_acc: 0.7083\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9760 - acc: 0.7454 - val_loss: 1.1723 - val_acc: 0.6875\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9544 - acc: 0.7517 - val_loss: 1.0907 - val_acc: 0.7292\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9173 - acc: 0.7571 - val_loss: 1.0636 - val_acc: 0.7083\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8983 - acc: 0.7649 - val_loss: 1.0462 - val_acc: 0.7083\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8768 - acc: 0.7721 - val_loss: 0.9880 - val_acc: 0.7083\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8362 - acc: 0.7807 - val_loss: 1.0251 - val_acc: 0.7500\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8334 - acc: 0.7809 - val_loss: 0.9619 - val_acc: 0.7292\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7945 - acc: 0.7960 - val_loss: 0.9978 - val_acc: 0.7083\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7931 - acc: 0.8047 - val_loss: 0.9798 - val_acc: 0.6875\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7867 - acc: 0.8003 - val_loss: 1.0127 - val_acc: 0.6875\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7552 - acc: 0.8045 - val_loss: 0.9278 - val_acc: 0.7292\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7470 - acc: 0.8110 - val_loss: 0.9032 - val_acc: 0.7500\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7272 - acc: 0.8131 - val_loss: 0.8885 - val_acc: 0.7500\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7245 - acc: 0.8112 - val_loss: 0.8340 - val_acc: 0.7500\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7205 - acc: 0.8131 - val_loss: 0.7612 - val_acc: 0.7917\n",
            "processing fold # 44\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 13s 3ms/step - loss: 2.9146 - acc: 0.2166 - val_loss: 2.5067 - val_acc: 0.3333\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.3958 - acc: 0.3246 - val_loss: 2.2372 - val_acc: 0.3542\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.1220 - acc: 0.3930 - val_loss: 1.7600 - val_acc: 0.5833\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.8496 - acc: 0.4880 - val_loss: 1.5707 - val_acc: 0.5417\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.6971 - acc: 0.5263 - val_loss: 1.5171 - val_acc: 0.5417\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5730 - acc: 0.5561 - val_loss: 1.3319 - val_acc: 0.5625\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4555 - acc: 0.5963 - val_loss: 1.3098 - val_acc: 0.6042\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.3513 - acc: 0.6220 - val_loss: 1.2174 - val_acc: 0.6250\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3001 - acc: 0.6373 - val_loss: 1.2028 - val_acc: 0.6667\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2194 - acc: 0.6693 - val_loss: 1.1537 - val_acc: 0.6875\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1651 - acc: 0.6888 - val_loss: 1.0936 - val_acc: 0.7083\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1192 - acc: 0.6937 - val_loss: 1.1486 - val_acc: 0.6667\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0486 - acc: 0.7204 - val_loss: 1.1605 - val_acc: 0.6667\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0206 - acc: 0.7262 - val_loss: 1.0342 - val_acc: 0.7708\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9899 - acc: 0.7386 - val_loss: 1.0752 - val_acc: 0.6875\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9601 - acc: 0.7422 - val_loss: 1.0779 - val_acc: 0.7083\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 31us/step - loss: 0.9227 - acc: 0.7502 - val_loss: 1.1355 - val_acc: 0.7292\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9308 - acc: 0.7559 - val_loss: 1.0864 - val_acc: 0.7500\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8950 - acc: 0.7635 - val_loss: 1.0596 - val_acc: 0.7083\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8540 - acc: 0.7687 - val_loss: 1.0519 - val_acc: 0.7708\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8342 - acc: 0.7796 - val_loss: 1.1183 - val_acc: 0.7083\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8219 - acc: 0.7878 - val_loss: 1.0320 - val_acc: 0.7083\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7974 - acc: 0.7925 - val_loss: 1.0120 - val_acc: 0.7292\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7612 - acc: 0.8019 - val_loss: 1.0420 - val_acc: 0.7917\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7690 - acc: 0.7939 - val_loss: 1.0478 - val_acc: 0.7917\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7301 - acc: 0.8103 - val_loss: 1.0259 - val_acc: 0.7500\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7395 - acc: 0.8003 - val_loss: 0.9802 - val_acc: 0.7917\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7181 - acc: 0.8150 - val_loss: 0.9801 - val_acc: 0.7708\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7128 - acc: 0.8091 - val_loss: 1.1724 - val_acc: 0.7500\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.6997 - acc: 0.8202 - val_loss: 1.1576 - val_acc: 0.7500\n",
            "processing fold # 45\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 13s 3ms/step - loss: 2.9259 - acc: 0.2208 - val_loss: 2.5341 - val_acc: 0.3958\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.4224 - acc: 0.3434 - val_loss: 2.1337 - val_acc: 0.4375\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.1649 - acc: 0.4052 - val_loss: 1.8679 - val_acc: 0.4583\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.9628 - acc: 0.4481 - val_loss: 1.7708 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.7771 - acc: 0.5088 - val_loss: 1.6153 - val_acc: 0.5417\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.6373 - acc: 0.5500 - val_loss: 1.5874 - val_acc: 0.6458\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5339 - acc: 0.5740 - val_loss: 1.5138 - val_acc: 0.6250\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4107 - acc: 0.6093 - val_loss: 1.3748 - val_acc: 0.6458\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3582 - acc: 0.6293 - val_loss: 1.3796 - val_acc: 0.6458\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2819 - acc: 0.6402 - val_loss: 1.3677 - val_acc: 0.6667\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2511 - acc: 0.6495 - val_loss: 1.2547 - val_acc: 0.7083\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1803 - acc: 0.6789 - val_loss: 1.1780 - val_acc: 0.7500\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1249 - acc: 0.7010 - val_loss: 1.2027 - val_acc: 0.7500\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0826 - acc: 0.7056 - val_loss: 1.2394 - val_acc: 0.7083\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0516 - acc: 0.7117 - val_loss: 1.1223 - val_acc: 0.7500\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0206 - acc: 0.7281 - val_loss: 1.1514 - val_acc: 0.7292\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0069 - acc: 0.7384 - val_loss: 1.1220 - val_acc: 0.7292\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9750 - acc: 0.7452 - val_loss: 1.1529 - val_acc: 0.7500\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9295 - acc: 0.7529 - val_loss: 1.0811 - val_acc: 0.7708\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9027 - acc: 0.7639 - val_loss: 1.0653 - val_acc: 0.7917\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8826 - acc: 0.7656 - val_loss: 1.0707 - val_acc: 0.7917\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8640 - acc: 0.7683 - val_loss: 1.0474 - val_acc: 0.7708\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8507 - acc: 0.7750 - val_loss: 1.0187 - val_acc: 0.8125\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8230 - acc: 0.7809 - val_loss: 1.1511 - val_acc: 0.7917\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7987 - acc: 0.7965 - val_loss: 0.9969 - val_acc: 0.7917\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7885 - acc: 0.7897 - val_loss: 0.9811 - val_acc: 0.8125\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7629 - acc: 0.7977 - val_loss: 1.0030 - val_acc: 0.7917\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7782 - acc: 0.7935 - val_loss: 0.9939 - val_acc: 0.7917\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7546 - acc: 0.7952 - val_loss: 0.9814 - val_acc: 0.8333\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7314 - acc: 0.8114 - val_loss: 1.0351 - val_acc: 0.8333\n",
            "processing fold # 46\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 13s 3ms/step - loss: 2.9460 - acc: 0.2082 - val_loss: 2.7330 - val_acc: 0.2292\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.4633 - acc: 0.3076 - val_loss: 2.5206 - val_acc: 0.3333\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.1955 - acc: 0.3909 - val_loss: 2.4257 - val_acc: 0.2500\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.9409 - acc: 0.4493 - val_loss: 2.1422 - val_acc: 0.3750\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.7463 - acc: 0.5088 - val_loss: 2.0054 - val_acc: 0.4375\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.6211 - acc: 0.5494 - val_loss: 1.8772 - val_acc: 0.5000\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5130 - acc: 0.5696 - val_loss: 1.6943 - val_acc: 0.5625\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4349 - acc: 0.6039 - val_loss: 1.6771 - val_acc: 0.5417\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.3234 - acc: 0.6337 - val_loss: 1.7049 - val_acc: 0.6042\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2413 - acc: 0.6644 - val_loss: 1.6636 - val_acc: 0.6042\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1954 - acc: 0.6722 - val_loss: 1.5373 - val_acc: 0.6042\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1256 - acc: 0.6918 - val_loss: 1.5776 - val_acc: 0.5208\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0881 - acc: 0.7096 - val_loss: 1.5603 - val_acc: 0.6458\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0475 - acc: 0.7214 - val_loss: 1.4886 - val_acc: 0.6875\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0204 - acc: 0.7229 - val_loss: 1.4044 - val_acc: 0.6667\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9760 - acc: 0.7376 - val_loss: 1.4969 - val_acc: 0.6042\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9413 - acc: 0.7489 - val_loss: 1.4715 - val_acc: 0.6458\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9233 - acc: 0.7540 - val_loss: 1.4748 - val_acc: 0.6875\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8946 - acc: 0.7635 - val_loss: 1.5389 - val_acc: 0.6667\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8826 - acc: 0.7729 - val_loss: 1.3688 - val_acc: 0.6667\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8778 - acc: 0.7649 - val_loss: 1.3853 - val_acc: 0.6875\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8454 - acc: 0.7744 - val_loss: 1.3329 - val_acc: 0.6875\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8250 - acc: 0.7832 - val_loss: 1.3129 - val_acc: 0.6875\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8083 - acc: 0.7916 - val_loss: 1.3296 - val_acc: 0.7500\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7956 - acc: 0.7965 - val_loss: 1.3793 - val_acc: 0.6667\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7731 - acc: 0.7973 - val_loss: 1.2749 - val_acc: 0.7083\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7571 - acc: 0.8101 - val_loss: 1.3380 - val_acc: 0.6667\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7412 - acc: 0.8055 - val_loss: 1.2119 - val_acc: 0.7500\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7438 - acc: 0.8061 - val_loss: 1.2580 - val_acc: 0.6875\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7245 - acc: 0.8106 - val_loss: 1.2718 - val_acc: 0.7917\n",
            "processing fold # 47\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 13s 3ms/step - loss: 3.0777 - acc: 0.1920 - val_loss: 2.9515 - val_acc: 0.1667\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.4929 - acc: 0.3167 - val_loss: 2.5279 - val_acc: 0.2292\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.2108 - acc: 0.3890 - val_loss: 2.2732 - val_acc: 0.2917\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.9850 - acc: 0.4500 - val_loss: 2.2690 - val_acc: 0.4167\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.7892 - acc: 0.5036 - val_loss: 2.1382 - val_acc: 0.3750\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.6472 - acc: 0.5496 - val_loss: 1.9802 - val_acc: 0.4375\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5431 - acc: 0.5797 - val_loss: 1.7851 - val_acc: 0.5000\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.4348 - acc: 0.6142 - val_loss: 1.7363 - val_acc: 0.5000\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3547 - acc: 0.6371 - val_loss: 1.6020 - val_acc: 0.5417\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2835 - acc: 0.6625 - val_loss: 1.6826 - val_acc: 0.5000\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2390 - acc: 0.6716 - val_loss: 1.6331 - val_acc: 0.5208\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1692 - acc: 0.6905 - val_loss: 1.4974 - val_acc: 0.5833\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.1256 - acc: 0.7040 - val_loss: 1.5004 - val_acc: 0.6042\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0857 - acc: 0.7090 - val_loss: 1.5280 - val_acc: 0.6042\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0458 - acc: 0.7250 - val_loss: 1.5192 - val_acc: 0.5417\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0214 - acc: 0.7277 - val_loss: 1.4069 - val_acc: 0.5833\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9632 - acc: 0.7540 - val_loss: 1.3912 - val_acc: 0.6250\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9529 - acc: 0.7519 - val_loss: 1.3631 - val_acc: 0.6667\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9251 - acc: 0.7593 - val_loss: 1.4086 - val_acc: 0.6250\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8894 - acc: 0.7696 - val_loss: 1.4338 - val_acc: 0.6042\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8730 - acc: 0.7744 - val_loss: 1.3553 - val_acc: 0.6250\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8436 - acc: 0.7801 - val_loss: 1.3799 - val_acc: 0.6875\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8554 - acc: 0.7803 - val_loss: 1.4467 - val_acc: 0.6667\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.8345 - acc: 0.7834 - val_loss: 1.4126 - val_acc: 0.6458\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8088 - acc: 0.7895 - val_loss: 1.3299 - val_acc: 0.6042\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7820 - acc: 0.7990 - val_loss: 1.4333 - val_acc: 0.6875\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7635 - acc: 0.8049 - val_loss: 1.4579 - val_acc: 0.6667\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7483 - acc: 0.8078 - val_loss: 1.6346 - val_acc: 0.6458\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7676 - acc: 0.8000 - val_loss: 1.5415 - val_acc: 0.6458\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7245 - acc: 0.8146 - val_loss: 1.4613 - val_acc: 0.6667\n",
            "processing fold # 48\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 13s 3ms/step - loss: 2.9315 - acc: 0.2084 - val_loss: 2.3818 - val_acc: 0.4792\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.3450 - acc: 0.3684 - val_loss: 1.9640 - val_acc: 0.4375\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 2.0622 - acc: 0.4315 - val_loss: 1.6800 - val_acc: 0.5625\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.8253 - acc: 0.5011 - val_loss: 1.6249 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.6708 - acc: 0.5389 - val_loss: 1.6114 - val_acc: 0.5000\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5333 - acc: 0.5696 - val_loss: 1.5770 - val_acc: 0.5417\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.4403 - acc: 0.6051 - val_loss: 1.4558 - val_acc: 0.6042\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3446 - acc: 0.6316 - val_loss: 1.3279 - val_acc: 0.6458\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2800 - acc: 0.6499 - val_loss: 1.3734 - val_acc: 0.6458\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2226 - acc: 0.6705 - val_loss: 1.3892 - val_acc: 0.6458\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1786 - acc: 0.6812 - val_loss: 1.2801 - val_acc: 0.6875\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1231 - acc: 0.6991 - val_loss: 1.3412 - val_acc: 0.6458\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0720 - acc: 0.7126 - val_loss: 1.3564 - val_acc: 0.6875\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0027 - acc: 0.7317 - val_loss: 1.3251 - val_acc: 0.6458\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9919 - acc: 0.7359 - val_loss: 1.3586 - val_acc: 0.6875\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9757 - acc: 0.7370 - val_loss: 1.5005 - val_acc: 0.6250\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9363 - acc: 0.7498 - val_loss: 1.3617 - val_acc: 0.7500\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9118 - acc: 0.7645 - val_loss: 1.4369 - val_acc: 0.7292\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9048 - acc: 0.7582 - val_loss: 1.3448 - val_acc: 0.7708\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8632 - acc: 0.7763 - val_loss: 1.4003 - val_acc: 0.7083\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8290 - acc: 0.7836 - val_loss: 1.3145 - val_acc: 0.7708\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8406 - acc: 0.7813 - val_loss: 1.4094 - val_acc: 0.7500\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8101 - acc: 0.7883 - val_loss: 1.3105 - val_acc: 0.7708\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8001 - acc: 0.7963 - val_loss: 1.4490 - val_acc: 0.7083\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7897 - acc: 0.7935 - val_loss: 1.4781 - val_acc: 0.7083\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7659 - acc: 0.7971 - val_loss: 1.3831 - val_acc: 0.6875\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7545 - acc: 0.8011 - val_loss: 1.4776 - val_acc: 0.7083\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7287 - acc: 0.8194 - val_loss: 1.4073 - val_acc: 0.7083\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7190 - acc: 0.8118 - val_loss: 1.3658 - val_acc: 0.7292\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7180 - acc: 0.8148 - val_loss: 1.4305 - val_acc: 0.7292\n",
            "processing fold # 49\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 14s 3ms/step - loss: 2.9812 - acc: 0.1937 - val_loss: 2.6249 - val_acc: 0.1667\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.4597 - acc: 0.2992 - val_loss: 2.2964 - val_acc: 0.3750\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.1643 - acc: 0.3968 - val_loss: 2.0734 - val_acc: 0.4583\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.9648 - acc: 0.4497 - val_loss: 1.8424 - val_acc: 0.5208\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.7795 - acc: 0.5082 - val_loss: 1.7088 - val_acc: 0.5625\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.6401 - acc: 0.5486 - val_loss: 1.6860 - val_acc: 0.5417\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5223 - acc: 0.5744 - val_loss: 1.4059 - val_acc: 0.6042\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4219 - acc: 0.6001 - val_loss: 1.2971 - val_acc: 0.6458\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3755 - acc: 0.6203 - val_loss: 1.2726 - val_acc: 0.6875\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3006 - acc: 0.6470 - val_loss: 1.2732 - val_acc: 0.6875\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2445 - acc: 0.6604 - val_loss: 1.1478 - val_acc: 0.6667\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1828 - acc: 0.6779 - val_loss: 1.1817 - val_acc: 0.7083\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1349 - acc: 0.6915 - val_loss: 1.0592 - val_acc: 0.7708\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1024 - acc: 0.7052 - val_loss: 0.9556 - val_acc: 0.7708\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0452 - acc: 0.7210 - val_loss: 0.9270 - val_acc: 0.7917\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0160 - acc: 0.7258 - val_loss: 0.9063 - val_acc: 0.7292\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9944 - acc: 0.7441 - val_loss: 0.9772 - val_acc: 0.7708\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9635 - acc: 0.7466 - val_loss: 1.1426 - val_acc: 0.6875\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9226 - acc: 0.7582 - val_loss: 0.9786 - val_acc: 0.7500\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8935 - acc: 0.7618 - val_loss: 0.8961 - val_acc: 0.8125\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8847 - acc: 0.7639 - val_loss: 0.8325 - val_acc: 0.7917\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8498 - acc: 0.7761 - val_loss: 0.8282 - val_acc: 0.8125\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8484 - acc: 0.7740 - val_loss: 0.9079 - val_acc: 0.7500\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8408 - acc: 0.7740 - val_loss: 0.9129 - val_acc: 0.7917\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8167 - acc: 0.7906 - val_loss: 0.8476 - val_acc: 0.8333\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7938 - acc: 0.7916 - val_loss: 0.7810 - val_acc: 0.7917\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7592 - acc: 0.8028 - val_loss: 0.9168 - val_acc: 0.8125\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7860 - acc: 0.7958 - val_loss: 0.8703 - val_acc: 0.7708\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7633 - acc: 0.8005 - val_loss: 0.9617 - val_acc: 0.7083\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7474 - acc: 0.8032 - val_loss: 0.9296 - val_acc: 0.7292\n",
            "processing fold # 50\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 14s 3ms/step - loss: 2.9511 - acc: 0.2016 - val_loss: 2.3204 - val_acc: 0.3958\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.4010 - acc: 0.3427 - val_loss: 1.9917 - val_acc: 0.4792\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.0970 - acc: 0.4319 - val_loss: 1.8833 - val_acc: 0.5000\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.8467 - acc: 0.4941 - val_loss: 1.6505 - val_acc: 0.5417\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.6877 - acc: 0.5433 - val_loss: 1.6280 - val_acc: 0.5833\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5553 - acc: 0.5799 - val_loss: 1.4175 - val_acc: 0.5625\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4399 - acc: 0.6102 - val_loss: 1.2530 - val_acc: 0.6667\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3445 - acc: 0.6354 - val_loss: 1.2299 - val_acc: 0.6042\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2768 - acc: 0.6590 - val_loss: 1.1046 - val_acc: 0.6667\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2114 - acc: 0.6819 - val_loss: 1.0879 - val_acc: 0.7083\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1569 - acc: 0.6871 - val_loss: 0.9789 - val_acc: 0.7500\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1434 - acc: 0.7014 - val_loss: 0.9916 - val_acc: 0.7292\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0829 - acc: 0.7134 - val_loss: 0.8696 - val_acc: 0.7708\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0351 - acc: 0.7296 - val_loss: 0.7958 - val_acc: 0.7917\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0122 - acc: 0.7315 - val_loss: 0.8641 - val_acc: 0.7500\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9728 - acc: 0.7471 - val_loss: 0.8340 - val_acc: 0.7917\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9386 - acc: 0.7563 - val_loss: 0.8402 - val_acc: 0.7500\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9085 - acc: 0.7637 - val_loss: 0.7691 - val_acc: 0.7917\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8967 - acc: 0.7704 - val_loss: 0.7729 - val_acc: 0.8125\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8564 - acc: 0.7778 - val_loss: 0.7697 - val_acc: 0.7917\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8419 - acc: 0.7857 - val_loss: 0.7484 - val_acc: 0.7292\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8214 - acc: 0.7870 - val_loss: 0.7829 - val_acc: 0.7917\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7984 - acc: 0.7946 - val_loss: 0.7289 - val_acc: 0.8333\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.7866 - acc: 0.7948 - val_loss: 0.8013 - val_acc: 0.7500\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7618 - acc: 0.8076 - val_loss: 0.7439 - val_acc: 0.8125\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7404 - acc: 0.8129 - val_loss: 0.7921 - val_acc: 0.8125\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7364 - acc: 0.8106 - val_loss: 0.6484 - val_acc: 0.7708\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7514 - acc: 0.8066 - val_loss: 0.6733 - val_acc: 0.8125\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7184 - acc: 0.8148 - val_loss: 0.7191 - val_acc: 0.7917\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.6985 - acc: 0.8194 - val_loss: 0.7173 - val_acc: 0.8333\n",
            "processing fold # 51\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 14s 3ms/step - loss: 2.9954 - acc: 0.1991 - val_loss: 2.3854 - val_acc: 0.4167\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.4379 - acc: 0.3305 - val_loss: 1.8937 - val_acc: 0.4583\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.1126 - acc: 0.4123 - val_loss: 1.6829 - val_acc: 0.5417\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.9318 - acc: 0.4619 - val_loss: 1.5473 - val_acc: 0.6042\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.7436 - acc: 0.5242 - val_loss: 1.4323 - val_acc: 0.6042\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.6029 - acc: 0.5589 - val_loss: 1.6146 - val_acc: 0.5208\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5320 - acc: 0.5732 - val_loss: 1.4525 - val_acc: 0.6250\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4256 - acc: 0.6106 - val_loss: 1.4575 - val_acc: 0.6458\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3660 - acc: 0.6266 - val_loss: 1.3470 - val_acc: 0.6250\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2779 - acc: 0.6566 - val_loss: 1.4041 - val_acc: 0.6250\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1950 - acc: 0.6766 - val_loss: 1.2907 - val_acc: 0.6458\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1476 - acc: 0.6911 - val_loss: 1.3654 - val_acc: 0.6875\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0864 - acc: 0.7111 - val_loss: 1.2336 - val_acc: 0.6458\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0659 - acc: 0.7136 - val_loss: 1.2203 - val_acc: 0.7083\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0242 - acc: 0.7315 - val_loss: 1.2963 - val_acc: 0.7083\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9974 - acc: 0.7342 - val_loss: 1.2070 - val_acc: 0.6875\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9800 - acc: 0.7389 - val_loss: 1.2470 - val_acc: 0.6042\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9549 - acc: 0.7483 - val_loss: 1.1822 - val_acc: 0.7083\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9465 - acc: 0.7500 - val_loss: 1.3763 - val_acc: 0.6667\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9069 - acc: 0.7672 - val_loss: 1.2084 - val_acc: 0.6875\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8776 - acc: 0.7691 - val_loss: 1.1968 - val_acc: 0.7292\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8642 - acc: 0.7763 - val_loss: 1.3005 - val_acc: 0.6875\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8462 - acc: 0.7752 - val_loss: 1.3284 - val_acc: 0.6875\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8397 - acc: 0.7809 - val_loss: 1.3336 - val_acc: 0.6875\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8041 - acc: 0.7923 - val_loss: 1.2500 - val_acc: 0.6875\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7900 - acc: 0.7984 - val_loss: 1.4156 - val_acc: 0.6667\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7806 - acc: 0.7996 - val_loss: 1.2775 - val_acc: 0.7500\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7563 - acc: 0.8091 - val_loss: 1.2106 - val_acc: 0.7292\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7651 - acc: 0.8007 - val_loss: 1.4093 - val_acc: 0.7292\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7436 - acc: 0.8093 - val_loss: 1.2249 - val_acc: 0.7083\n",
            "processing fold # 52\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 14s 3ms/step - loss: 2.8830 - acc: 0.2239 - val_loss: 2.2984 - val_acc: 0.3333\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.3671 - acc: 0.3436 - val_loss: 2.1634 - val_acc: 0.3542\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.1159 - acc: 0.4100 - val_loss: 2.0514 - val_acc: 0.4583\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.8932 - acc: 0.4853 - val_loss: 1.8503 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.6977 - acc: 0.5341 - val_loss: 1.6601 - val_acc: 0.5208\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.5479 - acc: 0.5799 - val_loss: 1.6198 - val_acc: 0.5417\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4434 - acc: 0.6030 - val_loss: 1.6420 - val_acc: 0.5625\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3712 - acc: 0.6289 - val_loss: 1.6250 - val_acc: 0.5000\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3142 - acc: 0.6455 - val_loss: 1.5116 - val_acc: 0.6250\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2292 - acc: 0.6686 - val_loss: 1.4227 - val_acc: 0.6042\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1717 - acc: 0.6854 - val_loss: 1.4061 - val_acc: 0.6458\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1500 - acc: 0.6924 - val_loss: 1.4216 - val_acc: 0.6875\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0800 - acc: 0.7113 - val_loss: 1.3405 - val_acc: 0.6667\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0498 - acc: 0.7283 - val_loss: 1.3011 - val_acc: 0.7083\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 0.9948 - acc: 0.7365 - val_loss: 1.2893 - val_acc: 0.6667\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9741 - acc: 0.7441 - val_loss: 1.3161 - val_acc: 0.6250\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9301 - acc: 0.7584 - val_loss: 1.2182 - val_acc: 0.7083\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9177 - acc: 0.7624 - val_loss: 1.2910 - val_acc: 0.7292\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8942 - acc: 0.7624 - val_loss: 1.2280 - val_acc: 0.6875\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8886 - acc: 0.7677 - val_loss: 1.1289 - val_acc: 0.7083\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8607 - acc: 0.7721 - val_loss: 1.2358 - val_acc: 0.7083\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8328 - acc: 0.7780 - val_loss: 1.2457 - val_acc: 0.7083\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8083 - acc: 0.7969 - val_loss: 1.1974 - val_acc: 0.7500\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7949 - acc: 0.7891 - val_loss: 1.2040 - val_acc: 0.7083\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7578 - acc: 0.8036 - val_loss: 1.1839 - val_acc: 0.7083\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7481 - acc: 0.8063 - val_loss: 1.1830 - val_acc: 0.6875\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7419 - acc: 0.8089 - val_loss: 1.1361 - val_acc: 0.7500\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7490 - acc: 0.8080 - val_loss: 1.2168 - val_acc: 0.6875\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7405 - acc: 0.8049 - val_loss: 1.2405 - val_acc: 0.7292\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7216 - acc: 0.8053 - val_loss: 1.2368 - val_acc: 0.7292\n",
            "processing fold # 53\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 14s 3ms/step - loss: 2.9407 - acc: 0.2067 - val_loss: 2.7140 - val_acc: 0.2917\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.4296 - acc: 0.3295 - val_loss: 2.4776 - val_acc: 0.2500\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.1434 - acc: 0.3877 - val_loss: 2.1311 - val_acc: 0.3333\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.9414 - acc: 0.4422 - val_loss: 2.0392 - val_acc: 0.4375\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.7621 - acc: 0.5118 - val_loss: 1.7152 - val_acc: 0.5000\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5799 - acc: 0.5570 - val_loss: 1.6064 - val_acc: 0.5417\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4606 - acc: 0.5984 - val_loss: 1.5184 - val_acc: 0.5833\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3699 - acc: 0.6302 - val_loss: 1.4358 - val_acc: 0.6458\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3118 - acc: 0.6459 - val_loss: 1.4961 - val_acc: 0.6042\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2391 - acc: 0.6594 - val_loss: 1.5455 - val_acc: 0.5208\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2097 - acc: 0.6722 - val_loss: 1.4881 - val_acc: 0.6042\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1341 - acc: 0.6930 - val_loss: 1.3948 - val_acc: 0.6458\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 20us/step - loss: 1.0899 - acc: 0.7109 - val_loss: 1.4147 - val_acc: 0.5625\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0469 - acc: 0.7235 - val_loss: 1.3457 - val_acc: 0.6667\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0092 - acc: 0.7260 - val_loss: 1.3908 - val_acc: 0.6875\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9625 - acc: 0.7483 - val_loss: 1.4317 - val_acc: 0.6667\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9371 - acc: 0.7511 - val_loss: 1.4093 - val_acc: 0.6042\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9347 - acc: 0.7506 - val_loss: 1.3142 - val_acc: 0.7083\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8957 - acc: 0.7599 - val_loss: 1.2904 - val_acc: 0.6875\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8697 - acc: 0.7687 - val_loss: 1.2436 - val_acc: 0.6667\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8473 - acc: 0.7721 - val_loss: 1.2643 - val_acc: 0.7292\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8287 - acc: 0.7780 - val_loss: 1.3083 - val_acc: 0.7083\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8287 - acc: 0.7862 - val_loss: 1.2450 - val_acc: 0.7292\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8157 - acc: 0.7830 - val_loss: 1.2986 - val_acc: 0.6667\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7997 - acc: 0.7832 - val_loss: 1.3052 - val_acc: 0.6667\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7867 - acc: 0.7860 - val_loss: 1.2354 - val_acc: 0.6875\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7610 - acc: 0.7967 - val_loss: 1.1913 - val_acc: 0.7083\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7478 - acc: 0.8030 - val_loss: 1.2436 - val_acc: 0.6875\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7259 - acc: 0.8063 - val_loss: 1.2550 - val_acc: 0.6667\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7214 - acc: 0.8078 - val_loss: 1.1851 - val_acc: 0.6875\n",
            "processing fold # 54\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 15s 3ms/step - loss: 3.0091 - acc: 0.1934 - val_loss: 2.6405 - val_acc: 0.2708\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.4434 - acc: 0.3392 - val_loss: 2.2878 - val_acc: 0.3333\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.1489 - acc: 0.4102 - val_loss: 2.0932 - val_acc: 0.4375\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.9349 - acc: 0.4664 - val_loss: 1.7449 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.7609 - acc: 0.5160 - val_loss: 1.5829 - val_acc: 0.5417\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.6182 - acc: 0.5477 - val_loss: 1.5812 - val_acc: 0.5833\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5008 - acc: 0.5824 - val_loss: 1.4544 - val_acc: 0.5833\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4157 - acc: 0.6081 - val_loss: 1.3588 - val_acc: 0.6250\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3187 - acc: 0.6400 - val_loss: 1.4349 - val_acc: 0.6042\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.2737 - acc: 0.6487 - val_loss: 1.2810 - val_acc: 0.6250\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2049 - acc: 0.6747 - val_loss: 1.2827 - val_acc: 0.6667\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1293 - acc: 0.6974 - val_loss: 1.2325 - val_acc: 0.7083\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0856 - acc: 0.7073 - val_loss: 1.0968 - val_acc: 0.6875\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0485 - acc: 0.7189 - val_loss: 1.1756 - val_acc: 0.6667\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0160 - acc: 0.7256 - val_loss: 1.0918 - val_acc: 0.7500\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9994 - acc: 0.7332 - val_loss: 1.1269 - val_acc: 0.7083\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9687 - acc: 0.7475 - val_loss: 1.0904 - val_acc: 0.7917\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9419 - acc: 0.7502 - val_loss: 1.0851 - val_acc: 0.7292\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 27us/step - loss: 0.8948 - acc: 0.7649 - val_loss: 1.0671 - val_acc: 0.7500\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 28us/step - loss: 0.8674 - acc: 0.7742 - val_loss: 1.0094 - val_acc: 0.7917\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8461 - acc: 0.7845 - val_loss: 0.9956 - val_acc: 0.7917\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8196 - acc: 0.7815 - val_loss: 1.0002 - val_acc: 0.7708\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8168 - acc: 0.7864 - val_loss: 1.0737 - val_acc: 0.7917\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7859 - acc: 0.7899 - val_loss: 1.0280 - val_acc: 0.7917\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7690 - acc: 0.7990 - val_loss: 1.0436 - val_acc: 0.7083\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7626 - acc: 0.7988 - val_loss: 0.9902 - val_acc: 0.7708\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7464 - acc: 0.8038 - val_loss: 1.0626 - val_acc: 0.7708\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7444 - acc: 0.8059 - val_loss: 1.0553 - val_acc: 0.7500\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7374 - acc: 0.7960 - val_loss: 0.9171 - val_acc: 0.7917\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7207 - acc: 0.8078 - val_loss: 1.0224 - val_acc: 0.7708\n",
            "processing fold # 55\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 15s 3ms/step - loss: 2.8670 - acc: 0.2275 - val_loss: 2.2804 - val_acc: 0.4375\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.3331 - acc: 0.3659 - val_loss: 1.9344 - val_acc: 0.5208\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.0099 - acc: 0.4500 - val_loss: 1.7729 - val_acc: 0.5208\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.7921 - acc: 0.5120 - val_loss: 1.5057 - val_acc: 0.5417\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.6412 - acc: 0.5488 - val_loss: 1.3907 - val_acc: 0.5417\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.5140 - acc: 0.5788 - val_loss: 1.3904 - val_acc: 0.6042\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3890 - acc: 0.6213 - val_loss: 1.3155 - val_acc: 0.7083\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3167 - acc: 0.6518 - val_loss: 1.3376 - val_acc: 0.6667\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2516 - acc: 0.6558 - val_loss: 1.3231 - val_acc: 0.7292\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1805 - acc: 0.6850 - val_loss: 1.2860 - val_acc: 0.6875\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1312 - acc: 0.7019 - val_loss: 1.2764 - val_acc: 0.7292\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0803 - acc: 0.7132 - val_loss: 1.2095 - val_acc: 0.6458\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0465 - acc: 0.7265 - val_loss: 1.3377 - val_acc: 0.7083\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9991 - acc: 0.7283 - val_loss: 1.3200 - val_acc: 0.6458\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9523 - acc: 0.7468 - val_loss: 1.2359 - val_acc: 0.7083\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9220 - acc: 0.7603 - val_loss: 1.2417 - val_acc: 0.7083\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8891 - acc: 0.7653 - val_loss: 1.3688 - val_acc: 0.7083\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8947 - acc: 0.7677 - val_loss: 1.2222 - val_acc: 0.7083\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8650 - acc: 0.7702 - val_loss: 1.2996 - val_acc: 0.7292\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8245 - acc: 0.7866 - val_loss: 1.2686 - val_acc: 0.7292\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8128 - acc: 0.7878 - val_loss: 1.2835 - val_acc: 0.7292\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8014 - acc: 0.7889 - val_loss: 1.3706 - val_acc: 0.7083\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7739 - acc: 0.7956 - val_loss: 1.2222 - val_acc: 0.7083\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7620 - acc: 0.8007 - val_loss: 1.3985 - val_acc: 0.6875\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7441 - acc: 0.8118 - val_loss: 1.4626 - val_acc: 0.6875\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7300 - acc: 0.8141 - val_loss: 1.3393 - val_acc: 0.7083\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7275 - acc: 0.8146 - val_loss: 1.4060 - val_acc: 0.6667\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.6959 - acc: 0.8244 - val_loss: 1.2582 - val_acc: 0.7708\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.6897 - acc: 0.8251 - val_loss: 1.4364 - val_acc: 0.6667\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.6612 - acc: 0.8272 - val_loss: 1.3294 - val_acc: 0.6875\n",
            "processing fold # 56\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 15s 3ms/step - loss: 2.9352 - acc: 0.2071 - val_loss: 2.6980 - val_acc: 0.2708\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.4389 - acc: 0.3406 - val_loss: 2.4199 - val_acc: 0.3750\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.1416 - acc: 0.4005 - val_loss: 2.0935 - val_acc: 0.4583\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.9039 - acc: 0.4767 - val_loss: 2.0058 - val_acc: 0.5208\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.7133 - acc: 0.5223 - val_loss: 1.8682 - val_acc: 0.5208\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.5904 - acc: 0.5568 - val_loss: 1.7032 - val_acc: 0.5417\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4770 - acc: 0.5929 - val_loss: 1.5454 - val_acc: 0.6042\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3883 - acc: 0.6188 - val_loss: 1.5043 - val_acc: 0.6042\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3020 - acc: 0.6482 - val_loss: 1.3526 - val_acc: 0.6667\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2279 - acc: 0.6646 - val_loss: 1.3092 - val_acc: 0.7292\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1964 - acc: 0.6751 - val_loss: 1.3940 - val_acc: 0.6458\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1404 - acc: 0.6966 - val_loss: 1.3600 - val_acc: 0.6458\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0822 - acc: 0.7117 - val_loss: 1.3767 - val_acc: 0.6875\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0438 - acc: 0.7256 - val_loss: 1.3206 - val_acc: 0.7083\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0147 - acc: 0.7309 - val_loss: 1.2645 - val_acc: 0.7292\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9670 - acc: 0.7435 - val_loss: 1.3045 - val_acc: 0.6667\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9582 - acc: 0.7464 - val_loss: 1.2753 - val_acc: 0.6875\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9258 - acc: 0.7576 - val_loss: 1.2869 - val_acc: 0.6875\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9071 - acc: 0.7614 - val_loss: 1.3402 - val_acc: 0.7083\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8955 - acc: 0.7567 - val_loss: 1.3553 - val_acc: 0.6667\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8423 - acc: 0.7853 - val_loss: 1.2489 - val_acc: 0.6875\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8164 - acc: 0.7883 - val_loss: 1.2705 - val_acc: 0.6667\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8092 - acc: 0.7881 - val_loss: 1.3622 - val_acc: 0.6875\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8004 - acc: 0.7933 - val_loss: 1.3689 - val_acc: 0.7083\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7798 - acc: 0.7914 - val_loss: 1.4074 - val_acc: 0.6875\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7879 - acc: 0.7984 - val_loss: 1.4104 - val_acc: 0.6875\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7463 - acc: 0.8059 - val_loss: 1.3574 - val_acc: 0.7292\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7299 - acc: 0.8089 - val_loss: 1.3456 - val_acc: 0.6875\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7292 - acc: 0.8188 - val_loss: 1.4384 - val_acc: 0.7292\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7049 - acc: 0.8198 - val_loss: 1.3553 - val_acc: 0.7083\n",
            "processing fold # 57\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 15s 3ms/step - loss: 2.9982 - acc: 0.2054 - val_loss: 2.7760 - val_acc: 0.2917\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.3942 - acc: 0.3675 - val_loss: 2.4528 - val_acc: 0.2917\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.0703 - acc: 0.4300 - val_loss: 1.9386 - val_acc: 0.4583\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.8535 - acc: 0.4971 - val_loss: 1.6991 - val_acc: 0.5625\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.6746 - acc: 0.5406 - val_loss: 1.6636 - val_acc: 0.5417\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.5394 - acc: 0.5767 - val_loss: 1.6731 - val_acc: 0.5625\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4214 - acc: 0.6102 - val_loss: 1.5373 - val_acc: 0.6042\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3848 - acc: 0.6234 - val_loss: 1.4395 - val_acc: 0.5833\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2908 - acc: 0.6533 - val_loss: 1.2577 - val_acc: 0.6458\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2073 - acc: 0.6804 - val_loss: 1.2768 - val_acc: 0.6458\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1331 - acc: 0.6941 - val_loss: 1.3761 - val_acc: 0.5833\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1003 - acc: 0.7126 - val_loss: 1.2575 - val_acc: 0.6458\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0618 - acc: 0.7239 - val_loss: 1.1786 - val_acc: 0.6667\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0093 - acc: 0.7374 - val_loss: 1.1714 - val_acc: 0.6458\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9760 - acc: 0.7471 - val_loss: 1.0792 - val_acc: 0.6458\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9628 - acc: 0.7489 - val_loss: 1.1407 - val_acc: 0.6875\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9320 - acc: 0.7595 - val_loss: 1.0616 - val_acc: 0.6875\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8945 - acc: 0.7662 - val_loss: 1.1302 - val_acc: 0.6667\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8677 - acc: 0.7801 - val_loss: 1.0879 - val_acc: 0.6875\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8454 - acc: 0.7784 - val_loss: 1.0844 - val_acc: 0.7083\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8502 - acc: 0.7786 - val_loss: 1.0772 - val_acc: 0.7292\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8428 - acc: 0.7773 - val_loss: 1.1030 - val_acc: 0.6875\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7926 - acc: 0.7977 - val_loss: 1.0554 - val_acc: 0.7292\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7640 - acc: 0.8057 - val_loss: 1.0424 - val_acc: 0.7083\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7513 - acc: 0.8112 - val_loss: 1.0812 - val_acc: 0.7083\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7351 - acc: 0.8099 - val_loss: 0.9977 - val_acc: 0.7083\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7350 - acc: 0.8082 - val_loss: 1.0545 - val_acc: 0.6875\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7108 - acc: 0.8209 - val_loss: 1.0065 - val_acc: 0.6667\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7224 - acc: 0.8181 - val_loss: 1.1160 - val_acc: 0.7500\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7088 - acc: 0.8143 - val_loss: 1.2020 - val_acc: 0.6875\n",
            "processing fold # 58\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 16s 3ms/step - loss: 2.9729 - acc: 0.2094 - val_loss: 2.8340 - val_acc: 0.1667\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.4157 - acc: 0.3408 - val_loss: 2.6460 - val_acc: 0.2083\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 2.1523 - acc: 0.3982 - val_loss: 2.4789 - val_acc: 0.2500\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.9411 - acc: 0.4754 - val_loss: 2.2197 - val_acc: 0.2917\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.8105 - acc: 0.4975 - val_loss: 2.0727 - val_acc: 0.3750\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.6155 - acc: 0.5561 - val_loss: 1.9510 - val_acc: 0.4375\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4961 - acc: 0.5873 - val_loss: 1.8152 - val_acc: 0.4167\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3997 - acc: 0.6203 - val_loss: 1.7716 - val_acc: 0.5000\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3223 - acc: 0.6501 - val_loss: 1.6393 - val_acc: 0.5208\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.2589 - acc: 0.6598 - val_loss: 1.6278 - val_acc: 0.5417\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1829 - acc: 0.6836 - val_loss: 1.5761 - val_acc: 0.4583\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1435 - acc: 0.7002 - val_loss: 1.5248 - val_acc: 0.5208\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0874 - acc: 0.7140 - val_loss: 1.5312 - val_acc: 0.5833\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0299 - acc: 0.7281 - val_loss: 1.4489 - val_acc: 0.5625\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9980 - acc: 0.7328 - val_loss: 1.5138 - val_acc: 0.5833\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9870 - acc: 0.7452 - val_loss: 1.4397 - val_acc: 0.5625\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9603 - acc: 0.7479 - val_loss: 1.5023 - val_acc: 0.6042\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9571 - acc: 0.7500 - val_loss: 1.3683 - val_acc: 0.6250\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8953 - acc: 0.7719 - val_loss: 1.4550 - val_acc: 0.6042\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8606 - acc: 0.7813 - val_loss: 1.3968 - val_acc: 0.6250\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8527 - acc: 0.7839 - val_loss: 1.4050 - val_acc: 0.6667\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8431 - acc: 0.7754 - val_loss: 1.3986 - val_acc: 0.6250\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8137 - acc: 0.7908 - val_loss: 1.3966 - val_acc: 0.6667\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8085 - acc: 0.7845 - val_loss: 1.2056 - val_acc: 0.6875\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7779 - acc: 0.7971 - val_loss: 1.4042 - val_acc: 0.6875\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7765 - acc: 0.8024 - val_loss: 1.3473 - val_acc: 0.6875\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7572 - acc: 0.8047 - val_loss: 1.3516 - val_acc: 0.6667\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7469 - acc: 0.8099 - val_loss: 1.2927 - val_acc: 0.7292\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7143 - acc: 0.8175 - val_loss: 1.2306 - val_acc: 0.7083\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7132 - acc: 0.8171 - val_loss: 1.2591 - val_acc: 0.6667\n",
            "processing fold # 59\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 16s 3ms/step - loss: 2.9426 - acc: 0.2115 - val_loss: 2.7819 - val_acc: 0.2500\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.4297 - acc: 0.3471 - val_loss: 2.3629 - val_acc: 0.3542\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.1671 - acc: 0.4052 - val_loss: 2.0594 - val_acc: 0.3750\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.9600 - acc: 0.4554 - val_loss: 1.9216 - val_acc: 0.4583\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.7621 - acc: 0.5114 - val_loss: 1.7527 - val_acc: 0.5208\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.6302 - acc: 0.5517 - val_loss: 1.5033 - val_acc: 0.5833\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.5175 - acc: 0.5782 - val_loss: 1.4470 - val_acc: 0.5208\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4309 - acc: 0.6060 - val_loss: 1.4188 - val_acc: 0.6667\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3327 - acc: 0.6432 - val_loss: 1.3404 - val_acc: 0.6458\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2855 - acc: 0.6451 - val_loss: 1.2051 - val_acc: 0.7083\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2058 - acc: 0.6770 - val_loss: 1.1724 - val_acc: 0.6458\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1420 - acc: 0.6979 - val_loss: 1.1926 - val_acc: 0.7500\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0994 - acc: 0.7054 - val_loss: 1.2208 - val_acc: 0.7500\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0252 - acc: 0.7296 - val_loss: 1.1678 - val_acc: 0.7500\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0158 - acc: 0.7304 - val_loss: 1.1168 - val_acc: 0.7292\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9703 - acc: 0.7414 - val_loss: 1.0762 - val_acc: 0.7292\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9461 - acc: 0.7452 - val_loss: 1.1682 - val_acc: 0.7500\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9318 - acc: 0.7593 - val_loss: 1.2076 - val_acc: 0.7292\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9056 - acc: 0.7605 - val_loss: 1.0865 - val_acc: 0.7917\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8869 - acc: 0.7649 - val_loss: 1.0594 - val_acc: 0.7708\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8620 - acc: 0.7738 - val_loss: 0.9961 - val_acc: 0.7292\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8308 - acc: 0.7845 - val_loss: 1.1102 - val_acc: 0.7500\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8202 - acc: 0.7876 - val_loss: 1.0245 - val_acc: 0.7292\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8048 - acc: 0.7883 - val_loss: 1.0072 - val_acc: 0.7708\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7655 - acc: 0.8059 - val_loss: 1.0600 - val_acc: 0.7292\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7915 - acc: 0.7969 - val_loss: 0.9667 - val_acc: 0.7708\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7518 - acc: 0.8063 - val_loss: 1.0110 - val_acc: 0.7500\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7483 - acc: 0.8070 - val_loss: 0.9953 - val_acc: 0.8125\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7214 - acc: 0.8103 - val_loss: 1.0155 - val_acc: 0.7083\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7149 - acc: 0.8185 - val_loss: 0.9810 - val_acc: 0.7917\n",
            "processing fold # 60\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 16s 3ms/step - loss: 3.0888 - acc: 0.1932 - val_loss: 2.5142 - val_acc: 0.2500\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.4941 - acc: 0.3238 - val_loss: 2.0735 - val_acc: 0.4583\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.1657 - acc: 0.4108 - val_loss: 1.9257 - val_acc: 0.4375\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.9877 - acc: 0.4510 - val_loss: 1.8025 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.8250 - acc: 0.4912 - val_loss: 1.5527 - val_acc: 0.5625\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.6650 - acc: 0.5416 - val_loss: 1.4083 - val_acc: 0.7083\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.5384 - acc: 0.5818 - val_loss: 1.3276 - val_acc: 0.6667\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4494 - acc: 0.6016 - val_loss: 1.3412 - val_acc: 0.6667\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3695 - acc: 0.6226 - val_loss: 1.1941 - val_acc: 0.6875\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2945 - acc: 0.6539 - val_loss: 1.1255 - val_acc: 0.7083\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2223 - acc: 0.6726 - val_loss: 1.0795 - val_acc: 0.7500\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1527 - acc: 0.6913 - val_loss: 1.0365 - val_acc: 0.7292\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1188 - acc: 0.7058 - val_loss: 1.0244 - val_acc: 0.7292\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0746 - acc: 0.7229 - val_loss: 0.9725 - val_acc: 0.7500\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0439 - acc: 0.7229 - val_loss: 1.0149 - val_acc: 0.7292\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0080 - acc: 0.7330 - val_loss: 0.9527 - val_acc: 0.8125\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9781 - acc: 0.7437 - val_loss: 0.9636 - val_acc: 0.7708\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9452 - acc: 0.7508 - val_loss: 0.9711 - val_acc: 0.7708\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9340 - acc: 0.7548 - val_loss: 0.9519 - val_acc: 0.7708\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9405 - acc: 0.7508 - val_loss: 0.8894 - val_acc: 0.8125\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8950 - acc: 0.7689 - val_loss: 0.9277 - val_acc: 0.7708\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8572 - acc: 0.7828 - val_loss: 0.9330 - val_acc: 0.7500\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8294 - acc: 0.7883 - val_loss: 0.8976 - val_acc: 0.7708\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8313 - acc: 0.7908 - val_loss: 0.8609 - val_acc: 0.7917\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8211 - acc: 0.7889 - val_loss: 0.8930 - val_acc: 0.7708\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7962 - acc: 0.7914 - val_loss: 0.8411 - val_acc: 0.7708\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7631 - acc: 0.7996 - val_loss: 0.9403 - val_acc: 0.7708\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7629 - acc: 0.8045 - val_loss: 0.9003 - val_acc: 0.7708\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7531 - acc: 0.8066 - val_loss: 0.8037 - val_acc: 0.7917\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7409 - acc: 0.8061 - val_loss: 0.8549 - val_acc: 0.7708\n",
            "processing fold # 61\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 16s 3ms/step - loss: 2.9519 - acc: 0.2149 - val_loss: 2.3674 - val_acc: 0.3333\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.3675 - acc: 0.3616 - val_loss: 2.1196 - val_acc: 0.3958\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.1047 - acc: 0.4239 - val_loss: 1.9400 - val_acc: 0.5625\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.9059 - acc: 0.4695 - val_loss: 1.6760 - val_acc: 0.6042\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.7188 - acc: 0.5320 - val_loss: 1.5046 - val_acc: 0.5833\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.5669 - acc: 0.5688 - val_loss: 1.4702 - val_acc: 0.6458\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4435 - acc: 0.6013 - val_loss: 1.4565 - val_acc: 0.6250\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3793 - acc: 0.6253 - val_loss: 1.4611 - val_acc: 0.6042\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2991 - acc: 0.6440 - val_loss: 1.3139 - val_acc: 0.7500\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2331 - acc: 0.6665 - val_loss: 1.3115 - val_acc: 0.6667\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1763 - acc: 0.6852 - val_loss: 1.1902 - val_acc: 0.7500\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1363 - acc: 0.6955 - val_loss: 1.1581 - val_acc: 0.6875\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0569 - acc: 0.7134 - val_loss: 1.1703 - val_acc: 0.7083\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0395 - acc: 0.7237 - val_loss: 1.0426 - val_acc: 0.7500\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 31us/step - loss: 1.0022 - acc: 0.7323 - val_loss: 1.0365 - val_acc: 0.7708\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9702 - acc: 0.7456 - val_loss: 1.0818 - val_acc: 0.7500\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9467 - acc: 0.7481 - val_loss: 1.0135 - val_acc: 0.7917\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9249 - acc: 0.7487 - val_loss: 0.9886 - val_acc: 0.8125\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8851 - acc: 0.7649 - val_loss: 0.9859 - val_acc: 0.7917\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8627 - acc: 0.7780 - val_loss: 0.9304 - val_acc: 0.8542\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8452 - acc: 0.7813 - val_loss: 0.9640 - val_acc: 0.7917\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8327 - acc: 0.7851 - val_loss: 0.9707 - val_acc: 0.8125\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8125 - acc: 0.7946 - val_loss: 0.9100 - val_acc: 0.8333\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7995 - acc: 0.7939 - val_loss: 0.9416 - val_acc: 0.8333\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7879 - acc: 0.7984 - val_loss: 0.9418 - val_acc: 0.8125\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7719 - acc: 0.7996 - val_loss: 0.9055 - val_acc: 0.8125\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7562 - acc: 0.8032 - val_loss: 0.9454 - val_acc: 0.7917\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7597 - acc: 0.8015 - val_loss: 0.8828 - val_acc: 0.8125\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7348 - acc: 0.8095 - val_loss: 0.8823 - val_acc: 0.8125\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7217 - acc: 0.8122 - val_loss: 0.8963 - val_acc: 0.8125\n",
            "processing fold # 62\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 17s 3ms/step - loss: 2.9850 - acc: 0.1943 - val_loss: 2.6582 - val_acc: 0.3125\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 30us/step - loss: 2.4708 - acc: 0.3293 - val_loss: 2.3678 - val_acc: 0.3750\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 2.2239 - acc: 0.3886 - val_loss: 2.0521 - val_acc: 0.3958\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 26us/step - loss: 1.9786 - acc: 0.4554 - val_loss: 1.8401 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.8257 - acc: 0.4964 - val_loss: 1.7087 - val_acc: 0.5208\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 35us/step - loss: 1.6613 - acc: 0.5408 - val_loss: 1.6352 - val_acc: 0.6042\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.5443 - acc: 0.5774 - val_loss: 1.5427 - val_acc: 0.5625\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4519 - acc: 0.5992 - val_loss: 1.4396 - val_acc: 0.6458\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3855 - acc: 0.6220 - val_loss: 1.3963 - val_acc: 0.6875\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3195 - acc: 0.6484 - val_loss: 1.1808 - val_acc: 0.6667\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2251 - acc: 0.6644 - val_loss: 0.9919 - val_acc: 0.7708\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1817 - acc: 0.6844 - val_loss: 1.0419 - val_acc: 0.7083\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1314 - acc: 0.6955 - val_loss: 0.8776 - val_acc: 0.7917\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0940 - acc: 0.7113 - val_loss: 0.8520 - val_acc: 0.8333\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0431 - acc: 0.7212 - val_loss: 0.8558 - val_acc: 0.7708\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0370 - acc: 0.7206 - val_loss: 0.7292 - val_acc: 0.8333\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0012 - acc: 0.7365 - val_loss: 0.7190 - val_acc: 0.8333\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9800 - acc: 0.7439 - val_loss: 0.7643 - val_acc: 0.7917\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9484 - acc: 0.7563 - val_loss: 0.7911 - val_acc: 0.7917\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9300 - acc: 0.7601 - val_loss: 0.7497 - val_acc: 0.8333\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8749 - acc: 0.7706 - val_loss: 0.6927 - val_acc: 0.8750\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8854 - acc: 0.7689 - val_loss: 0.7764 - val_acc: 0.7917\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8384 - acc: 0.7807 - val_loss: 0.6716 - val_acc: 0.8750\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8427 - acc: 0.7794 - val_loss: 0.6147 - val_acc: 0.8542\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8407 - acc: 0.7805 - val_loss: 0.5723 - val_acc: 0.8542\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8113 - acc: 0.7885 - val_loss: 0.5867 - val_acc: 0.8750\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7990 - acc: 0.7931 - val_loss: 0.7338 - val_acc: 0.8333\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7675 - acc: 0.8024 - val_loss: 0.6278 - val_acc: 0.8542\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7499 - acc: 0.8055 - val_loss: 0.5730 - val_acc: 0.8542\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7466 - acc: 0.8120 - val_loss: 0.5599 - val_acc: 0.8750\n",
            "processing fold # 63\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 17s 4ms/step - loss: 2.9295 - acc: 0.2201 - val_loss: 2.5183 - val_acc: 0.3125\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.3986 - acc: 0.3419 - val_loss: 2.1428 - val_acc: 0.4167\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.1445 - acc: 0.4014 - val_loss: 1.9318 - val_acc: 0.4583\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.9257 - acc: 0.4579 - val_loss: 1.7095 - val_acc: 0.5417\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.7391 - acc: 0.5252 - val_loss: 1.5904 - val_acc: 0.5625\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.6013 - acc: 0.5534 - val_loss: 1.4696 - val_acc: 0.5000\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4828 - acc: 0.5915 - val_loss: 1.3442 - val_acc: 0.6250\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.3841 - acc: 0.6198 - val_loss: 1.1912 - val_acc: 0.7292\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3216 - acc: 0.6356 - val_loss: 1.1370 - val_acc: 0.7292\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2642 - acc: 0.6602 - val_loss: 1.1473 - val_acc: 0.7083\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2154 - acc: 0.6735 - val_loss: 1.1441 - val_acc: 0.6875\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1441 - acc: 0.6926 - val_loss: 1.1005 - val_acc: 0.7292\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1062 - acc: 0.7073 - val_loss: 1.1362 - val_acc: 0.6667\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0777 - acc: 0.7088 - val_loss: 1.0019 - val_acc: 0.7500\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0252 - acc: 0.7233 - val_loss: 1.0596 - val_acc: 0.7292\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9868 - acc: 0.7372 - val_loss: 0.9601 - val_acc: 0.8125\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9567 - acc: 0.7517 - val_loss: 0.9583 - val_acc: 0.7917\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9410 - acc: 0.7527 - val_loss: 0.9019 - val_acc: 0.7917\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9130 - acc: 0.7557 - val_loss: 0.8671 - val_acc: 0.8333\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9124 - acc: 0.7632 - val_loss: 0.8668 - val_acc: 0.7708\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8739 - acc: 0.7790 - val_loss: 0.9127 - val_acc: 0.7708\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8716 - acc: 0.7712 - val_loss: 0.9010 - val_acc: 0.8125\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8279 - acc: 0.7820 - val_loss: 0.7951 - val_acc: 0.8333\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8158 - acc: 0.7895 - val_loss: 0.7949 - val_acc: 0.8333\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7872 - acc: 0.7904 - val_loss: 0.8152 - val_acc: 0.8125\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7842 - acc: 0.7895 - val_loss: 0.8713 - val_acc: 0.8125\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7858 - acc: 0.7933 - val_loss: 0.8784 - val_acc: 0.7917\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7586 - acc: 0.8015 - val_loss: 0.8286 - val_acc: 0.8542\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7302 - acc: 0.8093 - val_loss: 0.7895 - val_acc: 0.8125\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7341 - acc: 0.8106 - val_loss: 0.8099 - val_acc: 0.8125\n",
            "processing fold # 64\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 17s 4ms/step - loss: 3.0317 - acc: 0.2082 - val_loss: 2.5188 - val_acc: 0.2708\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.4584 - acc: 0.3305 - val_loss: 2.1764 - val_acc: 0.4167\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.1678 - acc: 0.3978 - val_loss: 1.8995 - val_acc: 0.4167\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.9518 - acc: 0.4563 - val_loss: 1.6736 - val_acc: 0.4375\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.8203 - acc: 0.4935 - val_loss: 1.6535 - val_acc: 0.4583\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.6821 - acc: 0.5391 - val_loss: 1.5204 - val_acc: 0.5417\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.5736 - acc: 0.5652 - val_loss: 1.4436 - val_acc: 0.5625\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4841 - acc: 0.5870 - val_loss: 1.4299 - val_acc: 0.5625\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4046 - acc: 0.6161 - val_loss: 1.3038 - val_acc: 0.5833\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.3322 - acc: 0.6386 - val_loss: 1.2383 - val_acc: 0.6458\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2624 - acc: 0.6602 - val_loss: 1.2086 - val_acc: 0.6458\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2073 - acc: 0.6775 - val_loss: 1.2641 - val_acc: 0.6250\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1493 - acc: 0.6955 - val_loss: 1.2303 - val_acc: 0.6250\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1026 - acc: 0.7086 - val_loss: 1.1006 - val_acc: 0.6875\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0669 - acc: 0.7174 - val_loss: 1.1249 - val_acc: 0.6458\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0167 - acc: 0.7368 - val_loss: 1.1182 - val_acc: 0.6667\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9972 - acc: 0.7403 - val_loss: 1.0551 - val_acc: 0.7083\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9617 - acc: 0.7443 - val_loss: 1.1227 - val_acc: 0.5625\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9215 - acc: 0.7529 - val_loss: 1.0793 - val_acc: 0.7083\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9198 - acc: 0.7487 - val_loss: 1.0970 - val_acc: 0.6667\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8981 - acc: 0.7542 - val_loss: 1.0386 - val_acc: 0.6875\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8692 - acc: 0.7687 - val_loss: 1.0044 - val_acc: 0.7500\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8597 - acc: 0.7738 - val_loss: 1.0140 - val_acc: 0.7083\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8316 - acc: 0.7815 - val_loss: 0.9849 - val_acc: 0.7500\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8212 - acc: 0.7809 - val_loss: 0.9948 - val_acc: 0.7083\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8011 - acc: 0.7889 - val_loss: 1.0091 - val_acc: 0.7083\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8009 - acc: 0.7923 - val_loss: 1.0115 - val_acc: 0.7083\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7688 - acc: 0.8017 - val_loss: 0.9603 - val_acc: 0.7708\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7596 - acc: 0.8055 - val_loss: 1.0032 - val_acc: 0.7292\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7450 - acc: 0.8042 - val_loss: 0.9883 - val_acc: 0.7500\n",
            "processing fold # 65\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 17s 4ms/step - loss: 2.9443 - acc: 0.2183 - val_loss: 2.4824 - val_acc: 0.3750\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.4636 - acc: 0.3217 - val_loss: 2.0524 - val_acc: 0.4167\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.1985 - acc: 0.3827 - val_loss: 1.9513 - val_acc: 0.4583\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 2.0236 - acc: 0.4338 - val_loss: 1.6925 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.8226 - acc: 0.5029 - val_loss: 1.6796 - val_acc: 0.6250\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.6449 - acc: 0.5536 - val_loss: 1.5196 - val_acc: 0.6042\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5362 - acc: 0.5719 - val_loss: 1.5357 - val_acc: 0.5833\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4361 - acc: 0.6062 - val_loss: 1.4021 - val_acc: 0.6042\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3379 - acc: 0.6341 - val_loss: 1.3641 - val_acc: 0.6250\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2553 - acc: 0.6644 - val_loss: 1.3064 - val_acc: 0.6667\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1904 - acc: 0.6745 - val_loss: 1.2117 - val_acc: 0.7292\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.1459 - acc: 0.6966 - val_loss: 1.3547 - val_acc: 0.6458\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0999 - acc: 0.7119 - val_loss: 1.1280 - val_acc: 0.7083\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0474 - acc: 0.7206 - val_loss: 1.2275 - val_acc: 0.7083\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0012 - acc: 0.7403 - val_loss: 1.2068 - val_acc: 0.7083\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9841 - acc: 0.7368 - val_loss: 1.2629 - val_acc: 0.7083\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9711 - acc: 0.7439 - val_loss: 1.1600 - val_acc: 0.7500\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9313 - acc: 0.7523 - val_loss: 1.1307 - val_acc: 0.7292\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8989 - acc: 0.7597 - val_loss: 1.2626 - val_acc: 0.7083\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8855 - acc: 0.7651 - val_loss: 1.1074 - val_acc: 0.7083\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8425 - acc: 0.7855 - val_loss: 1.1588 - val_acc: 0.7292\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8408 - acc: 0.7822 - val_loss: 1.2276 - val_acc: 0.7083\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8284 - acc: 0.7864 - val_loss: 1.1807 - val_acc: 0.7292\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8097 - acc: 0.7855 - val_loss: 1.2279 - val_acc: 0.6458\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7941 - acc: 0.7853 - val_loss: 1.2139 - val_acc: 0.7083\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7796 - acc: 0.7963 - val_loss: 1.2425 - val_acc: 0.7500\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7516 - acc: 0.8009 - val_loss: 1.2026 - val_acc: 0.6667\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7505 - acc: 0.8063 - val_loss: 1.0897 - val_acc: 0.7292\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7274 - acc: 0.8093 - val_loss: 1.2155 - val_acc: 0.7292\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7324 - acc: 0.8045 - val_loss: 1.1188 - val_acc: 0.7292\n",
            "processing fold # 66\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 18s 4ms/step - loss: 2.9179 - acc: 0.2117 - val_loss: 2.5142 - val_acc: 0.1875\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.3483 - acc: 0.3591 - val_loss: 2.1788 - val_acc: 0.3958\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.0544 - acc: 0.4306 - val_loss: 2.0002 - val_acc: 0.3542\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.8661 - acc: 0.4727 - val_loss: 1.6837 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.7034 - acc: 0.5326 - val_loss: 1.4423 - val_acc: 0.6250\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.5672 - acc: 0.5669 - val_loss: 1.3728 - val_acc: 0.6042\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4573 - acc: 0.6034 - val_loss: 1.2558 - val_acc: 0.6667\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3598 - acc: 0.6207 - val_loss: 1.2755 - val_acc: 0.6875\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2879 - acc: 0.6491 - val_loss: 1.2643 - val_acc: 0.7083\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2448 - acc: 0.6638 - val_loss: 1.2815 - val_acc: 0.6667\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1755 - acc: 0.6810 - val_loss: 1.0211 - val_acc: 0.7083\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1162 - acc: 0.7033 - val_loss: 1.1469 - val_acc: 0.7083\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0877 - acc: 0.7069 - val_loss: 1.2054 - val_acc: 0.6875\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0478 - acc: 0.7248 - val_loss: 1.0340 - val_acc: 0.7500\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0185 - acc: 0.7237 - val_loss: 0.9933 - val_acc: 0.7500\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9836 - acc: 0.7405 - val_loss: 1.0118 - val_acc: 0.7292\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9337 - acc: 0.7529 - val_loss: 0.9375 - val_acc: 0.7708\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.9091 - acc: 0.7588 - val_loss: 0.9491 - val_acc: 0.7083\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9025 - acc: 0.7599 - val_loss: 0.9113 - val_acc: 0.7292\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8704 - acc: 0.7649 - val_loss: 0.9937 - val_acc: 0.7500\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8475 - acc: 0.7729 - val_loss: 0.9325 - val_acc: 0.7708\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8300 - acc: 0.7763 - val_loss: 0.9018 - val_acc: 0.7708\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8129 - acc: 0.7878 - val_loss: 0.8860 - val_acc: 0.7292\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7934 - acc: 0.7933 - val_loss: 0.9486 - val_acc: 0.6875\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7646 - acc: 0.7981 - val_loss: 0.8635 - val_acc: 0.7917\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7805 - acc: 0.7981 - val_loss: 0.9163 - val_acc: 0.7292\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7691 - acc: 0.7975 - val_loss: 0.8872 - val_acc: 0.7500\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7287 - acc: 0.8095 - val_loss: 0.8443 - val_acc: 0.7708\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7106 - acc: 0.8158 - val_loss: 0.9373 - val_acc: 0.7500\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7040 - acc: 0.8188 - val_loss: 0.9294 - val_acc: 0.7500\n",
            "processing fold # 67\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 18s 4ms/step - loss: 2.9901 - acc: 0.2050 - val_loss: 2.7681 - val_acc: 0.2292\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.4034 - acc: 0.3440 - val_loss: 2.5052 - val_acc: 0.3125\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.1502 - acc: 0.4024 - val_loss: 2.3088 - val_acc: 0.3333\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.9363 - acc: 0.4531 - val_loss: 2.1804 - val_acc: 0.3542\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.7610 - acc: 0.5107 - val_loss: 1.9024 - val_acc: 0.4583\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.6384 - acc: 0.5498 - val_loss: 1.7730 - val_acc: 0.5208\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.5251 - acc: 0.5803 - val_loss: 1.7520 - val_acc: 0.5417\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.4361 - acc: 0.6053 - val_loss: 1.7039 - val_acc: 0.5625\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3544 - acc: 0.6268 - val_loss: 1.6898 - val_acc: 0.5417\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2898 - acc: 0.6499 - val_loss: 1.6093 - val_acc: 0.6250\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2087 - acc: 0.6724 - val_loss: 1.7419 - val_acc: 0.5625\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1771 - acc: 0.6783 - val_loss: 1.5780 - val_acc: 0.5833\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1159 - acc: 0.6983 - val_loss: 1.5136 - val_acc: 0.6250\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0565 - acc: 0.7206 - val_loss: 1.4500 - val_acc: 0.6250\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0273 - acc: 0.7235 - val_loss: 1.5019 - val_acc: 0.6875\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0051 - acc: 0.7275 - val_loss: 1.5051 - val_acc: 0.6042\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9953 - acc: 0.7338 - val_loss: 1.5229 - val_acc: 0.6042\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9497 - acc: 0.7483 - val_loss: 1.4723 - val_acc: 0.6875\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9203 - acc: 0.7578 - val_loss: 1.5296 - val_acc: 0.6667\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8915 - acc: 0.7653 - val_loss: 1.5109 - val_acc: 0.6458\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8671 - acc: 0.7733 - val_loss: 1.4409 - val_acc: 0.6875\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8543 - acc: 0.7740 - val_loss: 1.3727 - val_acc: 0.7292\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8296 - acc: 0.7794 - val_loss: 1.4944 - val_acc: 0.6667\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8235 - acc: 0.7773 - val_loss: 1.4226 - val_acc: 0.7708\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7945 - acc: 0.7948 - val_loss: 1.4219 - val_acc: 0.6667\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7684 - acc: 0.8076 - val_loss: 1.4332 - val_acc: 0.7083\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7679 - acc: 0.8042 - val_loss: 1.4784 - val_acc: 0.6875\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7554 - acc: 0.8005 - val_loss: 1.4612 - val_acc: 0.6875\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7498 - acc: 0.8030 - val_loss: 1.3719 - val_acc: 0.7292\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7283 - acc: 0.8118 - val_loss: 1.3735 - val_acc: 0.7083\n",
            "processing fold # 68\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 18s 4ms/step - loss: 2.9002 - acc: 0.2176 - val_loss: 2.2934 - val_acc: 0.4583\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.3185 - acc: 0.3734 - val_loss: 1.9251 - val_acc: 0.3958\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 30us/step - loss: 2.0220 - acc: 0.4418 - val_loss: 1.7937 - val_acc: 0.5000\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 26us/step - loss: 1.8450 - acc: 0.4895 - val_loss: 1.6365 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.6581 - acc: 0.5393 - val_loss: 1.4541 - val_acc: 0.6042\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.5280 - acc: 0.5749 - val_loss: 1.2367 - val_acc: 0.6875\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.4051 - acc: 0.6184 - val_loss: 1.1719 - val_acc: 0.7500\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.3345 - acc: 0.6356 - val_loss: 1.2447 - val_acc: 0.7500\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2686 - acc: 0.6564 - val_loss: 0.9861 - val_acc: 0.7292\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1899 - acc: 0.6815 - val_loss: 0.9687 - val_acc: 0.7708\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1382 - acc: 0.6960 - val_loss: 0.9401 - val_acc: 0.7500\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0994 - acc: 0.7096 - val_loss: 0.9103 - val_acc: 0.7708\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0522 - acc: 0.7250 - val_loss: 0.9436 - val_acc: 0.7292\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.0155 - acc: 0.7340 - val_loss: 0.9647 - val_acc: 0.7292\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9706 - acc: 0.7498 - val_loss: 0.9015 - val_acc: 0.7292\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9561 - acc: 0.7489 - val_loss: 0.8925 - val_acc: 0.7708\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9164 - acc: 0.7628 - val_loss: 0.8405 - val_acc: 0.7500\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9070 - acc: 0.7647 - val_loss: 0.8082 - val_acc: 0.7708\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8615 - acc: 0.7786 - val_loss: 0.8263 - val_acc: 0.7917\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8482 - acc: 0.7757 - val_loss: 0.8053 - val_acc: 0.8125\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8420 - acc: 0.7807 - val_loss: 0.7420 - val_acc: 0.8125\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8231 - acc: 0.7862 - val_loss: 0.7674 - val_acc: 0.7917\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7999 - acc: 0.7912 - val_loss: 0.7376 - val_acc: 0.8125\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7823 - acc: 0.7954 - val_loss: 0.7639 - val_acc: 0.8125\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7729 - acc: 0.7998 - val_loss: 0.7102 - val_acc: 0.8125\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7509 - acc: 0.8101 - val_loss: 0.7914 - val_acc: 0.7708\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7308 - acc: 0.8097 - val_loss: 0.7012 - val_acc: 0.8125\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7180 - acc: 0.8082 - val_loss: 0.8099 - val_acc: 0.7708\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7158 - acc: 0.8156 - val_loss: 0.8539 - val_acc: 0.7917\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.6904 - acc: 0.8217 - val_loss: 0.7383 - val_acc: 0.8125\n",
            "processing fold # 69\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 18s 4ms/step - loss: 2.8778 - acc: 0.2168 - val_loss: 2.6672 - val_acc: 0.1250\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.3718 - acc: 0.3471 - val_loss: 2.3145 - val_acc: 0.3125\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.0738 - acc: 0.4163 - val_loss: 1.8851 - val_acc: 0.4792\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.8290 - acc: 0.4910 - val_loss: 1.4825 - val_acc: 0.5417\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.6371 - acc: 0.5475 - val_loss: 1.3800 - val_acc: 0.6250\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.5152 - acc: 0.5862 - val_loss: 1.2925 - val_acc: 0.7292\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4242 - acc: 0.6154 - val_loss: 1.2456 - val_acc: 0.6667\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.3312 - acc: 0.6428 - val_loss: 1.1079 - val_acc: 0.7708\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2534 - acc: 0.6615 - val_loss: 1.0811 - val_acc: 0.7708\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 26us/step - loss: 1.2079 - acc: 0.6756 - val_loss: 1.0476 - val_acc: 0.7500\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1433 - acc: 0.6947 - val_loss: 1.0614 - val_acc: 0.7500\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0834 - acc: 0.7140 - val_loss: 1.0218 - val_acc: 0.7917\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0484 - acc: 0.7235 - val_loss: 0.9573 - val_acc: 0.7917\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0014 - acc: 0.7372 - val_loss: 1.0357 - val_acc: 0.7708\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9723 - acc: 0.7464 - val_loss: 0.9890 - val_acc: 0.7292\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9413 - acc: 0.7429 - val_loss: 0.9268 - val_acc: 0.8125\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9288 - acc: 0.7542 - val_loss: 0.9975 - val_acc: 0.7917\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8885 - acc: 0.7664 - val_loss: 0.9669 - val_acc: 0.8125\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8763 - acc: 0.7744 - val_loss: 0.8442 - val_acc: 0.8125\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8384 - acc: 0.7792 - val_loss: 0.8568 - val_acc: 0.8125\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8462 - acc: 0.7813 - val_loss: 0.8627 - val_acc: 0.8125\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8035 - acc: 0.7918 - val_loss: 0.8421 - val_acc: 0.8542\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7889 - acc: 0.7967 - val_loss: 0.8812 - val_acc: 0.8125\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7696 - acc: 0.8034 - val_loss: 0.9066 - val_acc: 0.7917\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7600 - acc: 0.8034 - val_loss: 0.7676 - val_acc: 0.7917\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7438 - acc: 0.8053 - val_loss: 0.8031 - val_acc: 0.7917\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7435 - acc: 0.8091 - val_loss: 0.8194 - val_acc: 0.7500\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7325 - acc: 0.8156 - val_loss: 0.8574 - val_acc: 0.8333\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.6967 - acc: 0.8238 - val_loss: 0.8210 - val_acc: 0.8542\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.6876 - acc: 0.8276 - val_loss: 0.8633 - val_acc: 0.8333\n",
            "processing fold # 70\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 18s 4ms/step - loss: 2.9669 - acc: 0.1983 - val_loss: 2.3870 - val_acc: 0.3125\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 2.3926 - acc: 0.3377 - val_loss: 2.1922 - val_acc: 0.4375\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.1314 - acc: 0.4056 - val_loss: 1.9723 - val_acc: 0.4583\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.8956 - acc: 0.4792 - val_loss: 1.8585 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.7130 - acc: 0.5309 - val_loss: 1.6406 - val_acc: 0.5833\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.5635 - acc: 0.5685 - val_loss: 1.6532 - val_acc: 0.5000\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4828 - acc: 0.5917 - val_loss: 1.4974 - val_acc: 0.6250\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3938 - acc: 0.6095 - val_loss: 1.4339 - val_acc: 0.6458\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2909 - acc: 0.6585 - val_loss: 1.4168 - val_acc: 0.6875\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2321 - acc: 0.6646 - val_loss: 1.2451 - val_acc: 0.6458\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1880 - acc: 0.6905 - val_loss: 1.2342 - val_acc: 0.7083\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1121 - acc: 0.7065 - val_loss: 1.1893 - val_acc: 0.7083\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0740 - acc: 0.7134 - val_loss: 1.2006 - val_acc: 0.7083\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0379 - acc: 0.7338 - val_loss: 1.1417 - val_acc: 0.6875\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9835 - acc: 0.7420 - val_loss: 1.0517 - val_acc: 0.7708\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9586 - acc: 0.7456 - val_loss: 1.1138 - val_acc: 0.6458\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9321 - acc: 0.7525 - val_loss: 1.0576 - val_acc: 0.6875\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9134 - acc: 0.7557 - val_loss: 1.1033 - val_acc: 0.7083\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8688 - acc: 0.7685 - val_loss: 0.9951 - val_acc: 0.7292\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8609 - acc: 0.7748 - val_loss: 1.0863 - val_acc: 0.7292\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8294 - acc: 0.7826 - val_loss: 1.0798 - val_acc: 0.7083\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.8166 - acc: 0.7885 - val_loss: 1.1440 - val_acc: 0.6875\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8144 - acc: 0.7847 - val_loss: 1.0724 - val_acc: 0.6875\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7800 - acc: 0.7965 - val_loss: 1.0095 - val_acc: 0.7292\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7766 - acc: 0.7984 - val_loss: 1.0567 - val_acc: 0.7708\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7654 - acc: 0.8011 - val_loss: 1.0476 - val_acc: 0.7083\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7430 - acc: 0.8143 - val_loss: 1.1136 - val_acc: 0.7500\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7328 - acc: 0.8089 - val_loss: 1.0265 - val_acc: 0.7708\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7111 - acc: 0.8133 - val_loss: 1.0636 - val_acc: 0.7500\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.6950 - acc: 0.8156 - val_loss: 1.0711 - val_acc: 0.6875\n",
            "processing fold # 71\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 19s 4ms/step - loss: 3.0013 - acc: 0.2071 - val_loss: 2.4835 - val_acc: 0.3125\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.3607 - acc: 0.3574 - val_loss: 2.2161 - val_acc: 0.3542\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.1033 - acc: 0.4272 - val_loss: 1.9695 - val_acc: 0.4792\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.8810 - acc: 0.4851 - val_loss: 1.8422 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.7828 - acc: 0.5103 - val_loss: 1.7627 - val_acc: 0.4792\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.6407 - acc: 0.5435 - val_loss: 1.6829 - val_acc: 0.5417\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.5308 - acc: 0.5753 - val_loss: 1.5479 - val_acc: 0.5417\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4422 - acc: 0.5990 - val_loss: 1.4236 - val_acc: 0.6042\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3658 - acc: 0.6201 - val_loss: 1.4289 - val_acc: 0.6250\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3025 - acc: 0.6426 - val_loss: 1.4538 - val_acc: 0.5208\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.2365 - acc: 0.6634 - val_loss: 1.3767 - val_acc: 0.5417\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1843 - acc: 0.6846 - val_loss: 1.3118 - val_acc: 0.5625\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1462 - acc: 0.6945 - val_loss: 1.2423 - val_acc: 0.6250\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0914 - acc: 0.7119 - val_loss: 1.1978 - val_acc: 0.6250\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0580 - acc: 0.7157 - val_loss: 1.2724 - val_acc: 0.5833\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0543 - acc: 0.7206 - val_loss: 1.1790 - val_acc: 0.6458\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0054 - acc: 0.7347 - val_loss: 1.2580 - val_acc: 0.6458\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9593 - acc: 0.7460 - val_loss: 1.1513 - val_acc: 0.7083\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9044 - acc: 0.7624 - val_loss: 1.2418 - val_acc: 0.6458\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9011 - acc: 0.7630 - val_loss: 1.1457 - val_acc: 0.6875\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8964 - acc: 0.7681 - val_loss: 1.2621 - val_acc: 0.6042\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8773 - acc: 0.7689 - val_loss: 1.2466 - val_acc: 0.6458\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8549 - acc: 0.7824 - val_loss: 1.1688 - val_acc: 0.6875\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8313 - acc: 0.7851 - val_loss: 1.1428 - val_acc: 0.6875\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8230 - acc: 0.7866 - val_loss: 1.0989 - val_acc: 0.7083\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7881 - acc: 0.7883 - val_loss: 1.0964 - val_acc: 0.7292\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7705 - acc: 0.8034 - val_loss: 1.0649 - val_acc: 0.7708\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 0.7616 - acc: 0.8015 - val_loss: 1.1051 - val_acc: 0.7292\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7503 - acc: 0.8028 - val_loss: 1.1262 - val_acc: 0.7708\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7366 - acc: 0.8089 - val_loss: 1.1180 - val_acc: 0.7292\n",
            "processing fold # 72\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 19s 4ms/step - loss: 3.0017 - acc: 0.2077 - val_loss: 2.3139 - val_acc: 0.4583\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.4417 - acc: 0.3394 - val_loss: 2.2129 - val_acc: 0.4375\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.1314 - acc: 0.4113 - val_loss: 1.8994 - val_acc: 0.5625\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.9289 - acc: 0.4607 - val_loss: 1.7401 - val_acc: 0.5625\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.7494 - acc: 0.5170 - val_loss: 1.8616 - val_acc: 0.5208\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.5994 - acc: 0.5589 - val_loss: 1.7162 - val_acc: 0.5833\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.4877 - acc: 0.5934 - val_loss: 1.6449 - val_acc: 0.6042\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3877 - acc: 0.6198 - val_loss: 1.5713 - val_acc: 0.6042\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3066 - acc: 0.6537 - val_loss: 1.5399 - val_acc: 0.5833\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2118 - acc: 0.6733 - val_loss: 1.6047 - val_acc: 0.5833\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1782 - acc: 0.6913 - val_loss: 1.4959 - val_acc: 0.6250\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1168 - acc: 0.7094 - val_loss: 1.5003 - val_acc: 0.6458\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0799 - acc: 0.7178 - val_loss: 1.4070 - val_acc: 0.6458\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0454 - acc: 0.7241 - val_loss: 1.4716 - val_acc: 0.6042\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0188 - acc: 0.7336 - val_loss: 1.3669 - val_acc: 0.6250\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0011 - acc: 0.7363 - val_loss: 1.4595 - val_acc: 0.6458\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9594 - acc: 0.7494 - val_loss: 1.4852 - val_acc: 0.6458\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9140 - acc: 0.7658 - val_loss: 1.3690 - val_acc: 0.7083\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8951 - acc: 0.7672 - val_loss: 1.3823 - val_acc: 0.6875\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8780 - acc: 0.7698 - val_loss: 1.4899 - val_acc: 0.6875\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8628 - acc: 0.7759 - val_loss: 1.5161 - val_acc: 0.6667\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8565 - acc: 0.7778 - val_loss: 1.3320 - val_acc: 0.6875\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8410 - acc: 0.7767 - val_loss: 1.4575 - val_acc: 0.6042\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8281 - acc: 0.7803 - val_loss: 1.4030 - val_acc: 0.6875\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7819 - acc: 0.7988 - val_loss: 1.4737 - val_acc: 0.5833\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7888 - acc: 0.7967 - val_loss: 1.4221 - val_acc: 0.6875\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7765 - acc: 0.8009 - val_loss: 1.3688 - val_acc: 0.6875\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7446 - acc: 0.8091 - val_loss: 1.4155 - val_acc: 0.7083\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7261 - acc: 0.8101 - val_loss: 1.3762 - val_acc: 0.7083\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7143 - acc: 0.8179 - val_loss: 1.4245 - val_acc: 0.6875\n",
            "processing fold # 73\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 19s 4ms/step - loss: 2.9867 - acc: 0.2117 - val_loss: 2.5714 - val_acc: 0.2708\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.4420 - acc: 0.3257 - val_loss: 2.4050 - val_acc: 0.2917\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.1831 - acc: 0.3846 - val_loss: 2.1660 - val_acc: 0.5000\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.9751 - acc: 0.4344 - val_loss: 1.9475 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.8103 - acc: 0.4861 - val_loss: 1.9905 - val_acc: 0.5417\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.6441 - acc: 0.5381 - val_loss: 1.8850 - val_acc: 0.6250\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.5382 - acc: 0.5641 - val_loss: 1.8633 - val_acc: 0.6250\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4444 - acc: 0.6062 - val_loss: 1.7506 - val_acc: 0.6042\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3781 - acc: 0.6243 - val_loss: 1.7617 - val_acc: 0.6042\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.2841 - acc: 0.6476 - val_loss: 1.7255 - val_acc: 0.6042\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2122 - acc: 0.6644 - val_loss: 1.7007 - val_acc: 0.6042\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1586 - acc: 0.6808 - val_loss: 1.6958 - val_acc: 0.6250\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1204 - acc: 0.7010 - val_loss: 1.6371 - val_acc: 0.6250\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0644 - acc: 0.7149 - val_loss: 1.6873 - val_acc: 0.6250\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0212 - acc: 0.7283 - val_loss: 1.6473 - val_acc: 0.6042\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9856 - acc: 0.7349 - val_loss: 1.6960 - val_acc: 0.6667\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9571 - acc: 0.7420 - val_loss: 1.6821 - val_acc: 0.6875\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9367 - acc: 0.7513 - val_loss: 1.7167 - val_acc: 0.6875\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9012 - acc: 0.7662 - val_loss: 1.7237 - val_acc: 0.6667\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8964 - acc: 0.7590 - val_loss: 1.5446 - val_acc: 0.7083\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8480 - acc: 0.7784 - val_loss: 1.7022 - val_acc: 0.6458\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8458 - acc: 0.7843 - val_loss: 1.5686 - val_acc: 0.7083\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8113 - acc: 0.7912 - val_loss: 1.5332 - val_acc: 0.6875\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8168 - acc: 0.7868 - val_loss: 1.5722 - val_acc: 0.7292\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8027 - acc: 0.7950 - val_loss: 1.5563 - val_acc: 0.6875\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7633 - acc: 0.8057 - val_loss: 1.5675 - val_acc: 0.7083\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7450 - acc: 0.8007 - val_loss: 1.4737 - val_acc: 0.7292\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7316 - acc: 0.8173 - val_loss: 1.5263 - val_acc: 0.7292\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7191 - acc: 0.8162 - val_loss: 1.4632 - val_acc: 0.7083\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7000 - acc: 0.8167 - val_loss: 1.4723 - val_acc: 0.6875\n",
            "processing fold # 74\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 19s 4ms/step - loss: 3.0943 - acc: 0.1644 - val_loss: 2.4502 - val_acc: 0.3958\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.5037 - acc: 0.3063 - val_loss: 2.1272 - val_acc: 0.4167\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.1737 - acc: 0.3987 - val_loss: 1.8284 - val_acc: 0.5417\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.9221 - acc: 0.4802 - val_loss: 1.7108 - val_acc: 0.4792\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.7525 - acc: 0.5187 - val_loss: 1.4925 - val_acc: 0.5833\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.5987 - acc: 0.5654 - val_loss: 1.3320 - val_acc: 0.6458\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.5166 - acc: 0.5879 - val_loss: 1.2835 - val_acc: 0.6458\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4118 - acc: 0.6121 - val_loss: 1.2307 - val_acc: 0.6458\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3572 - acc: 0.6316 - val_loss: 1.1906 - val_acc: 0.6250\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2642 - acc: 0.6600 - val_loss: 1.1387 - val_acc: 0.6875\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2163 - acc: 0.6730 - val_loss: 1.0236 - val_acc: 0.6875\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1522 - acc: 0.6941 - val_loss: 0.9648 - val_acc: 0.7500\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1097 - acc: 0.7058 - val_loss: 1.0318 - val_acc: 0.6875\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0624 - acc: 0.7161 - val_loss: 0.9798 - val_acc: 0.6875\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0275 - acc: 0.7334 - val_loss: 0.9105 - val_acc: 0.6667\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9929 - acc: 0.7407 - val_loss: 0.9549 - val_acc: 0.6875\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9754 - acc: 0.7471 - val_loss: 0.9015 - val_acc: 0.6667\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9453 - acc: 0.7536 - val_loss: 1.0024 - val_acc: 0.7083\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9030 - acc: 0.7614 - val_loss: 0.9272 - val_acc: 0.7292\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8864 - acc: 0.7693 - val_loss: 0.9484 - val_acc: 0.6875\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8614 - acc: 0.7784 - val_loss: 0.8312 - val_acc: 0.6667\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8638 - acc: 0.7750 - val_loss: 0.9493 - val_acc: 0.6875\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8435 - acc: 0.7847 - val_loss: 0.8939 - val_acc: 0.7083\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8209 - acc: 0.7883 - val_loss: 0.9852 - val_acc: 0.7083\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8056 - acc: 0.7878 - val_loss: 0.9399 - val_acc: 0.6875\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7896 - acc: 0.7979 - val_loss: 0.8779 - val_acc: 0.6875\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7586 - acc: 0.8045 - val_loss: 0.9530 - val_acc: 0.6875\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7857 - acc: 0.7937 - val_loss: 0.8712 - val_acc: 0.7292\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7446 - acc: 0.8070 - val_loss: 0.8917 - val_acc: 0.7083\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7340 - acc: 0.8080 - val_loss: 0.8798 - val_acc: 0.7292\n",
            "processing fold # 75\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 20s 4ms/step - loss: 3.0264 - acc: 0.1960 - val_loss: 2.4393 - val_acc: 0.3333\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.4507 - acc: 0.3337 - val_loss: 2.0155 - val_acc: 0.4167\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.1783 - acc: 0.4008 - val_loss: 1.9688 - val_acc: 0.5208\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.9965 - acc: 0.4502 - val_loss: 1.8448 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.8546 - acc: 0.4811 - val_loss: 1.5930 - val_acc: 0.6042\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.6987 - acc: 0.5290 - val_loss: 1.5235 - val_acc: 0.6667\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.5719 - acc: 0.5557 - val_loss: 1.4473 - val_acc: 0.6042\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.4860 - acc: 0.5875 - val_loss: 1.3992 - val_acc: 0.6875\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3860 - acc: 0.6131 - val_loss: 1.2774 - val_acc: 0.6875\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.3043 - acc: 0.6400 - val_loss: 1.2173 - val_acc: 0.6875\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2353 - acc: 0.6564 - val_loss: 1.2342 - val_acc: 0.7292\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1838 - acc: 0.6758 - val_loss: 1.2400 - val_acc: 0.7083\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1369 - acc: 0.6926 - val_loss: 1.2165 - val_acc: 0.6875\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0864 - acc: 0.7126 - val_loss: 1.2750 - val_acc: 0.6250\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0516 - acc: 0.7155 - val_loss: 1.2363 - val_acc: 0.6875\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0102 - acc: 0.7281 - val_loss: 1.0642 - val_acc: 0.7292\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9840 - acc: 0.7368 - val_loss: 1.2247 - val_acc: 0.7083\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9572 - acc: 0.7473 - val_loss: 1.1864 - val_acc: 0.7083\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9308 - acc: 0.7504 - val_loss: 1.1267 - val_acc: 0.7083\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9098 - acc: 0.7622 - val_loss: 1.1127 - val_acc: 0.7500\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8830 - acc: 0.7620 - val_loss: 1.0693 - val_acc: 0.7500\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8412 - acc: 0.7851 - val_loss: 0.9949 - val_acc: 0.7500\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8513 - acc: 0.7710 - val_loss: 1.0789 - val_acc: 0.7500\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8193 - acc: 0.7841 - val_loss: 1.1240 - val_acc: 0.7292\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7984 - acc: 0.7895 - val_loss: 1.0511 - val_acc: 0.7500\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7948 - acc: 0.7883 - val_loss: 1.0437 - val_acc: 0.7292\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7808 - acc: 0.7876 - val_loss: 1.0254 - val_acc: 0.7292\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7775 - acc: 0.7958 - val_loss: 0.9168 - val_acc: 0.7500\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7563 - acc: 0.8049 - val_loss: 1.0489 - val_acc: 0.7500\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7373 - acc: 0.8095 - val_loss: 0.9899 - val_acc: 0.7083\n",
            "processing fold # 76\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 20s 4ms/step - loss: 3.0268 - acc: 0.2063 - val_loss: 2.7950 - val_acc: 0.3125\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 2.4394 - acc: 0.3499 - val_loss: 2.5776 - val_acc: 0.2708\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.1957 - acc: 0.4010 - val_loss: 2.3569 - val_acc: 0.3125\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.0049 - acc: 0.4521 - val_loss: 2.2938 - val_acc: 0.3958\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.8505 - acc: 0.4874 - val_loss: 1.9593 - val_acc: 0.3958\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.6895 - acc: 0.5278 - val_loss: 1.7204 - val_acc: 0.5000\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.5743 - acc: 0.5624 - val_loss: 1.5917 - val_acc: 0.5208\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.4815 - acc: 0.5864 - val_loss: 1.5546 - val_acc: 0.5833\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4011 - acc: 0.6161 - val_loss: 1.4781 - val_acc: 0.6250\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3296 - acc: 0.6436 - val_loss: 1.4867 - val_acc: 0.6042\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 21us/step - loss: 1.2642 - acc: 0.6583 - val_loss: 1.4409 - val_acc: 0.6458\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2170 - acc: 0.6653 - val_loss: 1.4811 - val_acc: 0.6042\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1638 - acc: 0.6861 - val_loss: 1.4945 - val_acc: 0.6250\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1266 - acc: 0.7016 - val_loss: 1.3547 - val_acc: 0.7083\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0837 - acc: 0.7111 - val_loss: 1.3156 - val_acc: 0.6875\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0496 - acc: 0.7197 - val_loss: 1.1876 - val_acc: 0.7083\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0075 - acc: 0.7325 - val_loss: 1.3391 - val_acc: 0.6875\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9874 - acc: 0.7338 - val_loss: 1.4355 - val_acc: 0.6250\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9631 - acc: 0.7443 - val_loss: 1.3166 - val_acc: 0.6875\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9357 - acc: 0.7498 - val_loss: 1.4047 - val_acc: 0.6875\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8983 - acc: 0.7620 - val_loss: 1.4204 - val_acc: 0.7083\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8897 - acc: 0.7681 - val_loss: 1.3343 - val_acc: 0.6875\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8789 - acc: 0.7660 - val_loss: 1.4027 - val_acc: 0.6250\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8485 - acc: 0.7763 - val_loss: 1.2417 - val_acc: 0.7083\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8325 - acc: 0.7809 - val_loss: 1.3357 - val_acc: 0.7500\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8199 - acc: 0.7853 - val_loss: 1.3453 - val_acc: 0.6875\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7812 - acc: 0.7986 - val_loss: 1.3230 - val_acc: 0.7083\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7648 - acc: 0.7977 - val_loss: 1.3645 - val_acc: 0.7292\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7577 - acc: 0.8059 - val_loss: 1.4357 - val_acc: 0.7292\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7403 - acc: 0.8110 - val_loss: 1.3880 - val_acc: 0.7083\n",
            "processing fold # 77\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 20s 4ms/step - loss: 2.9587 - acc: 0.2077 - val_loss: 2.5843 - val_acc: 0.3542\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.4010 - acc: 0.3352 - val_loss: 2.3242 - val_acc: 0.3750\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.1296 - acc: 0.4174 - val_loss: 2.1064 - val_acc: 0.4583\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.8894 - acc: 0.4796 - val_loss: 1.9126 - val_acc: 0.4792\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.7328 - acc: 0.5257 - val_loss: 1.7345 - val_acc: 0.5625\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.6106 - acc: 0.5553 - val_loss: 1.6235 - val_acc: 0.5625\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4838 - acc: 0.5921 - val_loss: 1.4425 - val_acc: 0.5208\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.4056 - acc: 0.6135 - val_loss: 1.3939 - val_acc: 0.5417\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3496 - acc: 0.6297 - val_loss: 1.2554 - val_acc: 0.6458\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2534 - acc: 0.6526 - val_loss: 1.2492 - val_acc: 0.6250\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2124 - acc: 0.6743 - val_loss: 1.1932 - val_acc: 0.5833\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1594 - acc: 0.6859 - val_loss: 1.0792 - val_acc: 0.7292\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0923 - acc: 0.7090 - val_loss: 1.0447 - val_acc: 0.7083\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0483 - acc: 0.7208 - val_loss: 1.1046 - val_acc: 0.6667\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0602 - acc: 0.7208 - val_loss: 1.0418 - val_acc: 0.6875\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0025 - acc: 0.7321 - val_loss: 1.0178 - val_acc: 0.7083\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9781 - acc: 0.7412 - val_loss: 0.9973 - val_acc: 0.7500\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9350 - acc: 0.7525 - val_loss: 0.9883 - val_acc: 0.7292\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8977 - acc: 0.7645 - val_loss: 0.9627 - val_acc: 0.7708\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8878 - acc: 0.7733 - val_loss: 0.9384 - val_acc: 0.7083\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8535 - acc: 0.7788 - val_loss: 0.9116 - val_acc: 0.7292\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8342 - acc: 0.7815 - val_loss: 0.8741 - val_acc: 0.7083\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8471 - acc: 0.7805 - val_loss: 0.8706 - val_acc: 0.7292\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7986 - acc: 0.7971 - val_loss: 0.9100 - val_acc: 0.7500\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7925 - acc: 0.8021 - val_loss: 0.8977 - val_acc: 0.7708\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7634 - acc: 0.8061 - val_loss: 0.8378 - val_acc: 0.7917\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7652 - acc: 0.8087 - val_loss: 0.8943 - val_acc: 0.6875\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7443 - acc: 0.8038 - val_loss: 0.8745 - val_acc: 0.7292\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7500 - acc: 0.8110 - val_loss: 0.8493 - val_acc: 0.7500\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7302 - acc: 0.8087 - val_loss: 0.8470 - val_acc: 0.7708\n",
            "processing fold # 78\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 20s 4ms/step - loss: 2.9362 - acc: 0.2233 - val_loss: 2.6271 - val_acc: 0.2708\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.4262 - acc: 0.3370 - val_loss: 2.4355 - val_acc: 0.2500\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.2003 - acc: 0.3844 - val_loss: 2.3401 - val_acc: 0.2917\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.0367 - acc: 0.4241 - val_loss: 2.1567 - val_acc: 0.3542\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 28us/step - loss: 1.8449 - acc: 0.4830 - val_loss: 1.8906 - val_acc: 0.3750\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 27us/step - loss: 1.6865 - acc: 0.5328 - val_loss: 1.7754 - val_acc: 0.5000\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 27us/step - loss: 1.5698 - acc: 0.5669 - val_loss: 1.6597 - val_acc: 0.5417\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.4497 - acc: 0.5921 - val_loss: 1.5475 - val_acc: 0.5417\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 28us/step - loss: 1.3703 - acc: 0.6276 - val_loss: 1.5231 - val_acc: 0.5417\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.3037 - acc: 0.6440 - val_loss: 1.3698 - val_acc: 0.5625\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 26us/step - loss: 1.2473 - acc: 0.6573 - val_loss: 1.3993 - val_acc: 0.6042\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.2015 - acc: 0.6796 - val_loss: 1.3614 - val_acc: 0.6042\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1366 - acc: 0.6970 - val_loss: 1.3737 - val_acc: 0.6042\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 35us/step - loss: 1.1038 - acc: 0.7033 - val_loss: 1.1514 - val_acc: 0.6875\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0479 - acc: 0.7212 - val_loss: 1.2231 - val_acc: 0.6667\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0254 - acc: 0.7286 - val_loss: 1.1599 - val_acc: 0.7292\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9710 - acc: 0.7426 - val_loss: 1.1039 - val_acc: 0.6458\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9626 - acc: 0.7481 - val_loss: 1.0438 - val_acc: 0.7500\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9385 - acc: 0.7588 - val_loss: 1.0438 - val_acc: 0.6875\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9124 - acc: 0.7590 - val_loss: 1.0992 - val_acc: 0.6875\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8942 - acc: 0.7689 - val_loss: 1.1258 - val_acc: 0.6875\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8756 - acc: 0.7691 - val_loss: 1.1480 - val_acc: 0.6875\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8494 - acc: 0.7811 - val_loss: 1.1223 - val_acc: 0.7500\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8333 - acc: 0.7830 - val_loss: 1.1025 - val_acc: 0.6667\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8087 - acc: 0.7908 - val_loss: 1.0526 - val_acc: 0.7917\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7953 - acc: 0.7977 - val_loss: 1.1289 - val_acc: 0.7083\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7766 - acc: 0.8017 - val_loss: 1.0997 - val_acc: 0.7083\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7688 - acc: 0.8042 - val_loss: 1.1431 - val_acc: 0.7292\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7516 - acc: 0.8042 - val_loss: 1.1184 - val_acc: 0.7292\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7453 - acc: 0.8093 - val_loss: 0.9974 - val_acc: 0.7500\n",
            "processing fold # 79\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 21s 4ms/step - loss: 2.9027 - acc: 0.2073 - val_loss: 2.4445 - val_acc: 0.3542\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.3580 - acc: 0.3505 - val_loss: 2.0035 - val_acc: 0.3750\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.0798 - acc: 0.4174 - val_loss: 1.8515 - val_acc: 0.5000\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.8871 - acc: 0.4836 - val_loss: 1.6903 - val_acc: 0.5208\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.7534 - acc: 0.5086 - val_loss: 1.5286 - val_acc: 0.6250\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.6199 - acc: 0.5526 - val_loss: 1.5219 - val_acc: 0.5417\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.4612 - acc: 0.6007 - val_loss: 1.3901 - val_acc: 0.5833\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.3898 - acc: 0.6184 - val_loss: 1.4120 - val_acc: 0.6042\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2972 - acc: 0.6423 - val_loss: 1.4129 - val_acc: 0.6250\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2186 - acc: 0.6676 - val_loss: 1.3074 - val_acc: 0.6250\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1677 - acc: 0.6815 - val_loss: 1.3768 - val_acc: 0.6042\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1198 - acc: 0.6930 - val_loss: 1.2715 - val_acc: 0.6250\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.0687 - acc: 0.7101 - val_loss: 1.2366 - val_acc: 0.6458\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0255 - acc: 0.7288 - val_loss: 1.3192 - val_acc: 0.6250\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0010 - acc: 0.7344 - val_loss: 1.2205 - val_acc: 0.6458\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9519 - acc: 0.7504 - val_loss: 1.2115 - val_acc: 0.6458\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9143 - acc: 0.7569 - val_loss: 1.1706 - val_acc: 0.6667\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8947 - acc: 0.7601 - val_loss: 1.2959 - val_acc: 0.6250\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8612 - acc: 0.7757 - val_loss: 1.2614 - val_acc: 0.6667\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8706 - acc: 0.7660 - val_loss: 1.2440 - val_acc: 0.6250\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8516 - acc: 0.7750 - val_loss: 1.2226 - val_acc: 0.6458\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8342 - acc: 0.7849 - val_loss: 1.2647 - val_acc: 0.6458\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7994 - acc: 0.7891 - val_loss: 1.2345 - val_acc: 0.6875\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7609 - acc: 0.8024 - val_loss: 1.2189 - val_acc: 0.6875\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7731 - acc: 0.8057 - val_loss: 1.1818 - val_acc: 0.7083\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7712 - acc: 0.7950 - val_loss: 1.1709 - val_acc: 0.7083\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7261 - acc: 0.8114 - val_loss: 1.2323 - val_acc: 0.6875\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7232 - acc: 0.8167 - val_loss: 1.1446 - val_acc: 0.7083\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7280 - acc: 0.8103 - val_loss: 1.1108 - val_acc: 0.6667\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7062 - acc: 0.8200 - val_loss: 1.2355 - val_acc: 0.6875\n",
            "processing fold # 80\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 21s 4ms/step - loss: 2.9526 - acc: 0.1989 - val_loss: 2.3163 - val_acc: 0.4167\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.3913 - acc: 0.3608 - val_loss: 1.9672 - val_acc: 0.4375\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.1866 - acc: 0.4153 - val_loss: 1.8175 - val_acc: 0.5208\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.9400 - acc: 0.4819 - val_loss: 1.5868 - val_acc: 0.6042\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.7656 - acc: 0.5193 - val_loss: 1.4286 - val_acc: 0.5833\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.6340 - acc: 0.5517 - val_loss: 1.3443 - val_acc: 0.5625\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.5149 - acc: 0.5751 - val_loss: 1.1399 - val_acc: 0.6458\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.4362 - acc: 0.5990 - val_loss: 1.1291 - val_acc: 0.7083\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3610 - acc: 0.6146 - val_loss: 0.9640 - val_acc: 0.7292\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.2998 - acc: 0.6341 - val_loss: 0.9617 - val_acc: 0.7708\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2170 - acc: 0.6653 - val_loss: 0.8905 - val_acc: 0.7708\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1540 - acc: 0.6913 - val_loss: 0.8165 - val_acc: 0.7708\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1027 - acc: 0.6995 - val_loss: 0.8183 - val_acc: 0.8542\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0619 - acc: 0.7136 - val_loss: 0.8146 - val_acc: 0.8333\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0285 - acc: 0.7243 - val_loss: 0.7787 - val_acc: 0.8542\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9970 - acc: 0.7328 - val_loss: 0.6956 - val_acc: 0.8542\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9762 - acc: 0.7391 - val_loss: 0.7138 - val_acc: 0.8542\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9280 - acc: 0.7529 - val_loss: 0.6776 - val_acc: 0.8542\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8982 - acc: 0.7588 - val_loss: 0.8240 - val_acc: 0.7292\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8763 - acc: 0.7668 - val_loss: 0.7105 - val_acc: 0.7917\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8735 - acc: 0.7727 - val_loss: 0.7038 - val_acc: 0.8125\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8379 - acc: 0.7717 - val_loss: 0.6089 - val_acc: 0.8750\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8207 - acc: 0.7826 - val_loss: 0.6411 - val_acc: 0.8542\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7883 - acc: 0.7950 - val_loss: 0.6424 - val_acc: 0.8958\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7915 - acc: 0.7872 - val_loss: 0.6014 - val_acc: 0.8542\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7719 - acc: 0.7885 - val_loss: 0.6494 - val_acc: 0.8542\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7601 - acc: 0.7994 - val_loss: 0.5889 - val_acc: 0.9167\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7549 - acc: 0.8068 - val_loss: 0.5657 - val_acc: 0.8542\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7301 - acc: 0.8120 - val_loss: 0.5656 - val_acc: 0.8958\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7240 - acc: 0.8059 - val_loss: 0.6243 - val_acc: 0.8333\n",
            "processing fold # 81\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 21s 4ms/step - loss: 2.9154 - acc: 0.2124 - val_loss: 2.5685 - val_acc: 0.3542\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 2.3884 - acc: 0.3591 - val_loss: 2.0146 - val_acc: 0.4375\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.0624 - acc: 0.4371 - val_loss: 1.6418 - val_acc: 0.6042\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.8442 - acc: 0.4790 - val_loss: 1.3862 - val_acc: 0.6250\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.6877 - acc: 0.5366 - val_loss: 1.3150 - val_acc: 0.6667\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.5380 - acc: 0.5763 - val_loss: 1.1375 - val_acc: 0.6875\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.4421 - acc: 0.6045 - val_loss: 1.1517 - val_acc: 0.6667\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3510 - acc: 0.6369 - val_loss: 1.0831 - val_acc: 0.6875\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2796 - acc: 0.6516 - val_loss: 1.0112 - val_acc: 0.6875\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2326 - acc: 0.6665 - val_loss: 0.9867 - val_acc: 0.7500\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.1735 - acc: 0.6863 - val_loss: 0.8980 - val_acc: 0.7708\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1332 - acc: 0.7021 - val_loss: 0.8191 - val_acc: 0.8125\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0931 - acc: 0.7023 - val_loss: 0.8568 - val_acc: 0.7708\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0321 - acc: 0.7277 - val_loss: 0.7979 - val_acc: 0.7708\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9991 - acc: 0.7342 - val_loss: 0.8253 - val_acc: 0.7917\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9659 - acc: 0.7433 - val_loss: 0.7601 - val_acc: 0.8333\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9392 - acc: 0.7475 - val_loss: 0.8751 - val_acc: 0.7292\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8976 - acc: 0.7637 - val_loss: 0.7795 - val_acc: 0.7917\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8739 - acc: 0.7679 - val_loss: 0.7883 - val_acc: 0.7917\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8555 - acc: 0.7780 - val_loss: 0.7646 - val_acc: 0.8125\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8356 - acc: 0.7809 - val_loss: 0.7570 - val_acc: 0.8125\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8212 - acc: 0.7813 - val_loss: 0.8230 - val_acc: 0.7917\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8026 - acc: 0.7883 - val_loss: 0.7952 - val_acc: 0.8125\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7714 - acc: 0.8011 - val_loss: 0.8054 - val_acc: 0.8125\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7654 - acc: 0.8047 - val_loss: 0.7120 - val_acc: 0.8333\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7456 - acc: 0.8076 - val_loss: 0.8309 - val_acc: 0.7708\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7156 - acc: 0.8129 - val_loss: 0.6878 - val_acc: 0.8333\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7179 - acc: 0.8112 - val_loss: 0.7908 - val_acc: 0.7500\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7079 - acc: 0.8202 - val_loss: 0.7126 - val_acc: 0.7708\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.6826 - acc: 0.8221 - val_loss: 0.7454 - val_acc: 0.7708\n",
            "processing fold # 82\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 22s 5ms/step - loss: 2.8685 - acc: 0.2325 - val_loss: 2.3873 - val_acc: 0.3125\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.3606 - acc: 0.3484 - val_loss: 2.2185 - val_acc: 0.4792\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.0888 - acc: 0.4230 - val_loss: 2.0854 - val_acc: 0.4375\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.8656 - acc: 0.4874 - val_loss: 1.9321 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.6856 - acc: 0.5368 - val_loss: 1.7750 - val_acc: 0.5000\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.5634 - acc: 0.5725 - val_loss: 1.5810 - val_acc: 0.5833\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4644 - acc: 0.6016 - val_loss: 1.5034 - val_acc: 0.6042\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.3545 - acc: 0.6388 - val_loss: 1.3348 - val_acc: 0.6875\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2811 - acc: 0.6539 - val_loss: 1.3713 - val_acc: 0.6875\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2075 - acc: 0.6770 - val_loss: 1.1602 - val_acc: 0.7083\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.1706 - acc: 0.6894 - val_loss: 1.1752 - val_acc: 0.7083\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0999 - acc: 0.7155 - val_loss: 1.1214 - val_acc: 0.7500\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0759 - acc: 0.7143 - val_loss: 1.1039 - val_acc: 0.7500\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0302 - acc: 0.7273 - val_loss: 1.0097 - val_acc: 0.8125\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9882 - acc: 0.7395 - val_loss: 1.0022 - val_acc: 0.7917\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9566 - acc: 0.7506 - val_loss: 1.0066 - val_acc: 0.7917\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8971 - acc: 0.7607 - val_loss: 0.9480 - val_acc: 0.7917\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8904 - acc: 0.7660 - val_loss: 1.0117 - val_acc: 0.8125\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8843 - acc: 0.7721 - val_loss: 0.9465 - val_acc: 0.7292\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8466 - acc: 0.7765 - val_loss: 0.9171 - val_acc: 0.7708\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8395 - acc: 0.7824 - val_loss: 1.0078 - val_acc: 0.7708\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8372 - acc: 0.7767 - val_loss: 0.8715 - val_acc: 0.8125\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8032 - acc: 0.7870 - val_loss: 0.8863 - val_acc: 0.8125\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7826 - acc: 0.7925 - val_loss: 0.9438 - val_acc: 0.7917\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7845 - acc: 0.7904 - val_loss: 0.9245 - val_acc: 0.8125\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.7575 - acc: 0.8049 - val_loss: 0.9003 - val_acc: 0.7708\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7301 - acc: 0.8034 - val_loss: 0.8845 - val_acc: 0.7917\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7141 - acc: 0.8112 - val_loss: 0.8657 - val_acc: 0.8333\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.7040 - acc: 0.8135 - val_loss: 0.8692 - val_acc: 0.8333\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7013 - acc: 0.8171 - val_loss: 0.8213 - val_acc: 0.8125\n",
            "processing fold # 83\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 22s 5ms/step - loss: 2.9970 - acc: 0.1960 - val_loss: 2.9958 - val_acc: 0.1875\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.4812 - acc: 0.3074 - val_loss: 2.4690 - val_acc: 0.3542\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.1903 - acc: 0.4022 - val_loss: 2.1962 - val_acc: 0.4375\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.9802 - acc: 0.4527 - val_loss: 2.0020 - val_acc: 0.3958\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.8138 - acc: 0.4977 - val_loss: 2.0124 - val_acc: 0.4375\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.6500 - acc: 0.5444 - val_loss: 1.7297 - val_acc: 0.5625\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.5361 - acc: 0.5751 - val_loss: 1.7122 - val_acc: 0.5208\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.4542 - acc: 0.6051 - val_loss: 1.6932 - val_acc: 0.5833\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.3746 - acc: 0.6198 - val_loss: 1.6786 - val_acc: 0.5833\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.3103 - acc: 0.6379 - val_loss: 1.5937 - val_acc: 0.6042\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.2507 - acc: 0.6566 - val_loss: 1.6273 - val_acc: 0.5833\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2119 - acc: 0.6684 - val_loss: 1.6510 - val_acc: 0.6042\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1652 - acc: 0.6840 - val_loss: 1.6560 - val_acc: 0.5833\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1178 - acc: 0.6972 - val_loss: 1.6684 - val_acc: 0.6042\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0708 - acc: 0.7151 - val_loss: 1.5762 - val_acc: 0.6042\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0447 - acc: 0.7218 - val_loss: 1.5703 - val_acc: 0.6250\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9903 - acc: 0.7412 - val_loss: 1.5497 - val_acc: 0.6250\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9780 - acc: 0.7420 - val_loss: 1.5210 - val_acc: 0.6667\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9582 - acc: 0.7475 - val_loss: 1.5137 - val_acc: 0.6250\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9388 - acc: 0.7498 - val_loss: 1.5628 - val_acc: 0.6875\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9166 - acc: 0.7538 - val_loss: 1.4630 - val_acc: 0.6458\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9013 - acc: 0.7616 - val_loss: 1.4344 - val_acc: 0.7083\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8669 - acc: 0.7735 - val_loss: 1.3824 - val_acc: 0.7083\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8417 - acc: 0.7769 - val_loss: 1.4515 - val_acc: 0.7500\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8428 - acc: 0.7788 - val_loss: 1.4965 - val_acc: 0.6458\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8119 - acc: 0.7878 - val_loss: 1.4892 - val_acc: 0.6667\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8151 - acc: 0.7805 - val_loss: 1.4325 - val_acc: 0.7083\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.8085 - acc: 0.7893 - val_loss: 1.4050 - val_acc: 0.7083\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7578 - acc: 0.8068 - val_loss: 1.5074 - val_acc: 0.6458\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7527 - acc: 0.8015 - val_loss: 1.4828 - val_acc: 0.6875\n",
            "processing fold # 84\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 23s 5ms/step - loss: 2.9895 - acc: 0.1972 - val_loss: 2.7209 - val_acc: 0.2917\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.4920 - acc: 0.3158 - val_loss: 2.3693 - val_acc: 0.3750\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.1805 - acc: 0.4054 - val_loss: 2.0839 - val_acc: 0.5000\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.9430 - acc: 0.4674 - val_loss: 1.9405 - val_acc: 0.4583\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.7310 - acc: 0.5271 - val_loss: 1.6785 - val_acc: 0.5417\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.5991 - acc: 0.5587 - val_loss: 1.5811 - val_acc: 0.6250\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.4870 - acc: 0.5950 - val_loss: 1.4749 - val_acc: 0.6250\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.3899 - acc: 0.6201 - val_loss: 1.3620 - val_acc: 0.6250\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.2836 - acc: 0.6520 - val_loss: 1.2600 - val_acc: 0.6875\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2351 - acc: 0.6665 - val_loss: 1.1953 - val_acc: 0.7083\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1907 - acc: 0.6848 - val_loss: 1.0779 - val_acc: 0.7500\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1414 - acc: 0.6964 - val_loss: 1.1976 - val_acc: 0.7500\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0799 - acc: 0.7212 - val_loss: 1.0698 - val_acc: 0.7083\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.0388 - acc: 0.7283 - val_loss: 1.1340 - val_acc: 0.6667\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0117 - acc: 0.7416 - val_loss: 1.0433 - val_acc: 0.7292\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9798 - acc: 0.7403 - val_loss: 1.0218 - val_acc: 0.7708\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9386 - acc: 0.7599 - val_loss: 0.9973 - val_acc: 0.7708\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9059 - acc: 0.7653 - val_loss: 0.9444 - val_acc: 0.8125\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9048 - acc: 0.7672 - val_loss: 0.9560 - val_acc: 0.7708\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8723 - acc: 0.7731 - val_loss: 0.9039 - val_acc: 0.8125\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8495 - acc: 0.7786 - val_loss: 0.9033 - val_acc: 0.8125\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.8368 - acc: 0.7878 - val_loss: 0.8690 - val_acc: 0.8333\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8158 - acc: 0.7878 - val_loss: 0.8359 - val_acc: 0.8542\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7929 - acc: 0.7912 - val_loss: 0.8056 - val_acc: 0.8333\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7776 - acc: 0.8036 - val_loss: 0.8005 - val_acc: 0.7917\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7512 - acc: 0.8068 - val_loss: 0.8572 - val_acc: 0.8333\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7436 - acc: 0.8118 - val_loss: 0.7802 - val_acc: 0.8125\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.7188 - acc: 0.8173 - val_loss: 0.7389 - val_acc: 0.8333\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7286 - acc: 0.8101 - val_loss: 0.8064 - val_acc: 0.8333\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7258 - acc: 0.8122 - val_loss: 0.7280 - val_acc: 0.8542\n",
            "processing fold # 85\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 23s 5ms/step - loss: 2.9834 - acc: 0.1871 - val_loss: 2.2591 - val_acc: 0.4167\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 2.3783 - acc: 0.3478 - val_loss: 1.9336 - val_acc: 0.5208\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.1125 - acc: 0.4020 - val_loss: 1.7579 - val_acc: 0.5000\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.8889 - acc: 0.4697 - val_loss: 1.5963 - val_acc: 0.5625\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.6859 - acc: 0.5385 - val_loss: 1.5380 - val_acc: 0.5833\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.5601 - acc: 0.5694 - val_loss: 1.4824 - val_acc: 0.5417\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.4615 - acc: 0.6030 - val_loss: 1.4487 - val_acc: 0.5208\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.3579 - acc: 0.6293 - val_loss: 1.3931 - val_acc: 0.5833\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.2692 - acc: 0.6562 - val_loss: 1.3420 - val_acc: 0.5625\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.2537 - acc: 0.6627 - val_loss: 1.3012 - val_acc: 0.6042\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1710 - acc: 0.6831 - val_loss: 1.3398 - val_acc: 0.6250\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1052 - acc: 0.6970 - val_loss: 1.2639 - val_acc: 0.6458\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0581 - acc: 0.7252 - val_loss: 1.3011 - val_acc: 0.6458\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0290 - acc: 0.7243 - val_loss: 1.2227 - val_acc: 0.6667\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0121 - acc: 0.7323 - val_loss: 1.2347 - val_acc: 0.6875\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9482 - acc: 0.7460 - val_loss: 1.1796 - val_acc: 0.7083\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9287 - acc: 0.7597 - val_loss: 1.2415 - val_acc: 0.7292\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9110 - acc: 0.7664 - val_loss: 1.2493 - val_acc: 0.6667\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9280 - acc: 0.7590 - val_loss: 1.2354 - val_acc: 0.6667\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8776 - acc: 0.7727 - val_loss: 1.2419 - val_acc: 0.7500\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8510 - acc: 0.7778 - val_loss: 1.2250 - val_acc: 0.6875\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8534 - acc: 0.7767 - val_loss: 1.2432 - val_acc: 0.6458\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8152 - acc: 0.7857 - val_loss: 1.1955 - val_acc: 0.7083\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7950 - acc: 0.7923 - val_loss: 1.2898 - val_acc: 0.7083\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7778 - acc: 0.8007 - val_loss: 1.1613 - val_acc: 0.7083\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.7675 - acc: 0.8026 - val_loss: 1.1878 - val_acc: 0.7292\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7482 - acc: 0.8080 - val_loss: 1.1861 - val_acc: 0.7708\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7258 - acc: 0.8089 - val_loss: 1.2607 - val_acc: 0.7500\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7132 - acc: 0.8202 - val_loss: 1.1387 - val_acc: 0.7292\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7318 - acc: 0.8085 - val_loss: 1.0955 - val_acc: 0.7292\n",
            "processing fold # 86\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 23s 5ms/step - loss: 2.9452 - acc: 0.2084 - val_loss: 2.4234 - val_acc: 0.2917\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.4211 - acc: 0.3328 - val_loss: 2.0865 - val_acc: 0.3333\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 2.1226 - acc: 0.4106 - val_loss: 1.8873 - val_acc: 0.4583\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.9156 - acc: 0.4640 - val_loss: 1.5955 - val_acc: 0.6042\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.7585 - acc: 0.5185 - val_loss: 1.5543 - val_acc: 0.5417\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 37us/step - loss: 1.6129 - acc: 0.5574 - val_loss: 1.4503 - val_acc: 0.5417\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.5059 - acc: 0.5864 - val_loss: 1.2429 - val_acc: 0.6042\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.3919 - acc: 0.6175 - val_loss: 1.0844 - val_acc: 0.7292\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.2983 - acc: 0.6495 - val_loss: 1.0866 - val_acc: 0.7083\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.2442 - acc: 0.6657 - val_loss: 1.1077 - val_acc: 0.6875\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1728 - acc: 0.6863 - val_loss: 1.0214 - val_acc: 0.7500\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1120 - acc: 0.6970 - val_loss: 1.0360 - val_acc: 0.7083\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0862 - acc: 0.7143 - val_loss: 1.0873 - val_acc: 0.7083\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0534 - acc: 0.7166 - val_loss: 1.0133 - val_acc: 0.7500\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0054 - acc: 0.7304 - val_loss: 0.9596 - val_acc: 0.7292\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9741 - acc: 0.7426 - val_loss: 1.0005 - val_acc: 0.7292\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9622 - acc: 0.7487 - val_loss: 1.0091 - val_acc: 0.7500\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9321 - acc: 0.7546 - val_loss: 0.9713 - val_acc: 0.7500\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9111 - acc: 0.7601 - val_loss: 0.9745 - val_acc: 0.7500\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8807 - acc: 0.7660 - val_loss: 0.9290 - val_acc: 0.7292\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8536 - acc: 0.7742 - val_loss: 0.9510 - val_acc: 0.7708\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.8286 - acc: 0.7860 - val_loss: 0.9462 - val_acc: 0.7708\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8068 - acc: 0.7878 - val_loss: 1.0433 - val_acc: 0.7292\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8169 - acc: 0.7845 - val_loss: 0.9841 - val_acc: 0.7708\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7838 - acc: 0.7950 - val_loss: 1.0010 - val_acc: 0.7500\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7560 - acc: 0.8003 - val_loss: 1.0431 - val_acc: 0.7083\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7568 - acc: 0.7998 - val_loss: 1.0867 - val_acc: 0.7500\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7348 - acc: 0.8082 - val_loss: 1.0875 - val_acc: 0.7500\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7228 - acc: 0.8101 - val_loss: 0.8790 - val_acc: 0.7917\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7101 - acc: 0.8124 - val_loss: 0.9319 - val_acc: 0.7917\n",
            "processing fold # 87\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 23s 5ms/step - loss: 2.8848 - acc: 0.2258 - val_loss: 2.2638 - val_acc: 0.4167\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.3260 - acc: 0.3661 - val_loss: 1.9768 - val_acc: 0.4792\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.0491 - acc: 0.4388 - val_loss: 1.7606 - val_acc: 0.5625\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.8113 - acc: 0.5048 - val_loss: 1.5630 - val_acc: 0.6042\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.6280 - acc: 0.5507 - val_loss: 1.6584 - val_acc: 0.5417\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.5329 - acc: 0.5818 - val_loss: 1.4876 - val_acc: 0.5625\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.4271 - acc: 0.6072 - val_loss: 1.4312 - val_acc: 0.6250\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.3488 - acc: 0.6386 - val_loss: 1.3482 - val_acc: 0.6458\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2620 - acc: 0.6627 - val_loss: 1.3295 - val_acc: 0.6250\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.2243 - acc: 0.6674 - val_loss: 1.2220 - val_acc: 0.6875\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1797 - acc: 0.6890 - val_loss: 1.2228 - val_acc: 0.6667\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 1.1158 - acc: 0.7021 - val_loss: 1.2854 - val_acc: 0.6250\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0659 - acc: 0.7193 - val_loss: 1.1911 - val_acc: 0.6875\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0038 - acc: 0.7319 - val_loss: 1.1361 - val_acc: 0.7292\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0061 - acc: 0.7349 - val_loss: 1.1663 - val_acc: 0.6667\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9766 - acc: 0.7418 - val_loss: 1.1894 - val_acc: 0.6667\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9484 - acc: 0.7462 - val_loss: 1.2310 - val_acc: 0.6667\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8984 - acc: 0.7662 - val_loss: 1.1561 - val_acc: 0.6458\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8704 - acc: 0.7771 - val_loss: 1.1517 - val_acc: 0.7083\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8595 - acc: 0.7742 - val_loss: 1.1515 - val_acc: 0.6875\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8376 - acc: 0.7826 - val_loss: 1.1292 - val_acc: 0.7292\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8223 - acc: 0.7855 - val_loss: 1.2170 - val_acc: 0.6875\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7863 - acc: 0.7984 - val_loss: 1.1978 - val_acc: 0.6875\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7730 - acc: 0.7967 - val_loss: 1.1344 - val_acc: 0.6667\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7749 - acc: 0.7916 - val_loss: 1.1097 - val_acc: 0.6875\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.7557 - acc: 0.8026 - val_loss: 1.1617 - val_acc: 0.6458\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7399 - acc: 0.8082 - val_loss: 1.1161 - val_acc: 0.7083\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7284 - acc: 0.8099 - val_loss: 1.1743 - val_acc: 0.7083\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7163 - acc: 0.8120 - val_loss: 1.0968 - val_acc: 0.7292\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7233 - acc: 0.8009 - val_loss: 1.1047 - val_acc: 0.7083\n",
            "processing fold # 88\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 23s 5ms/step - loss: 2.9195 - acc: 0.2262 - val_loss: 2.7386 - val_acc: 0.1875\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.4121 - acc: 0.3339 - val_loss: 2.3683 - val_acc: 0.2917\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.1218 - acc: 0.4144 - val_loss: 2.2475 - val_acc: 0.4167\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 26us/step - loss: 1.8821 - acc: 0.4765 - val_loss: 1.9863 - val_acc: 0.4583\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.6726 - acc: 0.5456 - val_loss: 1.6935 - val_acc: 0.5208\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.5344 - acc: 0.5913 - val_loss: 1.5801 - val_acc: 0.5625\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.4288 - acc: 0.6156 - val_loss: 1.4106 - val_acc: 0.6458\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.3323 - acc: 0.6388 - val_loss: 1.4046 - val_acc: 0.6875\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.2749 - acc: 0.6577 - val_loss: 1.2777 - val_acc: 0.6667\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2059 - acc: 0.6743 - val_loss: 1.2605 - val_acc: 0.6667\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1652 - acc: 0.6951 - val_loss: 1.2050 - val_acc: 0.6667\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1019 - acc: 0.7128 - val_loss: 1.2992 - val_acc: 0.6458\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.0707 - acc: 0.7197 - val_loss: 1.1464 - val_acc: 0.7500\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0490 - acc: 0.7311 - val_loss: 1.0923 - val_acc: 0.7292\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0026 - acc: 0.7389 - val_loss: 1.0757 - val_acc: 0.7292\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9517 - acc: 0.7534 - val_loss: 1.1775 - val_acc: 0.7083\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9134 - acc: 0.7609 - val_loss: 1.2148 - val_acc: 0.7083\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9057 - acc: 0.7658 - val_loss: 1.0580 - val_acc: 0.7500\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8863 - acc: 0.7712 - val_loss: 1.0278 - val_acc: 0.7708\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8607 - acc: 0.7748 - val_loss: 1.0054 - val_acc: 0.7500\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8355 - acc: 0.7855 - val_loss: 1.0349 - val_acc: 0.6875\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.8288 - acc: 0.7832 - val_loss: 1.0876 - val_acc: 0.7292\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8299 - acc: 0.7809 - val_loss: 0.9977 - val_acc: 0.7500\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8177 - acc: 0.7832 - val_loss: 0.9378 - val_acc: 0.7708\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7865 - acc: 0.7969 - val_loss: 1.0748 - val_acc: 0.7500\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7654 - acc: 0.8045 - val_loss: 0.9376 - val_acc: 0.7500\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7357 - acc: 0.8087 - val_loss: 0.9827 - val_acc: 0.7500\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7268 - acc: 0.8106 - val_loss: 1.0014 - val_acc: 0.7500\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7176 - acc: 0.8181 - val_loss: 0.9559 - val_acc: 0.7917\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.6876 - acc: 0.8217 - val_loss: 0.9702 - val_acc: 0.7292\n",
            "processing fold # 89\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 23s 5ms/step - loss: 2.9424 - acc: 0.2113 - val_loss: 2.3868 - val_acc: 0.3333\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.3904 - acc: 0.3452 - val_loss: 1.9812 - val_acc: 0.4375\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.0622 - acc: 0.4350 - val_loss: 1.6413 - val_acc: 0.5833\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.8026 - acc: 0.5006 - val_loss: 1.6032 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 26us/step - loss: 1.6437 - acc: 0.5473 - val_loss: 1.4142 - val_acc: 0.6250\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.4922 - acc: 0.5896 - val_loss: 1.2522 - val_acc: 0.6875\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.3741 - acc: 0.6213 - val_loss: 1.2852 - val_acc: 0.6667\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.3215 - acc: 0.6373 - val_loss: 1.1710 - val_acc: 0.7292\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2385 - acc: 0.6644 - val_loss: 1.1329 - val_acc: 0.7500\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1865 - acc: 0.6812 - val_loss: 1.0700 - val_acc: 0.7917\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1307 - acc: 0.7019 - val_loss: 1.0558 - val_acc: 0.7292\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0895 - acc: 0.7061 - val_loss: 0.9481 - val_acc: 0.8125\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0429 - acc: 0.7225 - val_loss: 0.9551 - val_acc: 0.7917\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9975 - acc: 0.7338 - val_loss: 0.9535 - val_acc: 0.7708\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9787 - acc: 0.7424 - val_loss: 0.9332 - val_acc: 0.8125\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9428 - acc: 0.7485 - val_loss: 0.9493 - val_acc: 0.8125\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9039 - acc: 0.7645 - val_loss: 0.8670 - val_acc: 0.8125\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.8703 - acc: 0.7757 - val_loss: 0.8660 - val_acc: 0.7708\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8604 - acc: 0.7727 - val_loss: 0.9000 - val_acc: 0.8125\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8437 - acc: 0.7750 - val_loss: 0.9009 - val_acc: 0.7708\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8219 - acc: 0.7857 - val_loss: 0.8678 - val_acc: 0.7708\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7925 - acc: 0.7927 - val_loss: 0.8997 - val_acc: 0.8333\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7780 - acc: 0.7981 - val_loss: 0.8324 - val_acc: 0.7917\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7877 - acc: 0.7937 - val_loss: 0.8753 - val_acc: 0.7708\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7606 - acc: 0.8026 - val_loss: 0.9414 - val_acc: 0.7917\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7436 - acc: 0.8074 - val_loss: 0.8703 - val_acc: 0.8333\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7158 - acc: 0.8143 - val_loss: 0.8732 - val_acc: 0.7917\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7073 - acc: 0.8171 - val_loss: 0.8697 - val_acc: 0.8125\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7065 - acc: 0.8162 - val_loss: 0.8972 - val_acc: 0.7917\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7009 - acc: 0.8146 - val_loss: 0.9096 - val_acc: 0.7708\n",
            "processing fold # 90\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 24s 5ms/step - loss: 2.9500 - acc: 0.2107 - val_loss: 2.3531 - val_acc: 0.4167\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 27us/step - loss: 2.3918 - acc: 0.3455 - val_loss: 2.1865 - val_acc: 0.4792\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 27us/step - loss: 2.1174 - acc: 0.4172 - val_loss: 1.9181 - val_acc: 0.4583\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.9243 - acc: 0.4653 - val_loss: 1.8078 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.7253 - acc: 0.5254 - val_loss: 1.6607 - val_acc: 0.5417\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.5922 - acc: 0.5652 - val_loss: 1.5409 - val_acc: 0.5208\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.4913 - acc: 0.5942 - val_loss: 1.5179 - val_acc: 0.5625\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.3894 - acc: 0.6182 - val_loss: 1.4515 - val_acc: 0.6250\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.3244 - acc: 0.6426 - val_loss: 1.4570 - val_acc: 0.5833\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2519 - acc: 0.6581 - val_loss: 1.3313 - val_acc: 0.6667\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1785 - acc: 0.6941 - val_loss: 1.3264 - val_acc: 0.6458\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.1150 - acc: 0.7029 - val_loss: 1.3664 - val_acc: 0.6250\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0984 - acc: 0.7014 - val_loss: 1.3720 - val_acc: 0.6250\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0513 - acc: 0.7206 - val_loss: 1.3219 - val_acc: 0.6875\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0216 - acc: 0.7332 - val_loss: 1.3021 - val_acc: 0.6250\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9867 - acc: 0.7468 - val_loss: 1.3305 - val_acc: 0.7292\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9834 - acc: 0.7359 - val_loss: 1.1745 - val_acc: 0.7083\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9192 - acc: 0.7569 - val_loss: 1.2752 - val_acc: 0.6458\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 22us/step - loss: 0.9201 - acc: 0.7582 - val_loss: 1.1913 - val_acc: 0.6458\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8765 - acc: 0.7714 - val_loss: 1.1851 - val_acc: 0.6875\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.8656 - acc: 0.7742 - val_loss: 1.1197 - val_acc: 0.6875\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8302 - acc: 0.7874 - val_loss: 1.0994 - val_acc: 0.7083\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8135 - acc: 0.7883 - val_loss: 1.1106 - val_acc: 0.7292\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7892 - acc: 0.7929 - val_loss: 1.0945 - val_acc: 0.7083\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7682 - acc: 0.8015 - val_loss: 1.0393 - val_acc: 0.7292\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7564 - acc: 0.8036 - val_loss: 1.1356 - val_acc: 0.7083\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7421 - acc: 0.8108 - val_loss: 1.1004 - val_acc: 0.6875\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7303 - acc: 0.8154 - val_loss: 1.0163 - val_acc: 0.6667\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7270 - acc: 0.8131 - val_loss: 0.9457 - val_acc: 0.7708\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.7213 - acc: 0.8093 - val_loss: 1.0350 - val_acc: 0.6667\n",
            "processing fold # 91\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 24s 5ms/step - loss: 2.9790 - acc: 0.1951 - val_loss: 2.6824 - val_acc: 0.2500\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.4336 - acc: 0.3383 - val_loss: 2.2186 - val_acc: 0.3958\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.1225 - acc: 0.4134 - val_loss: 1.9238 - val_acc: 0.4583\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.9390 - acc: 0.4567 - val_loss: 1.8047 - val_acc: 0.4792\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.7408 - acc: 0.5204 - val_loss: 1.6226 - val_acc: 0.5625\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.6151 - acc: 0.5618 - val_loss: 1.3820 - val_acc: 0.6250\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.5139 - acc: 0.5862 - val_loss: 1.3485 - val_acc: 0.6458\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.4123 - acc: 0.6161 - val_loss: 1.2031 - val_acc: 0.6458\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.3492 - acc: 0.6327 - val_loss: 1.2042 - val_acc: 0.6875\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2735 - acc: 0.6569 - val_loss: 1.1191 - val_acc: 0.6875\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.2145 - acc: 0.6718 - val_loss: 1.0308 - val_acc: 0.6875\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1590 - acc: 0.6930 - val_loss: 0.9591 - val_acc: 0.7083\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1343 - acc: 0.6922 - val_loss: 0.8562 - val_acc: 0.7083\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0914 - acc: 0.7035 - val_loss: 0.9010 - val_acc: 0.7292\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0303 - acc: 0.7273 - val_loss: 0.8094 - val_acc: 0.7292\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0041 - acc: 0.7357 - val_loss: 0.9038 - val_acc: 0.7292\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9750 - acc: 0.7389 - val_loss: 0.8947 - val_acc: 0.7708\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9726 - acc: 0.7378 - val_loss: 0.7684 - val_acc: 0.8125\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9226 - acc: 0.7508 - val_loss: 0.7726 - val_acc: 0.8125\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9081 - acc: 0.7639 - val_loss: 0.8389 - val_acc: 0.7917\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8755 - acc: 0.7710 - val_loss: 0.7792 - val_acc: 0.7708\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8826 - acc: 0.7620 - val_loss: 0.7464 - val_acc: 0.7917\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8735 - acc: 0.7721 - val_loss: 0.7428 - val_acc: 0.7917\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8460 - acc: 0.7717 - val_loss: 0.7533 - val_acc: 0.8125\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8137 - acc: 0.7845 - val_loss: 0.7672 - val_acc: 0.7708\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8009 - acc: 0.7969 - val_loss: 0.7261 - val_acc: 0.8125\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7713 - acc: 0.7969 - val_loss: 0.7869 - val_acc: 0.8125\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7579 - acc: 0.8005 - val_loss: 0.6865 - val_acc: 0.8125\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7554 - acc: 0.8053 - val_loss: 0.6877 - val_acc: 0.8125\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7484 - acc: 0.8122 - val_loss: 0.7583 - val_acc: 0.8125\n",
            "processing fold # 92\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 24s 5ms/step - loss: 2.9558 - acc: 0.2132 - val_loss: 2.2765 - val_acc: 0.3958\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.3738 - acc: 0.3726 - val_loss: 1.8899 - val_acc: 0.5208\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 2.1116 - acc: 0.4186 - val_loss: 1.7407 - val_acc: 0.5208\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.8805 - acc: 0.4859 - val_loss: 1.5874 - val_acc: 0.6042\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.7100 - acc: 0.5280 - val_loss: 1.4710 - val_acc: 0.5833\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.5848 - acc: 0.5610 - val_loss: 1.2509 - val_acc: 0.7083\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.4773 - acc: 0.5948 - val_loss: 1.3944 - val_acc: 0.6458\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 29us/step - loss: 1.3766 - acc: 0.6190 - val_loss: 1.2228 - val_acc: 0.6875\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 30us/step - loss: 1.3001 - acc: 0.6489 - val_loss: 1.1054 - val_acc: 0.7708\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.2377 - acc: 0.6659 - val_loss: 1.1505 - val_acc: 0.7292\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.1826 - acc: 0.6806 - val_loss: 1.0351 - val_acc: 0.7500\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1126 - acc: 0.7042 - val_loss: 1.0590 - val_acc: 0.7917\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0913 - acc: 0.7054 - val_loss: 1.1251 - val_acc: 0.7500\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0377 - acc: 0.7212 - val_loss: 1.0466 - val_acc: 0.7292\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9931 - acc: 0.7420 - val_loss: 1.0137 - val_acc: 0.7500\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9610 - acc: 0.7468 - val_loss: 1.1110 - val_acc: 0.7292\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9518 - acc: 0.7447 - val_loss: 1.0247 - val_acc: 0.7500\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8901 - acc: 0.7635 - val_loss: 0.9552 - val_acc: 0.7708\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8879 - acc: 0.7675 - val_loss: 0.9448 - val_acc: 0.7500\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 26us/step - loss: 0.8747 - acc: 0.7757 - val_loss: 0.9924 - val_acc: 0.7917\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8398 - acc: 0.7811 - val_loss: 1.0205 - val_acc: 0.7917\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8289 - acc: 0.7828 - val_loss: 1.0024 - val_acc: 0.7708\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7929 - acc: 0.8000 - val_loss: 1.0093 - val_acc: 0.7708\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7956 - acc: 0.7942 - val_loss: 0.9988 - val_acc: 0.7708\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7618 - acc: 0.8063 - val_loss: 0.9415 - val_acc: 0.7292\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7613 - acc: 0.8015 - val_loss: 0.9453 - val_acc: 0.7292\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7410 - acc: 0.8087 - val_loss: 0.9649 - val_acc: 0.8125\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7294 - acc: 0.8120 - val_loss: 0.9554 - val_acc: 0.8125\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.7094 - acc: 0.8150 - val_loss: 0.9514 - val_acc: 0.8125\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7134 - acc: 0.8114 - val_loss: 0.9894 - val_acc: 0.7500\n",
            "processing fold # 93\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 25s 5ms/step - loss: 2.9770 - acc: 0.2233 - val_loss: 2.4856 - val_acc: 0.3542\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 2.3903 - acc: 0.3553 - val_loss: 2.0523 - val_acc: 0.4375\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.0832 - acc: 0.4264 - val_loss: 1.7772 - val_acc: 0.5208\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.8476 - acc: 0.4958 - val_loss: 1.7711 - val_acc: 0.5625\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.7061 - acc: 0.5294 - val_loss: 1.6206 - val_acc: 0.6042\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.5888 - acc: 0.5650 - val_loss: 1.4899 - val_acc: 0.5208\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.4684 - acc: 0.5929 - val_loss: 1.3636 - val_acc: 0.6458\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.3778 - acc: 0.6259 - val_loss: 1.2339 - val_acc: 0.6875\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 30us/step - loss: 1.2839 - acc: 0.6550 - val_loss: 1.2268 - val_acc: 0.6875\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 32us/step - loss: 1.2294 - acc: 0.6770 - val_loss: 1.2370 - val_acc: 0.6250\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1701 - acc: 0.6869 - val_loss: 1.1948 - val_acc: 0.6667\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1256 - acc: 0.6989 - val_loss: 1.1187 - val_acc: 0.6875\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0743 - acc: 0.7147 - val_loss: 1.1503 - val_acc: 0.6875\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0417 - acc: 0.7258 - val_loss: 1.0882 - val_acc: 0.7083\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0058 - acc: 0.7403 - val_loss: 1.0731 - val_acc: 0.7292\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9805 - acc: 0.7450 - val_loss: 1.1637 - val_acc: 0.7292\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9631 - acc: 0.7529 - val_loss: 1.0563 - val_acc: 0.7083\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9332 - acc: 0.7588 - val_loss: 1.0752 - val_acc: 0.6875\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9320 - acc: 0.7571 - val_loss: 1.0035 - val_acc: 0.7292\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8905 - acc: 0.7721 - val_loss: 1.1071 - val_acc: 0.7292\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8595 - acc: 0.7826 - val_loss: 1.1127 - val_acc: 0.6875\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8427 - acc: 0.7857 - val_loss: 1.1290 - val_acc: 0.7083\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8148 - acc: 0.7971 - val_loss: 1.0496 - val_acc: 0.7292\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7901 - acc: 0.7971 - val_loss: 1.1216 - val_acc: 0.6875\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7741 - acc: 0.7969 - val_loss: 1.1041 - val_acc: 0.7083\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7649 - acc: 0.8045 - val_loss: 1.1757 - val_acc: 0.7083\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7565 - acc: 0.8074 - val_loss: 1.1469 - val_acc: 0.7292\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7430 - acc: 0.8076 - val_loss: 1.1192 - val_acc: 0.7083\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7364 - acc: 0.8042 - val_loss: 1.0197 - val_acc: 0.7292\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7139 - acc: 0.8143 - val_loss: 1.1592 - val_acc: 0.7083\n",
            "processing fold # 94\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 25s 5ms/step - loss: 3.0386 - acc: 0.1934 - val_loss: 2.4710 - val_acc: 0.3125\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.5171 - acc: 0.2954 - val_loss: 2.1723 - val_acc: 0.4167\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.2186 - acc: 0.3823 - val_loss: 1.9636 - val_acc: 0.5208\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.0007 - acc: 0.4415 - val_loss: 1.7582 - val_acc: 0.5833\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.7889 - acc: 0.5057 - val_loss: 1.5487 - val_acc: 0.5625\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.6622 - acc: 0.5408 - val_loss: 1.3916 - val_acc: 0.6667\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.5482 - acc: 0.5753 - val_loss: 1.2881 - val_acc: 0.6042\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.4348 - acc: 0.6112 - val_loss: 1.2224 - val_acc: 0.6250\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.3485 - acc: 0.6384 - val_loss: 1.1690 - val_acc: 0.6250\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.2674 - acc: 0.6611 - val_loss: 1.0849 - val_acc: 0.6458\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2014 - acc: 0.6852 - val_loss: 1.0709 - val_acc: 0.7500\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1458 - acc: 0.6937 - val_loss: 0.9945 - val_acc: 0.6875\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1060 - acc: 0.7115 - val_loss: 1.0322 - val_acc: 0.7083\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0644 - acc: 0.7218 - val_loss: 1.0238 - val_acc: 0.7083\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0299 - acc: 0.7283 - val_loss: 0.9633 - val_acc: 0.7292\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0043 - acc: 0.7336 - val_loss: 0.9435 - val_acc: 0.7292\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9681 - acc: 0.7462 - val_loss: 0.9204 - val_acc: 0.7500\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9308 - acc: 0.7630 - val_loss: 0.9937 - val_acc: 0.7500\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9053 - acc: 0.7647 - val_loss: 0.9049 - val_acc: 0.8125\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8833 - acc: 0.7717 - val_loss: 0.9403 - val_acc: 0.8125\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8529 - acc: 0.7866 - val_loss: 0.9593 - val_acc: 0.7917\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8282 - acc: 0.7857 - val_loss: 0.8642 - val_acc: 0.7083\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8207 - acc: 0.7899 - val_loss: 0.9089 - val_acc: 0.8125\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8084 - acc: 0.7857 - val_loss: 0.8854 - val_acc: 0.7917\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8255 - acc: 0.7893 - val_loss: 0.9164 - val_acc: 0.7292\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.7746 - acc: 0.8005 - val_loss: 0.8733 - val_acc: 0.7917\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7673 - acc: 0.7971 - val_loss: 0.8641 - val_acc: 0.7708\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7248 - acc: 0.8154 - val_loss: 0.9158 - val_acc: 0.8125\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7180 - acc: 0.8135 - val_loss: 0.8894 - val_acc: 0.8125\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7060 - acc: 0.8192 - val_loss: 0.9024 - val_acc: 0.8125\n",
            "processing fold # 95\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 25s 5ms/step - loss: 2.9896 - acc: 0.2008 - val_loss: 2.7135 - val_acc: 0.2500\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.4043 - acc: 0.3488 - val_loss: 2.5347 - val_acc: 0.2917\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 2.1498 - acc: 0.4094 - val_loss: 2.0876 - val_acc: 0.4583\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.9356 - acc: 0.4748 - val_loss: 1.9952 - val_acc: 0.5417\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.7386 - acc: 0.5252 - val_loss: 1.8246 - val_acc: 0.5208\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.6224 - acc: 0.5435 - val_loss: 1.7639 - val_acc: 0.5833\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.4977 - acc: 0.5971 - val_loss: 1.6994 - val_acc: 0.5833\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.4021 - acc: 0.6232 - val_loss: 1.6087 - val_acc: 0.5417\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.3224 - acc: 0.6451 - val_loss: 1.6203 - val_acc: 0.6250\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.2487 - acc: 0.6655 - val_loss: 1.5073 - val_acc: 0.6250\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2059 - acc: 0.6762 - val_loss: 1.3515 - val_acc: 0.6250\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 26us/step - loss: 1.1518 - acc: 0.6854 - val_loss: 1.3985 - val_acc: 0.5833\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0940 - acc: 0.7130 - val_loss: 1.3802 - val_acc: 0.6458\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0784 - acc: 0.7088 - val_loss: 1.2278 - val_acc: 0.7083\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0273 - acc: 0.7265 - val_loss: 1.3331 - val_acc: 0.6458\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9883 - acc: 0.7450 - val_loss: 1.1984 - val_acc: 0.6875\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9718 - acc: 0.7317 - val_loss: 1.2429 - val_acc: 0.6875\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9337 - acc: 0.7565 - val_loss: 1.3455 - val_acc: 0.6667\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8926 - acc: 0.7664 - val_loss: 1.2918 - val_acc: 0.6667\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8837 - acc: 0.7696 - val_loss: 1.1816 - val_acc: 0.6875\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.8600 - acc: 0.7763 - val_loss: 1.0600 - val_acc: 0.6875\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8379 - acc: 0.7748 - val_loss: 1.2613 - val_acc: 0.6458\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8496 - acc: 0.7708 - val_loss: 1.1628 - val_acc: 0.6875\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8201 - acc: 0.7868 - val_loss: 1.0907 - val_acc: 0.6458\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7985 - acc: 0.7981 - val_loss: 1.1115 - val_acc: 0.7083\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7764 - acc: 0.7979 - val_loss: 1.2337 - val_acc: 0.6875\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7517 - acc: 0.8087 - val_loss: 1.1037 - val_acc: 0.7292\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7399 - acc: 0.8063 - val_loss: 1.0919 - val_acc: 0.7500\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7198 - acc: 0.8152 - val_loss: 1.0721 - val_acc: 0.6875\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 34us/step - loss: 0.7043 - acc: 0.8221 - val_loss: 1.1042 - val_acc: 0.6875\n",
            "processing fold # 96\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 25s 5ms/step - loss: 3.0805 - acc: 0.1819 - val_loss: 2.5511 - val_acc: 0.3958\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.4729 - acc: 0.3341 - val_loss: 2.1990 - val_acc: 0.3750\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.2332 - acc: 0.3766 - val_loss: 2.0928 - val_acc: 0.4792\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 2.0507 - acc: 0.4243 - val_loss: 1.9144 - val_acc: 0.4167\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.8698 - acc: 0.4781 - val_loss: 1.6738 - val_acc: 0.5000\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 26us/step - loss: 1.7298 - acc: 0.5193 - val_loss: 1.6008 - val_acc: 0.5417\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.6103 - acc: 0.5479 - val_loss: 1.5104 - val_acc: 0.5625\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.5136 - acc: 0.5833 - val_loss: 1.4367 - val_acc: 0.6042\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.4322 - acc: 0.6039 - val_loss: 1.2820 - val_acc: 0.7500\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.3673 - acc: 0.6188 - val_loss: 1.2075 - val_acc: 0.7708\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.2963 - acc: 0.6472 - val_loss: 1.1597 - val_acc: 0.7500\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.2088 - acc: 0.6665 - val_loss: 1.1142 - val_acc: 0.8333\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1833 - acc: 0.6798 - val_loss: 1.1044 - val_acc: 0.7917\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1097 - acc: 0.7010 - val_loss: 1.0630 - val_acc: 0.7708\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 26us/step - loss: 1.0702 - acc: 0.7098 - val_loss: 1.0318 - val_acc: 0.7500\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0455 - acc: 0.7220 - val_loss: 1.0399 - val_acc: 0.8125\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.0048 - acc: 0.7353 - val_loss: 0.9346 - val_acc: 0.8125\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9751 - acc: 0.7468 - val_loss: 0.8985 - val_acc: 0.8333\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9410 - acc: 0.7576 - val_loss: 0.9833 - val_acc: 0.7917\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9142 - acc: 0.7553 - val_loss: 0.8575 - val_acc: 0.8125\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8939 - acc: 0.7590 - val_loss: 0.9151 - val_acc: 0.8542\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8759 - acc: 0.7689 - val_loss: 0.9652 - val_acc: 0.8333\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8544 - acc: 0.7765 - val_loss: 0.8826 - val_acc: 0.8333\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 26us/step - loss: 0.8400 - acc: 0.7775 - val_loss: 0.8449 - val_acc: 0.7708\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8348 - acc: 0.7826 - val_loss: 0.8459 - val_acc: 0.8333\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8118 - acc: 0.7817 - val_loss: 0.8537 - val_acc: 0.8542\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7855 - acc: 0.8007 - val_loss: 0.8740 - val_acc: 0.8333\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7796 - acc: 0.7956 - val_loss: 0.8626 - val_acc: 0.8333\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.7656 - acc: 0.7977 - val_loss: 0.8911 - val_acc: 0.8125\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7474 - acc: 0.8124 - val_loss: 0.8626 - val_acc: 0.8333\n",
            "processing fold # 97\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 25s 5ms/step - loss: 2.9479 - acc: 0.2243 - val_loss: 2.6583 - val_acc: 0.2917\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.3980 - acc: 0.3518 - val_loss: 2.4548 - val_acc: 0.4167\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.0788 - acc: 0.4376 - val_loss: 2.3087 - val_acc: 0.4167\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.8504 - acc: 0.4964 - val_loss: 2.2253 - val_acc: 0.3958\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.7248 - acc: 0.5280 - val_loss: 2.1801 - val_acc: 0.4583\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.5769 - acc: 0.5738 - val_loss: 2.2396 - val_acc: 0.4583\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.4462 - acc: 0.6032 - val_loss: 2.2774 - val_acc: 0.4375\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.3692 - acc: 0.6356 - val_loss: 2.1837 - val_acc: 0.4375\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2815 - acc: 0.6646 - val_loss: 2.2234 - val_acc: 0.4167\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.2130 - acc: 0.6726 - val_loss: 2.1684 - val_acc: 0.4583\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1348 - acc: 0.7058 - val_loss: 2.1074 - val_acc: 0.4792\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1023 - acc: 0.7088 - val_loss: 2.1244 - val_acc: 0.5208\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0629 - acc: 0.7262 - val_loss: 2.0833 - val_acc: 0.5208\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0624 - acc: 0.7239 - val_loss: 2.3338 - val_acc: 0.4167\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9860 - acc: 0.7475 - val_loss: 2.2319 - val_acc: 0.5000\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9480 - acc: 0.7519 - val_loss: 2.2247 - val_acc: 0.5208\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9225 - acc: 0.7649 - val_loss: 2.2112 - val_acc: 0.4583\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8896 - acc: 0.7651 - val_loss: 2.1845 - val_acc: 0.5417\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8761 - acc: 0.7769 - val_loss: 2.1762 - val_acc: 0.4375\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8410 - acc: 0.7864 - val_loss: 2.2526 - val_acc: 0.5417\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8157 - acc: 0.7895 - val_loss: 2.2820 - val_acc: 0.5208\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8022 - acc: 0.7885 - val_loss: 1.9825 - val_acc: 0.5208\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8005 - acc: 0.7939 - val_loss: 2.3166 - val_acc: 0.4792\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7931 - acc: 0.7975 - val_loss: 2.1683 - val_acc: 0.5000\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 26us/step - loss: 0.7830 - acc: 0.7952 - val_loss: 2.1862 - val_acc: 0.5625\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7696 - acc: 0.7969 - val_loss: 2.1527 - val_acc: 0.5417\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7317 - acc: 0.8146 - val_loss: 2.2324 - val_acc: 0.4583\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7276 - acc: 0.8127 - val_loss: 2.1687 - val_acc: 0.5000\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7237 - acc: 0.8196 - val_loss: 2.2325 - val_acc: 0.4792\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.6917 - acc: 0.8253 - val_loss: 2.1496 - val_acc: 0.5000\n",
            "processing fold # 98\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 26s 5ms/step - loss: 2.9420 - acc: 0.2157 - val_loss: 2.8146 - val_acc: 0.3125\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 2.4388 - acc: 0.3335 - val_loss: 2.4575 - val_acc: 0.3958\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.0986 - acc: 0.4226 - val_loss: 2.2738 - val_acc: 0.4583\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.8663 - acc: 0.4943 - val_loss: 2.0226 - val_acc: 0.5208\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.6632 - acc: 0.5477 - val_loss: 1.8244 - val_acc: 0.5000\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.5385 - acc: 0.5772 - val_loss: 1.7310 - val_acc: 0.5417\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.4250 - acc: 0.6028 - val_loss: 1.7146 - val_acc: 0.5208\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.3510 - acc: 0.6346 - val_loss: 1.6114 - val_acc: 0.6042\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.2713 - acc: 0.6590 - val_loss: 1.5310 - val_acc: 0.6875\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.2050 - acc: 0.6789 - val_loss: 1.5105 - val_acc: 0.5833\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.1436 - acc: 0.6918 - val_loss: 1.4415 - val_acc: 0.6458\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1146 - acc: 0.7014 - val_loss: 1.4503 - val_acc: 0.5833\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0581 - acc: 0.7204 - val_loss: 1.4277 - val_acc: 0.6875\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.0226 - acc: 0.7275 - val_loss: 1.4100 - val_acc: 0.6250\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9978 - acc: 0.7416 - val_loss: 1.4393 - val_acc: 0.6875\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.9830 - acc: 0.7460 - val_loss: 1.3048 - val_acc: 0.6667\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9202 - acc: 0.7578 - val_loss: 1.2288 - val_acc: 0.7083\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9052 - acc: 0.7607 - val_loss: 1.3118 - val_acc: 0.6875\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8727 - acc: 0.7761 - val_loss: 1.1949 - val_acc: 0.7500\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8555 - acc: 0.7752 - val_loss: 1.2078 - val_acc: 0.6875\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8361 - acc: 0.7904 - val_loss: 1.2136 - val_acc: 0.6875\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8005 - acc: 0.7988 - val_loss: 1.1832 - val_acc: 0.7292\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7868 - acc: 0.7979 - val_loss: 1.2053 - val_acc: 0.7292\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 26us/step - loss: 0.7697 - acc: 0.8040 - val_loss: 1.0594 - val_acc: 0.7708\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7815 - acc: 0.7975 - val_loss: 1.1342 - val_acc: 0.7292\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7526 - acc: 0.8116 - val_loss: 1.0766 - val_acc: 0.7500\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7361 - acc: 0.8078 - val_loss: 1.0557 - val_acc: 0.7292\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7243 - acc: 0.8150 - val_loss: 1.0881 - val_acc: 0.7292\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.7097 - acc: 0.8223 - val_loss: 1.1175 - val_acc: 0.7083\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7082 - acc: 0.8154 - val_loss: 1.0872 - val_acc: 0.7917\n",
            "processing fold # 99\n",
            "Train on 4756 samples, validate on 48 samples\n",
            "Epoch 1/30\n",
            "4756/4756 [==============================] - 26s 5ms/step - loss: 2.9262 - acc: 0.2128 - val_loss: 2.2243 - val_acc: 0.4583\n",
            "Epoch 2/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.3707 - acc: 0.3484 - val_loss: 2.0355 - val_acc: 0.3542\n",
            "Epoch 3/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 2.1338 - acc: 0.3871 - val_loss: 1.6569 - val_acc: 0.5625\n",
            "Epoch 4/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.9211 - acc: 0.4605 - val_loss: 1.5702 - val_acc: 0.5417\n",
            "Epoch 5/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.7133 - acc: 0.5210 - val_loss: 1.3548 - val_acc: 0.6667\n",
            "Epoch 6/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 1.5615 - acc: 0.5637 - val_loss: 1.3737 - val_acc: 0.6250\n",
            "Epoch 7/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.4784 - acc: 0.5938 - val_loss: 1.2207 - val_acc: 0.7083\n",
            "Epoch 8/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.3695 - acc: 0.6207 - val_loss: 1.1291 - val_acc: 0.6250\n",
            "Epoch 9/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.3084 - acc: 0.6381 - val_loss: 1.0235 - val_acc: 0.6875\n",
            "Epoch 10/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.2394 - acc: 0.6634 - val_loss: 0.9549 - val_acc: 0.7083\n",
            "Epoch 11/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1560 - acc: 0.6878 - val_loss: 0.9415 - val_acc: 0.7292\n",
            "Epoch 12/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 1.1208 - acc: 0.6964 - val_loss: 1.0052 - val_acc: 0.6667\n",
            "Epoch 13/30\n",
            "4756/4756 [==============================] - 0s 27us/step - loss: 1.0897 - acc: 0.7136 - val_loss: 0.9278 - val_acc: 0.7083\n",
            "Epoch 14/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 1.0334 - acc: 0.7229 - val_loss: 0.8596 - val_acc: 0.6875\n",
            "Epoch 15/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9949 - acc: 0.7403 - val_loss: 0.8462 - val_acc: 0.7292\n",
            "Epoch 16/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.9735 - acc: 0.7431 - val_loss: 0.8463 - val_acc: 0.7500\n",
            "Epoch 17/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9482 - acc: 0.7523 - val_loss: 0.7959 - val_acc: 0.7917\n",
            "Epoch 18/30\n",
            "4756/4756 [==============================] - 0s 25us/step - loss: 0.9232 - acc: 0.7584 - val_loss: 0.7861 - val_acc: 0.7917\n",
            "Epoch 19/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8977 - acc: 0.7624 - val_loss: 0.8128 - val_acc: 0.6875\n",
            "Epoch 20/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8845 - acc: 0.7662 - val_loss: 0.8320 - val_acc: 0.7500\n",
            "Epoch 21/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8397 - acc: 0.7767 - val_loss: 0.7498 - val_acc: 0.8333\n",
            "Epoch 22/30\n",
            "4756/4756 [==============================] - 0s 26us/step - loss: 0.8292 - acc: 0.7794 - val_loss: 0.6936 - val_acc: 0.7500\n",
            "Epoch 23/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.8263 - acc: 0.7864 - val_loss: 0.7496 - val_acc: 0.7917\n",
            "Epoch 24/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.8206 - acc: 0.7883 - val_loss: 0.7216 - val_acc: 0.8125\n",
            "Epoch 25/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7660 - acc: 0.8061 - val_loss: 0.7306 - val_acc: 0.7917\n",
            "Epoch 26/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7624 - acc: 0.8005 - val_loss: 0.7395 - val_acc: 0.8125\n",
            "Epoch 27/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7338 - acc: 0.8114 - val_loss: 0.7912 - val_acc: 0.7917\n",
            "Epoch 28/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7345 - acc: 0.8097 - val_loss: 0.7239 - val_acc: 0.8333\n",
            "Epoch 29/30\n",
            "4756/4756 [==============================] - 0s 23us/step - loss: 0.7348 - acc: 0.8040 - val_loss: 0.7268 - val_acc: 0.8542\n",
            "Epoch 30/30\n",
            "4756/4756 [==============================] - 0s 24us/step - loss: 0.7214 - acc: 0.8146 - val_loss: 0.7354 - val_acc: 0.8542\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  model=models.Sequential()\\n  model.add(Dense(512,kernel_regularizer=regularizers.l2(.001), activation=\"relu\",input_shape=(len(word2index),)))\\n  model.add(Dropout(.2))\\n  model.add(Dense(256, activation=\\'relu\\'))\\n  model.add(Dropout(.1))\\n  model.add(Dense(128, activation=\\'relu\\'))\\n  model.add(Dense(128,kernel_regularizer=regularizers.l2(.001),activation=\\'relu\\'))\\n  model.add(Dense(26,activation=\"softmax\"))\\n\\n  model.compile(optimizer=sgd,loss=\\'categorical_crossentropy\\',metrics=[\\'accuracy\\'])'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "Nuh1jcmL9sRb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "e64d4741-c205-41c4-92a2-1332ae907492"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "average_train_loss=[np.mean([x[i] for x in train_history_loss]) for i in range(epoch)]\n",
        "average_val_loss=[np.mean([x[i] for x in val_history_loss]) for i in range(epoch)]\n",
        "\n",
        "average_train_acc=[np.mean([x[i] for x in train_history]) for i in range(epoch)]\n",
        "average_val_acc=[np.mean([x[i] for x in val_history]) for i in range(epoch)]\n",
        "\n",
        "average_train_acc\n",
        "average_val_acc"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.31270833387970925,\n",
              " 0.3868749998509884,\n",
              " 0.4564583320915699,\n",
              " 0.5047916686534881,\n",
              " 0.5475000008940697,\n",
              " 0.5812500029802322,\n",
              " 0.6120833343267441,\n",
              " 0.6320833334326744,\n",
              " 0.6541666665673256,\n",
              " 0.6616666653752327,\n",
              " 0.6758333319425582,\n",
              " 0.6856249958276749,\n",
              " 0.6958333307504654,\n",
              " 0.7024999961256981,\n",
              " 0.7075000023841858,\n",
              " 0.7177083343267441,\n",
              " 0.7235416665673255,\n",
              " 0.728125,\n",
              " 0.7258333331346511,\n",
              " 0.7356249994039535,\n",
              " 0.7360416680574418,\n",
              " 0.736666664481163,\n",
              " 0.7410416683554649,\n",
              " 0.7447916662693024,\n",
              " 0.742708330154419,\n",
              " 0.7481249964237213,\n",
              " 0.7489583340287208,\n",
              " 0.7514583331346512,\n",
              " 0.7539583346247674,\n",
              " 0.7510416662693024]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "_FanG3DB4HOu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "fa7fd007-a9bc-4f1f-dd5c-803cd76e54ba"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "epochs=range(0,epoch)\n",
        "plt.plot(epochs,average_train_loss,'bo', label=\"Train Loss\")\n",
        "plt.plot(epochs,average_val_loss,'b',label=\"Validation Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "uNLGZ5sm9t7Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "a5536aa5-49ac-4e63-810b-0c148178ec39"
      },
      "cell_type": "code",
      "source": [
        "##LOSS FOR THE LAST FOLD IN KFOLD CV\n",
        "import matplotlib.pyplot as plt\n",
        "history_dict=history.history\n",
        "epochs=range(0,epoch)\n",
        "plt.plot(epochs,average_train_acc,'bo', label=\"Train Acc\")\n",
        "plt.plot(epochs,average_val_acc,'b',label=\"Validation Acc\")\n",
        "plt.title(\"Training and Validation Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Acc\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "DafYH8MF9xib",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "2420fcdf-0667-463d-d4a6-071cf8ae18d9"
      },
      "cell_type": "code",
      "source": [
        "###ACCURACY FOR THE LAST FOLD IN KFOLD CV\n",
        "\n",
        "acc_vlues=train_acc\n",
        "val_acc_values=val_acc\n",
        "plt.plot(epochs,train_acc,'bo', label=\"Train Acc\")\n",
        "plt.plot(epochs,val_acc_values,'b',label=\"Validation Acc\")\n",
        "plt.title(\"Training and Validation Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "gzRSmm7Y9_Rj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1054
        },
        "outputId": "ced78f1e-4feb-45a2-d04f-65e71390bc2f"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\"\"\"del model\n",
        "model=load_model(\"trained_model.h5\")\n",
        "\"\"\"\n",
        "model=keras_model()\n",
        "\n",
        "\n",
        "#model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "history=model.fit(dset_cv,one_hot_labels_cv,epochs=epoch,batch_size=512,validation_data=[dset_test,one_hot_labels_test])\n",
        "\n",
        "  \n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4804 samples, validate on 534 samples\n",
            "Epoch 1/30\n",
            "4804/4804 [==============================] - 26s 5ms/step - loss: 2.9185 - acc: 0.2140 - val_loss: 2.4352 - val_acc: 0.3652\n",
            "Epoch 2/30\n",
            "4804/4804 [==============================] - 0s 24us/step - loss: 2.4113 - acc: 0.3381 - val_loss: 2.0657 - val_acc: 0.4363\n",
            "Epoch 3/30\n",
            "4804/4804 [==============================] - 0s 25us/step - loss: 2.1197 - acc: 0.4074 - val_loss: 1.8380 - val_acc: 0.5075\n",
            "Epoch 4/30\n",
            "4804/4804 [==============================] - 0s 27us/step - loss: 1.8959 - acc: 0.4744 - val_loss: 1.6380 - val_acc: 0.5375\n",
            "Epoch 5/30\n",
            "4804/4804 [==============================] - 0s 25us/step - loss: 1.7187 - acc: 0.5237 - val_loss: 1.4730 - val_acc: 0.6124\n",
            "Epoch 6/30\n",
            "4804/4804 [==============================] - 0s 25us/step - loss: 1.5751 - acc: 0.5729 - val_loss: 1.3872 - val_acc: 0.6273\n",
            "Epoch 7/30\n",
            "4804/4804 [==============================] - 0s 24us/step - loss: 1.4532 - acc: 0.6003 - val_loss: 1.2833 - val_acc: 0.6648\n",
            "Epoch 8/30\n",
            "4804/4804 [==============================] - 0s 24us/step - loss: 1.3817 - acc: 0.6195 - val_loss: 1.2513 - val_acc: 0.6592\n",
            "Epoch 9/30\n",
            "4804/4804 [==============================] - 0s 24us/step - loss: 1.2994 - acc: 0.6384 - val_loss: 1.2168 - val_acc: 0.6854\n",
            "Epoch 10/30\n",
            "4804/4804 [==============================] - 0s 25us/step - loss: 1.2315 - acc: 0.6653 - val_loss: 1.2001 - val_acc: 0.6910\n",
            "Epoch 11/30\n",
            "4804/4804 [==============================] - 0s 25us/step - loss: 1.1783 - acc: 0.6774 - val_loss: 1.1574 - val_acc: 0.7097\n",
            "Epoch 12/30\n",
            "4804/4804 [==============================] - 0s 26us/step - loss: 1.1245 - acc: 0.7030 - val_loss: 1.0962 - val_acc: 0.7210\n",
            "Epoch 13/30\n",
            "4804/4804 [==============================] - 0s 25us/step - loss: 1.0702 - acc: 0.7150 - val_loss: 1.1131 - val_acc: 0.7079\n",
            "Epoch 14/30\n",
            "4804/4804 [==============================] - 0s 25us/step - loss: 1.0325 - acc: 0.7198 - val_loss: 1.0667 - val_acc: 0.7453\n",
            "Epoch 15/30\n",
            "4804/4804 [==============================] - 0s 24us/step - loss: 0.9884 - acc: 0.7373 - val_loss: 1.0521 - val_acc: 0.7434\n",
            "Epoch 16/30\n",
            "4804/4804 [==============================] - 0s 26us/step - loss: 0.9561 - acc: 0.7419 - val_loss: 1.1260 - val_acc: 0.7135\n",
            "Epoch 17/30\n",
            "4804/4804 [==============================] - 0s 25us/step - loss: 0.9458 - acc: 0.7419 - val_loss: 1.0438 - val_acc: 0.7491\n",
            "Epoch 18/30\n",
            "4804/4804 [==============================] - 0s 25us/step - loss: 0.9095 - acc: 0.7598 - val_loss: 1.0601 - val_acc: 0.7378\n",
            "Epoch 19/30\n",
            "4804/4804 [==============================] - 0s 25us/step - loss: 0.8761 - acc: 0.7631 - val_loss: 1.0247 - val_acc: 0.7453\n",
            "Epoch 20/30\n",
            "4804/4804 [==============================] - 0s 25us/step - loss: 0.8638 - acc: 0.7694 - val_loss: 1.0430 - val_acc: 0.7322\n",
            "Epoch 21/30\n",
            "4804/4804 [==============================] - 0s 27us/step - loss: 0.8662 - acc: 0.7681 - val_loss: 1.0539 - val_acc: 0.7341\n",
            "Epoch 22/30\n",
            "4804/4804 [==============================] - 0s 25us/step - loss: 0.8502 - acc: 0.7764 - val_loss: 1.0446 - val_acc: 0.7509\n",
            "Epoch 23/30\n",
            "4804/4804 [==============================] - 0s 25us/step - loss: 0.8232 - acc: 0.7756 - val_loss: 1.0542 - val_acc: 0.7453\n",
            "Epoch 24/30\n",
            "4804/4804 [==============================] - 0s 25us/step - loss: 0.8048 - acc: 0.7881 - val_loss: 1.0258 - val_acc: 0.7453\n",
            "Epoch 25/30\n",
            "4804/4804 [==============================] - 0s 25us/step - loss: 0.7759 - acc: 0.7966 - val_loss: 1.0339 - val_acc: 0.7491\n",
            "Epoch 26/30\n",
            "4804/4804 [==============================] - 0s 25us/step - loss: 0.7621 - acc: 0.7935 - val_loss: 0.9989 - val_acc: 0.7603\n",
            "Epoch 27/30\n",
            "4804/4804 [==============================] - 0s 24us/step - loss: 0.7387 - acc: 0.8091 - val_loss: 1.0029 - val_acc: 0.7753\n",
            "Epoch 28/30\n",
            "4804/4804 [==============================] - 0s 24us/step - loss: 0.7234 - acc: 0.8087 - val_loss: 1.0127 - val_acc: 0.7566\n",
            "Epoch 29/30\n",
            "4804/4804 [==============================] - 0s 26us/step - loss: 0.7077 - acc: 0.8120 - val_loss: 1.0161 - val_acc: 0.7603\n",
            "Epoch 30/30\n",
            "4804/4804 [==============================] - 0s 26us/step - loss: 0.7000 - acc: 0.8179 - val_loss: 1.0613 - val_acc: 0.7566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MD3tAE25-oB-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "e2e06207-b367-4e92-e8f3-f1a2867353f6"
      },
      "cell_type": "code",
      "source": [
        "model.predict(dset_test)\n",
        "history_keys=history.history\n",
        "history_keys['val_acc']\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.36516853966070023,\n",
              " 0.43632958790336207,\n",
              " 0.5074906365925007,\n",
              " 0.5374531824044074,\n",
              " 0.6123595514547512,\n",
              " 0.6273408230770839,\n",
              " 0.6647940072673983,\n",
              " 0.6591760297393084,\n",
              " 0.6853932582037279,\n",
              " 0.6910112357318178,\n",
              " 0.7097378268224023,\n",
              " 0.7209737825482972,\n",
              " 0.7078651694322793,\n",
              " 0.7453183518366867,\n",
              " 0.7434456926606567,\n",
              " 0.7134831458441774,\n",
              " 0.7490636695190316,\n",
              " 0.7378277144628518,\n",
              " 0.7453183518366867,\n",
              " 0.7322097387206689,\n",
              " 0.7340823961107918,\n",
              " 0.7509363304809684,\n",
              " 0.7453183511669716,\n",
              " 0.7453183511669716,\n",
              " 0.7490636695190316,\n",
              " 0.7602996245752113,\n",
              " 0.775280897983451,\n",
              " 0.7565543068928665,\n",
              " 0.7602996245752113,\n",
              " 0.7565543062231513]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "OsuzDjIKROdQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "338b856f-2800-4b0d-a700-4f7c2e000c9c"
      },
      "cell_type": "code",
      "source": [
        "df[0:10]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Features</th>\n",
              "      <th>Labels</th>\n",
              "      <th>lookup</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>873</th>\n",
              "      <td>1009N4H -  10000,STDIN,80ETH,ZT40,L9</td>\n",
              "      <td>NAS : Non-Air Standard</td>\n",
              "      <td>793.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3383</th>\n",
              "      <td>000000000045697214 -  4500,PERFORM65,65B60BH,Z...</td>\n",
              "      <td>FLO : Flowabilty</td>\n",
              "      <td>440.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1161</th>\n",
              "      <td>4085CETE -  4000,FTB FILL,65BETN,Z,3</td>\n",
              "      <td>FLO : Flowabilty</td>\n",
              "      <td>440.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>984</th>\n",
              "      <td>2451. -  3000,B4.0,6050M,ZC20G</td>\n",
              "      <td>AS. : Air Standard</td>\n",
              "      <td>1110.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3373</th>\n",
              "      <td>000000000045687702 -  4500,PERFORM63,40BET,ZS35,L</td>\n",
              "      <td>NAS : Non-Air Standard</td>\n",
              "      <td>793.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1613</th>\n",
              "      <td>9583. -  4500,B6.5,7045BH,ZF20L</td>\n",
              "      <td>FLO : Flowabilty</td>\n",
              "      <td>440.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5082</th>\n",
              "      <td>SCCHPC-1 -  4000,SCCHPC-1,S6CAEBH,ZC20,G9</td>\n",
              "      <td>AGI : Agilia</td>\n",
              "      <td>42.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>557</th>\n",
              "      <td>61N25IN -  B6.0,STDIN,40ETL,EF25</td>\n",
              "      <td>CC. : Cement Content</td>\n",
              "      <td>216.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>3A32-2 -  3900,3A32-2,3065BL,EF15</td>\n",
              "      <td>SA. : Strength Authority</td>\n",
              "      <td>844.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3934</th>\n",
              "      <td>HE3COF -  4000,FASTSET,2065BL,E</td>\n",
              "      <td>CC. : Cement Content</td>\n",
              "      <td>216.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Features  \\\n",
              "873                1009N4H -  10000,STDIN,80ETH,ZT40,L9   \n",
              "3383  000000000045697214 -  4500,PERFORM65,65B60BH,Z...   \n",
              "1161               4085CETE -  4000,FTB FILL,65BETN,Z,3   \n",
              "984                      2451. -  3000,B4.0,6050M,ZC20G   \n",
              "3373  000000000045687702 -  4500,PERFORM63,40BET,ZS35,L   \n",
              "1613                    9583. -  4500,B6.5,7045BH,ZF20L   \n",
              "5082          SCCHPC-1 -  4000,SCCHPC-1,S6CAEBH,ZC20,G9   \n",
              "557                    61N25IN -  B6.0,STDIN,40ETL,EF25   \n",
              "40                    3A32-2 -  3900,3A32-2,3065BL,EF15   \n",
              "3934                    HE3COF -  4000,FASTSET,2065BL,E   \n",
              "\n",
              "                        Labels  lookup  \n",
              "873     NAS : Non-Air Standard   793.0  \n",
              "3383          FLO : Flowabilty   440.0  \n",
              "1161          FLO : Flowabilty   440.0  \n",
              "984         AS. : Air Standard  1110.0  \n",
              "3373    NAS : Non-Air Standard   793.0  \n",
              "1613          FLO : Flowabilty   440.0  \n",
              "5082              AGI : Agilia    42.0  \n",
              "557       CC. : Cement Content   216.0  \n",
              "40    SA. : Strength Authority   844.0  \n",
              "3934      CC. : Cement Content   216.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "2j_1F7C3-osU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dset_check=conv2inputs([\"430602. - 4000,LIGHT WTI,50B30BA,5F10\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pU27pRIL-s4C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prediction=model.predict(dset_test) ##Get the numerical value of all test examples in a matrix (n x labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d9NKLU40fLXw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred=np.argmax(prediction,axis=1) ###get the index corresponding to the highest value of each prediction (n x 1)\n",
        "\n",
        "#indices = [i for i,v in enumerate(pred) if pred[i]==label2index[list(labels_test)[i]]]\n",
        "indices=[i for i,v in enumerate(pred)] \n",
        "predicted_labels=pd.Series([index2label[pred[index]] for index in indices ]) ##output the labels from the pred index\n",
        "subset_of_wrongly_predicted_labels = pd.Series([list(labels_test)[i] for i in indices ]) ##actual labels from test dataset\n",
        "subset_of_wrongly_features = pd.Series([test[i] for i in indices ]) ##features from the test dset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zCCHFG1nfOd5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "a=pd.concat([subset_of_wrongly_features,subset_of_wrongly_predicted_labels, predicted_labels],axis=1) #concat feature, actuals, prediction\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "onhgyzx1fPpM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"predictedvsactual_all.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8pKCbIFIfwH8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9095
        },
        "outputId": "5c76a536-f1cd-4a0f-84dc-77d7a364db3d"
      },
      "cell_type": "code",
      "source": [
        "test"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['000000000030600314 -  3000,PERFORM56,65B60BH,ZS50,G',\n",
              " 'ALDI-1 -  3000,SALESFLOOR,50ETL,E',\n",
              " '3R57C-HE2 -  4000,3R57C-HE2,20AEBN,Z,G9',\n",
              " '000000000035657702 -  3500,PERFORM55,40ETL,ZS25L',\n",
              " '3F32-F3 -  4500,3F32-F3,3065L,EF15',\n",
              " '6BAGFNA -  3000,FINEGROUT,65BETL,ZF15',\n",
              " '6579. -  3500,B5.5,4045BL,ZF20L1',\n",
              " '6420023. -  4000,WC45,4060BN,Z',\n",
              " '4586. -  3500,B5.75,60M,ZF15L',\n",
              " '304530MR -  3000,REG,5060BM,ZF20',\n",
              " '271. -  4500,B5.5,4060N,ZC20L',\n",
              " '5256140. -  2500,REG,4060BL,ZF20',\n",
              " '409304. -  4000,REG,40ETL,2F15,9',\n",
              " 'RMXEUS4012 -  3000,AGILIA AUGER CAST,T1ETH,Z',\n",
              " 'SLP3NH2 -  4000,C-SERIES SLAB,50ETM,ZC15,G9',\n",
              " '1450100. -  4500,SHOTCRETE,5040N,Z',\n",
              " '454N1H-P -  4500,STDIN,60ETM,ZC10,G9',\n",
              " '100NHB1 -  5000,PUMP,70ETM,ZC20,G3B1',\n",
              " '9980MG -  4000,TDOT P(H),4545BL,ZF25,L1U',\n",
              " '408309SC -  4000,SCC,S760BH,ZF30,V',\n",
              " '704338. -  7000,PERFORM,6050H,ZF30',\n",
              " 'MD3825SF -  3500,S3W-N50-12-67 SF',\n",
              " 'GREY1 -  4500,TOPPING,60ETH,EF25',\n",
              " 'MN92 -  4000,SHOTCRETE,20AEBM,ZC15,G3',\n",
              " '4568F712 -  4500,PERFORM65,4060BL,ZF15,L',\n",
              " 'RMX18 -  3000,SPECMAT,4050L,ZC1,B1',\n",
              " '000000000050652314 -  5000,PERFORM64,65B60BH,ZF20,G',\n",
              " '9986SA -  4000,S TXDOT,4045BL,ZF25,LU',\n",
              " '1352. -  3000,B5.25,40L,Z',\n",
              " 'OXBOW1 -  4000,CART-PATH,4060L,EF25',\n",
              " 'GFPCG-29 -  B6.0,GFPAVECG,2060L,EF29',\n",
              " '000000000045677314 -  4500,PERFORM62,65B60BH,ZS25,G',\n",
              " '8522. -  3000,B5.0,40L,ZF30L',\n",
              " '000000000035600512 -  3500,PERFORM56,4060BL,ZS50,L3',\n",
              " '8BAGFNA -  8BAG,FINEGROUT,7BET,ZF15',\n",
              " 'RMXUX4038 -  4000,ULTRA EXPOSED,5060M,2T2,G3',\n",
              " '9759. -  B6.0 ,REG RC,7045BH,ZF20G3',\n",
              " 'TR67NH-P -  4000,TR67NH-P,70ETBM,ZC20,G9',\n",
              " '408739SP -  4000,ARTEVIA EX,3060BL,Z,A3',\n",
              " '000000000040453142 -  4000,REG,3560BL,ZF30',\n",
              " '5006. -  WCR65,B4.0,40L,ZF20L',\n",
              " 'RMX92RS -  4000,SPECMAT ,60AEM,ZC2,B1',\n",
              " '4065TRAP -  4000,SPECMAT,65BETH,ZF20,T',\n",
              " '000000000040600312 -  4000,PERFORM56,4060BL,ZS50G',\n",
              " 'SUPER3IN -  4000,SUPER3IN,50ETM,ZF30',\n",
              " '408309. -  4000,REG,3050BL,2F20,3',\n",
              " '3A41 -  3900,3A41,4070BL,EF,1',\n",
              " 'FTPSNH251 -  3500,C-SERIES,40ETM,ZS25,L9',\n",
              " '3705959. -  7000,SCC,S3ETH,ZF15,9RV',\n",
              " '40A25FT -  4000,FOOTING,4065L,EF25',\n",
              " '000000000040453730 -  4000,EMERALD HS,5060BM,2F30',\n",
              " 'STMP3500 -  3500,STAMPED,4065L,EF',\n",
              " '404500KM -  4000, W REPELENT,5060BM,Z,W',\n",
              " 'RMXEUS267H -  HYDROMEDIA,H,Z',\n",
              " 'LB3YAHP2 -  4300,LB3YAHP2,50AEBM,ZF30,M9',\n",
              " '3284. -  B5.5 ,REG RC,40L,ZGX3',\n",
              " 'RMXCYPAN -  4000,MASS,65ETA,ZS6',\n",
              " 'RMXDRYSHOTA -  50,SHOTCRETE,ETN,2F1',\n",
              " 'OPATRIL5 -  5000,MNFOOT-O5,40ETL,EF',\n",
              " '000000000035950530 -  3500,EMERALD HS,5050BM,2F30,9',\n",
              " 'MS2-COLD -  B5.0,MS2-COLD,40ETL,E',\n",
              " '640242WG -  4000,REG,50B65BA,WF5R',\n",
              " '9915OPT -  2000,TXDOTB OPT,45ETL,ZF25,B12',\n",
              " 'COFSW-C7 -  4000,COFSW-C7,4065BL,EF20',\n",
              " '000000000030631712 -  3000,PERFORM59,4060BL,Z,L',\n",
              " '5000001. -  B1.0,CLSM,70AEN,Z',\n",
              " '4065F712 -  4000,PERFORM61,4060BL,ZF15,L',\n",
              " 'APLEX -  2500,APLEX,10AEBM,ZC20,G9',\n",
              " '1950. -  5000,B7.25,40L,Z',\n",
              " '5070753C -  5000,PERFORM65,65B60BH,ZS35L3',\n",
              " '000000000025507212 -  2500,PERFORM47,40B60B,ZS35,G3',\n",
              " '604538C3 -  6000,CIH,65B60BH,ZF25,D3',\n",
              " '504137C3 -  5000,CIH,5060BM,Z,D3',\n",
              " '1A43-F -  3900,1A43-F,30ETN,ZC15,G9',\n",
              " 'RMX151 -  4000,SPECMAT,4050L,ZC2,B1',\n",
              " 'WALMART1 -  4500,WALMART1,4060L,EF25',\n",
              " '304309. -  3000,REG,4050BL,2F20,9',\n",
              " '5250020. -  2500,REG,4060BN,Z',\n",
              " '409011. -  4000,REG,50ETM,2,9',\n",
              " 'RMXUS4017 -  4000,ULTRA STAMP,4060M,1T2',\n",
              " 'SC90NH -  4000,SCC,70ETM,ZC15,G3',\n",
              " 'RMXEUS9078M -  4500,S6W-N25-12-77 SF,1065L,Z',\n",
              " 'MDGROT51 -  SG-N25-6-2',\n",
              " 'MIX5050PS -  MIXER 50 P ROCK 50 SAND PER CYD BATCHED',\n",
              " 'RMXEUS8551 -  4000,4000 NA .50',\n",
              " '000000000060457740 -  6000,MASS,45B50BM,ZF40,U',\n",
              " '100NH -  5000,PUMP,70ETM,ZC20,G3',\n",
              " '354106. -  3500,REG,30ETL,2',\n",
              " 'RMXEUS125 -  4000,PERFORM 01,40ETL,Z',\n",
              " '404400MR -  4000,REG,5060BM,2',\n",
              " 'RMXEUS6 -  3000,PERFORM 01,40ETL,Z',\n",
              " '4568F702 -  4500,PERFORM66,40BETL,ZF15,L',\n",
              " '50A30EX -  5000,STDEX,4060L,EF30',\n",
              " '6502GRO -  B6.0,GROUT,4060BN,Z',\n",
              " '45NXXIN -  4500,STDIN,40ETL,E',\n",
              " '4509R -  B4.5,3000,6045BM,ZF15,L',\n",
              " '3A22-2 -  3900,3A22-2,1065BL,EF15',\n",
              " '000000000040230725 -  4000,LIGHT WTS,5050BM,ZF25,W3X',\n",
              " 'SCC4500S -  4500,SCC,S5B60BH,ZS50L',\n",
              " '353A1H -  3500,STDEX,40AEBM,ZC15,G9',\n",
              " 'MC68M235 -  MC17-099-S2W-N35-21-25',\n",
              " 'MD66735 -  W6W-N35-99-4,35B',\n",
              " '2045083T -  2000,TREMIE,75B40BH,ZF30,U',\n",
              " '000000000035551302 -  3500,PERFORM50,40ETL,ZG',\n",
              " 'FTPSNH252 -  3500,C-SERIES,40ETM,ZS25,G9',\n",
              " '000000000050453730 -  5000,REG,5060BM,ZF30',\n",
              " 'WEA35AE -  3500,WEATHER MIX AE',\n",
              " 'NHQC404F -  4000,NHDOT,60C70CH,ZF30,F',\n",
              " '9906. -  3000,TXDOT A,4045BN,ZF25LU',\n",
              " 'RMXEUS73 -  3000,PERFORM 01,6060A,Z',\n",
              " 'RMX108 -  4000,PEA,40ETL,Z',\n",
              " '3340. -  4000,B6.5,40L,ZG3',\n",
              " 'BWTP1 -  5000,FT-BWTP1,40ETL,EF25',\n",
              " '000000000040657712 -  4000,PERFORM61,4060BL,ZS25,L',\n",
              " 'TSC1 -  4000,TSC1,55ETL,E',\n",
              " '504999. -  5000,STRIP48,55B50BM,2,X',\n",
              " '4520. -  3000,B5.0,40L,ZF15L',\n",
              " '6500220R -  5000,PAVING,4060CH,Z,R',\n",
              " '404A1H -  4000,STDEX,40AEBL,ZC10,G9',\n",
              " '66N15IN -  B6.5,STDIN,40ETL,EF15',\n",
              " '3060LW92 -  3000,LIGHT WTS,40B60B,ZS35,W2',\n",
              " 'COFSW-C2 -  4000,COFSW-C2,4065BL,EF20',\n",
              " '000000000040657214 -  4000,PERFORM61,65B60BH,ZS25,G3',\n",
              " '9935. -  3600,TXDOT C(DS),40N,ZF25LU',\n",
              " 'PINPLUS25F -  4000,C-SERIES SLAB,50ETM,ZS25,G7F',\n",
              " '1375. -  3500,B5.25,4045BL,Z',\n",
              " 'TYPEB-GM1 -  5000,TYPEB-GM1,30AEBM,ZC15,M1',\n",
              " '000000000050836030 -  5000,TREMIE,75B40BH,ZF30,3U',\n",
              " '9458126. -  4500,CDOTDHAND,35B65BA,ZF30',\n",
              " '2008850. -  C120,CLSM,90A6N,ZF50,3',\n",
              " '9948CF2X -  B4.0,WD 9948CF2X,4060M,ZC30,BL8',\n",
              " 'RMXEUS9227M -  500,CLSM-B-41-4,9030L,Z',\n",
              " '000000000045670704 -  4500,PERFORM62,65BETH,ZS50,L',\n",
              " '9020G-LAF -  B6.0,9020G-LAF,4060BN,Z',\n",
              " '454530HR -  4500,REG,65B60BH,ZF20',\n",
              " '3F52-GF1 -  4500,3F52-GF1,4065L,EF15',\n",
              " 'H4641 -  4500,PERFORM,40B60B,ZF20,L',\n",
              " '40N25FT -  4000,FOOTING,40ETL,EF25',\n",
              " '6411101. -  4000,WC54,40ETL,Z',\n",
              " 'CF-1 -  150,CD FILL,S0CAEBN,Z,Z',\n",
              " '2042F312 -  2000,PERFORM45,40B60BL,ZF15G',\n",
              " '6474120. -  4500,REG,5060BL,ZF15',\n",
              " 'GOETTLE3 -  B11.0,TESTPILE,90ETL,E',\n",
              " 'H3-R -  4000,H3-R,40AEBL,Z,G9',\n",
              " 'N403N1H-F -  4000,WC45,40ETM,ZC15,G9F',\n",
              " '000000000035602512 -  3500PERFORM56,4060BL,ZF20,L3',\n",
              " '5475. -  4000,B6.25,4045BL,ZF20L',\n",
              " '3216. -  4000,B6.0,40L,ZG',\n",
              " '1206000. -  B8.5 ,GROUT,7060N,ZF20,0',\n",
              " '50A20FW -  5000,STDEX,4065L,EF20',\n",
              " 'MD86550 -  S8W-N50-28-3',\n",
              " '3A32-1 -  3900,3A32-1,3065BL,EF15',\n",
              " '000000000025451302 -  2500,PERFORM45,40BETL,ZG',\n",
              " '3304120. -  3000,PEA,8060BL,ZF15,3',\n",
              " 'DC45PTRA -  4500,DCABP,45B60BA,ES50L3',\n",
              " 'AE3MACH -  B6.0,AE3,1060L,EF',\n",
              " '51A25EX -  B5.0,STDEX,4060L.EF25',\n",
              " 'RMXWM54007A -  4000,PERFORM 01,5060M,1',\n",
              " '000000000040701512 -  4000,PERFORM65,40B6B,Z,L3',\n",
              " '9918SA -  3600,C TXDOT,4045BL,ZF25,LU',\n",
              " '235575WC -  3500,EMERALDHR,50BETA,1V',\n",
              " 'WLP3AH2 -  4500,C-SERIES,50AEBM,ZC15,G9',\n",
              " '9208. -  3500,B5.5,70H,ZL11',\n",
              " '353N1H -  3500,STDIN,40ETM,ZC15,G9',\n",
              " '35A35EX -  3500,EMERALD HS,4060L,EF35',\n",
              " 'RMXEUS9039M -  3500,S3W-N25-7-60 SF,1065L,Z',\n",
              " '3773. -  B5.0 ,FLOWGROUT,A1N,ZZ',\n",
              " '2191. -  4000,B5.0,40ETM,ZC20G',\n",
              " '650072BG -  5000,PICASSO EX,5065BH,Z,R',\n",
              " 'RMX54 -  4000,PERFORM 01,4050L,ZC2,B1',\n",
              " '6357141. -  3500,REGCOARSE,40ETL,ZF20',\n",
              " 'RMX31A -  3500,SPECMAT,4050L,ZF1,G1',\n",
              " '3A22 -  3900,3A22,2065BL,E',\n",
              " '000000000040454915 -  4000,REG,3040L,ZF15',\n",
              " 'AGH80A -  8000,AGILIA H,S8C45BH,ZF35,G3,R',\n",
              " '6426100. -  4000,REG,4060BL,ZF20',\n",
              " 'RMXEUS320 -  4000,LIGHTWEIGHT,65B65BH,ZS35,WD3',\n",
              " 'RMX530072D -  3000,NYSDOT CLASS D,3075L,2',\n",
              " 'RMX140575 -  4000,PERFORM 01,30ETL,2S2',\n",
              " 'RMX70 -  3000,LIGHT WTS ,8060M,ZC2,W3',\n",
              " 'CB3Y33AHP -  4300,CB3Y33AHP,50AEBM,ZF30,M9',\n",
              " 'RMX64 -  3000,PUMP,4050L,ZT4,B1',\n",
              " 'RMXEUS9015M -  3000,N2W-N35-58-16,3565L,Z',\n",
              " 'MC3A21-2 -  3900,MC3A21-2,20AEBM,ZC30,G7',\n",
              " '3B57CF-2 -  4000,3B57CF-2,5065L,EF15',\n",
              " '5602. -  5000,B7.0,40L,ZF20L',\n",
              " '353A2H -  3500,STDEX,90AEBM,ZC15,G3',\n",
              " '3S52-W1 -  4500,3S52-W1,4065L,EF15',\n",
              " 'RMX2 -  3000,SPECMAT,A0ETL,Z',\n",
              " '000000000030550312 -  3000,PERFORM50,4060BL,ZS50G',\n",
              " 'RMX39 -  4500,SPECMAT,40ETL,Z',\n",
              " '403X2HT-F -  4000,HALF-AIR TOPPING,40AEBM,ZC15,G3F',\n",
              " '6075751S -  6000,STRIP72,65B60BH,ZS25L3',\n",
              " '11. -  B21.0,FLOWGROUT,S0CETN,Z,',\n",
              " 'M403N1H -  4000,WC44,40ETH,ZC15,G9',\n",
              " 'RMXCYPA2PMP -  4000,REG RC,6550A,ZS4',\n",
              " '7345. -  B7.0,SHOTCRETE,2045BM,ZF20,G3',\n",
              " '40A25EP -  4000,PICASSO EX,4060L,EF25,G4',\n",
              " '7341. -  WCR40,SHOTCRETE,6045BM,ZF20G3',\n",
              " '603A4H -  6000,STDEX,40AEBM,ZC15,L9',\n",
              " 'RMXEUS9029M -  3500,S3W-N25-7-58,3565L,Z',\n",
              " '3S12-M1 -  4500,3S12-M1,1065L,EF15',\n",
              " '2050F712 -  2000,PERFORM47,460BL,ZF15,L',\n",
              " '000000000035600314 -  3500,PERFORM56,65B60BH,ZS50,G',\n",
              " '404530CR -  4000,CIH4060BMZF25,D3',\n",
              " '3A42A- -  3900,3A42A-,30AEBN,Z,M9',\n",
              " '2400120. -  4000,PEA,3060BL,Z,3',\n",
              " '3A22-3HHE -  4000,HEACURB,10AEBM,ZC20,G9',\n",
              " '5069773C -  5000,PERFORM61,65B60BH,ZS35,LD3',\n",
              " '3F52-FF -  4500,3F52-FF,4065L,EF25',\n",
              " '7456120L -  4500,REG,4040L,ZF20',\n",
              " '304417. -  3000,PERFORM,3040L,ZF20',\n",
              " 'WTHR-1H -  4000,81-85WEATHER,5060L,EF',\n",
              " 'WFMIX1 -  4000,WFMIX1,50ETM,ZS40,B7',\n",
              " '35AXXFW -  3500,STDEX,4060M,E',\n",
              " '7456120B -  4500,ARTEVIA BB,4065B,ZF20',\n",
              " '4569753C -  4500,PERFORM65,65B60BH,ZS35,L3D3',\n",
              " 'NH308TAW -  3000,NHDOT,65B55CH,ZF30,3AW',\n",
              " 'NC8 -  B10.5,CELLULAR,A0,B0,N,EF60',\n",
              " '9580. -  4000,WC42,40ETM,ZF,B13',\n",
              " 'DC35DRYA -  3500,DCDRYE,ET,ZS,S',\n",
              " '3050F712 -  3000,PERFORM47,4060BL,ZF15L',\n",
              " '5352. -  4000,B5.75,40L,ZF20L',\n",
              " '9BAGGRNA -  4000,FINEGROUT,65ETL,ZF15',\n",
              " '7095. -  4000,B6.5,4045BL,ZF20L11',\n",
              " '40MODAXX -  4000,MODAIR,4045L,E',\n",
              " '409A1H -  4000,STDEX,40AEBL,ZT40,G9',\n",
              " '000000000030531712 -  3000,PERFORM48,4060BL,ZL',\n",
              " '104309. -  1000,LEAN,40B50BL,2F20,9',\n",
              " '7450708. -  4500,REG,70C45BH,Z,R',\n",
              " 'MD66850 -  S6W-N50-21-30',\n",
              " '6400122. -  4000,SLIPFORM,2060BL,Z',\n",
              " '000000000045453730 -  4500,REG,5060BM,ZF30,9',\n",
              " '50AXXEX -  5000,STDEX,3560L.E',\n",
              " '61AXXMR -  B6.0,STDEX,5065L,E',\n",
              " '5617. -  5000,B7.0,4045BL,ZF20L',\n",
              " 'FTC1NH2 -  3500,C-SERIES,30ETM,Z,G9',\n",
              " 'GFP-15 -  B6.0,GFPAVE,2060L,EF15',\n",
              " '000000000035750212 -  3500,SPECIAL,460BL,ZS25,G3',\n",
              " '2257174. -  2500,GROUT,60C50BL,5F30',\n",
              " '40AXXMR -  3900,BP3A41-1,40AEBM,ZC30,G9',\n",
              " '6309700R -  3000,REG,45CETH,ZF30,R',\n",
              " '6508110. -  5000,MADOT,3070BM,2F30,3U',\n",
              " 'H455A1HF -  4500,WC40,40AEBM,ZC25,G9F',\n",
              " '508103. -  5000,REG,50ETM,Z,3',\n",
              " '9607601. -  6000,REG,65B60BA,5F30',\n",
              " '5400020. -  4000,REG,4060BN,Z',\n",
              " '708338DS -  7000,DRILLSHAFT,80C50CH,ZF20,3X',\n",
              " '404307. -  4000,REG,55B50BM,2F20',\n",
              " '508743. -  5000,REG,70BETH,ZF20,3',\n",
              " '8409. -  B7.5 ,GROUT,80N,ZF25',\n",
              " '7621. -  B6.5,BLEND,6060BM,ZF20,B12',\n",
              " '5075250S -  5000,STRIP72,65BETH,ZF20,L3',\n",
              " 'RMX102SLM -  800,PEA,8050H,ZS4',\n",
              " 'N90NH-1 -  4000,WC45,70ETB,Z,G3',\n",
              " '241. -  3000,B4.3,4060N,ZC15L',\n",
              " '5279. -  3500,B5.5,6045BM,ZF20L',\n",
              " '61A30TP -  B6.0,EMERALD HS,4060L,EF30,3',\n",
              " '8496. -  B7.0 ,FLOWGROUT,A1N,ZF61Z',\n",
              " 'MIX075LB -  MIXER 3/4\" LIMESTONE PER CYD BATCHED',\n",
              " '3060BFHR -  3000,CM GROUT,65B60BH,ZS50G6',\n",
              " '2801. -  5000,B7.0,4045BL,ZL1',\n",
              " 'H4500EXP -  4500,PICASSO EXP,40B60B,Z,G3',\n",
              " 'GABEL1 -  3000,GABEL1,60ETL,E',\n",
              " 'RMX80 -  3500,PEA,40ETL,ZC5',\n",
              " '000000000035600212 -  3500,PERFORM56,4060BL,ZS50,G6',\n",
              " 'RMX245572 -  4500,PERFORM 01,3065M,2',\n",
              " '7986. -  2500,B4.0,40L,ZF25L',\n",
              " '251HA -  3500,B4.4,4030N,ZC20L',\n",
              " 'MBJT40YN1 -  4000,MBJT40YN1,40ETN,ZT40,G9',\n",
              " '6350520. -  3500,B6.0,7060BH,Z',\n",
              " '9458481. -  4500,CDOT D,4065BA,ZF30',\n",
              " '5600. -  B7.0,5000,40ETN,ZF20L',\n",
              " '403A2HT -  4000,TOPPING,40AEBM,ZC15,G3',\n",
              " 'N403N1HB1 -  4000,WC45,60ETM,ZC15,G9B1',\n",
              " '404309. -  4000,REG,3050BL,2F20',\n",
              " '7424740. -  4000,REG A,6060BH,ZF15',\n",
              " 'WHS2 -  4000,WALLS,40ETL,EF25',\n",
              " '3HE52-F -  4500,3HE52-F,40AEBN,Z,G9',\n",
              " '1230. -  3000,B5.0,60M,Z',\n",
              " '5023. -  2500,B4.25,6045BM,ZF20L',\n",
              " '503A7H -  5000,STDEX,50AEBM,ZC15,M9',\n",
              " 'RMX140072 -  4000,PERFORM 01,30ETL,2',\n",
              " 'SUPER2FE -  3000,SUPER2FE,5050BM,ZF30',\n",
              " '6401103. -  4000,REGCOARSE,40ETL,Z',\n",
              " '551A1H -  5500,STDEX,40AEBM,Z,G9',\n",
              " '2148. -  5000,B5.6,50ETM,ZG',\n",
              " '9906OPT -  3000,TXDOTA OPT,4045BL,ZF25,B12',\n",
              " '7587. -  3000,PMP,4045BL,ZF20L12',\n",
              " 'RMX95SLA1 -  4500,SPECMAT,6050L,ZS5,G1',\n",
              " 'RMX140670 -  4000,PERFORM 01,30ETL,2S3',\n",
              " 'TR67NH1FP -  4000,TR67NH1FP,65ETM,Z,G9FU',\n",
              " '453N2HT -  4500,TOPPING,55ETB,ZC15,G3',\n",
              " '1P67CF-2 -  3000,1P67CF-2,60ETL,EF15',\n",
              " '000000000084548862 -  4500,CHRONOLIA24,60CAEB,ZF15,9N',\n",
              " 'N403N2HTF -  4000,WC45,40ETM,ZC15,G3F',\n",
              " 'RMXCYPB2PMP -  4000,REG RC,6550A,ZS4',\n",
              " 'RMX109 -  3000,STRIP24,80ETM,Z',\n",
              " '7552718. -  5500,REG,55B50BH,ZF5,R',\n",
              " '81NXXGT -  B8.0,GROUT,50ETL,E',\n",
              " '000000000030450325 -  3000,REG,60BETM,2F25',\n",
              " 'MD36535 -  S3W-N35-28-9',\n",
              " '408500HR -  4000,REG,65B60BH,Z,3X',\n",
              " 'EMP3A21HE -  3900,EMP3A21HE,20AEBN,Z,G7',\n",
              " '5106. -  3000,B4.75,40L,ZF20L',\n",
              " '604307. -  6000,REG,6050M,2F20',\n",
              " '1350. -  3500,B5.25,40L,Z',\n",
              " 'AAE310FA -  B6.5 ,AAE3,2065BL,EF',\n",
              " '4661. -  5000,B7.5,4045BL,ZF15L',\n",
              " '000000000030557202 -  3000,PERFORM50,40ETL,ZS25G6',\n",
              " '000000000020450212 -  2000,PERFORM42,40B60BL,ZS50G6',\n",
              " 'PCI3A21-2 -  3900,PCI3A21-2,10AEBN,ZC15,G9',\n",
              " 'H35A -  3500,PICASSO CO,40B60B,Z,L',\n",
              " '453A1H-I -  4500,PICASSO C1,40AEBM,ZC15,G9I',\n",
              " 'C3EXNCA -  4500,WEATHERMIX C3NCA,40AEBM,ZC20,G9',\n",
              " 'WASA400H -  4000,WA,4060BH,ES50L',\n",
              " '000000000045652304 -  4500,PERFORM61,65BETH,ZF20,G',\n",
              " '000000000045662702 -  4500,PERFORM62,40ETL,ZF20,L',\n",
              " '6501483S -  5000,REG,40ETA,Z,E',\n",
              " '2300120. -  3000,PEA,7060BL,Z,3',\n",
              " 'RMXEUS7 -  3000,PERFORM 01,4060L,Z',\n",
              " '3052. -  4000,B6.0,40L,ZL11',\n",
              " '000000000035651712 -  3500,PERFORM60,4060BL,Z,L',\n",
              " 'VCFLOOD3 -  B6.0,FLOODWALL,3065L,EF15',\n",
              " '5302120. -  3000,REG,7060BL,ZF15',\n",
              " 'TR67NH1B1 -  4000,TR67NH1B1,70ETBM,Z,G9',\n",
              " '61A25EX -  B6.0,STDEX,4065BL,EF25',\n",
              " 'MD6685SF -  S6W-N50-21-31SF',\n",
              " '1124R -  B4.5,RES,50ETM,Z,L',\n",
              " '8767R -  B4.5,RES,6045BM,ZF40,L',\n",
              " '407A1L25 -  4000,407A1L25,40AEBM,ZS25,G9',\n",
              " '4561. -  3500,B5.5,4045BL,ZF15L',\n",
              " '9605. -  B7.0,BLEND,6040H,ZF20,B12',\n",
              " '5077. -  3000,B4.5,4045BL,ZF20L',\n",
              " 'CF-2 -  150,CD FILL,S0AEBN,Z',\n",
              " '3G52-FF -  4500,3G52-FF,4065L,EF25',\n",
              " '2007602. -  B10.0,RRGROUT,80B50BN,5F25',\n",
              " 'WLP3NH2 -  4000,C-SERIES,50ETM,ZC15,G9',\n",
              " '000000000030507312 -  3000,PERFORM45,4060BL,ZS25G',\n",
              " 'MDPTCH11 -  S9WH-20-1,65B',\n",
              " '7082. -  B6.25,REG RC,40L,ZF20L11',\n",
              " 'RMX510575A -  3000,NYSDOT CLASS A,3065L,2S2,G',\n",
              " '1600. -  4000,B5.75,40L,Z',\n",
              " 'N403N2TF1 -  4000,WC45,45ETM,ZC15,G3F',\n",
              " '2220. -  4000,B6.0,5060M,ZL',\n",
              " '3306120. -  2500,PRPUMP,8060BL,ZF20,3',\n",
              " '8001000. -  B20.7,SLURRY,A0AEN,Z',\n",
              " 'ADM2 -  4500,ADM2,40ETL,EF25',\n",
              " '000000000035450630 -  3500,EMERALD HS,30ETL,2F30',\n",
              " 'RMX150072 -  5000,PERFORM 01,30ETL,2',\n",
              " '308001. -  3000,REG,5060L,2,3',\n",
              " '000000000045657302 -  4500,PERFORM60,40ETL,ZS25,G',\n",
              " '4559. -  B5.5 ,REG,4045BL,ZF15L',\n",
              " '000000000040652514 -  4000,PERFORM61,65B60BH,ZF20,L3',\n",
              " 'M40TN4H -  4000,WC44,40ETL,ZC20,L9',\n",
              " '9808. -  3000,B5.0,70H,ZF30L',\n",
              " '000000000050707812 -  5000,LIGHT WTS,4065BL,ZS35,W',\n",
              " 'RMXEUS229 -  3500,PERFORM 01,4060L,Z',\n",
              " 'ROHRICH5 -  3500,ROHRICH5,5045L,EF',\n",
              " 'FLOOD3 -  3000,CONCRETEFILL,4060L,EF25',\n",
              " '000000000050450730 -  5000,REG,5050BM,ZF30',\n",
              " '9650G -  B5.1,WD 9650G,4060BM,ZC20',\n",
              " '251N1H -  2500,STDIN,40ETM,Z,G9',\n",
              " '40AXXGR -  4000,STDEX,4060L,EFC',\n",
              " '5250. -  3000,B5.5,40L,ZF20L',\n",
              " 'MDPTCH67 -  W9WH-99-1,65B',\n",
              " 'RMXEUS253 -  5000,PEA,4060A,Z',\n",
              " '6506720. -  5000,REG A,5060BH,ZF20',\n",
              " '406Y2714 -  4000,SPECYLD61,65B60BH,ZF20,L',\n",
              " '000000000040650212 -  4000,PERFORM60,4060BL,ZS50,G6',\n",
              " '3Y42A-S -  4000,3Y42A-S,40AEBN,ZC15,M9',\n",
              " 'RMXEUS9069M -  4500,S6W-N25-7-50,3565L,Z',\n",
              " '3500AE -  B5.5,PERFORM 00,40AEN,ZG',\n",
              " 'RMX150 -  4000,SPECMAT,40ETL,ZC2,B1',\n",
              " 'RMX510074D -  3000,NYSDOT CLASS D,3275L,2F2,G2',\n",
              " 'RES40AEG -  4000,RES,4060BL,ZF25G',\n",
              " '3Y43FA -  4300,3Y43,4065BL,EF15',\n",
              " '000000000050697714 -  5000,PERFORM65,65B60BH,ZS25,L',\n",
              " 'RMXEUS5 -  3000,PERFORM 01,6060A,Z',\n",
              " '6HP4041S -  4000,MADOT,6070BM,PS25,SU',\n",
              " 'RMXEUS8 -  3000,PERFORM 01,40ETL,Z',\n",
              " '1925. -  4500,B7.0,6045BM,Z',\n",
              " 'RMXUG4017 -  4000,ULTRA GARAGE,4060M,1T2',\n",
              " '000000000045650702 -  4500,PERFORM61,40ETL,ZS50,L',\n",
              " '404507C2 -  4000,CIH,30B50BL,ZF25,D2',\n",
              " '309500. -  3000,REG,3060BL,2,9',\n",
              " '000000000040850730 -  4000,EMERALD HS,5050BM,2F30,3',\n",
              " 'RMX236572 -  4000,PERFORM 01,3065L,2',\n",
              " 'RMX235672 -  3500,PERFORM 01,3065L,2',\n",
              " 'RMX54S -  4000,FLOWABLE,8050M,ZC2,B1',\n",
              " 'RMXUD4008 -  4000,ULTRA DRIVEWAY,4060M,2F1',\n",
              " 'RMXEUS9012M -  3000,W2W-N35-82-53,3565L,Z',\n",
              " '7603. -  4000,BLEND,6045M,ZF20B12',\n",
              " '3201B -  B4.3,BLEND AGG,40ETM,ZC20G9',\n",
              " '2370020. -  3500,PICASSO SP,4060BN,Z,3',\n",
              " 'HUB25SEG -  2500,HUB SECANT,90ETN,ZF60',\n",
              " '000000000040412864 -  4000,REG,65B60BH,Z',\n",
              " '3S52C-F -  4000,3S52C-F,30AEBN,ZC15,C9',\n",
              " '5406120. -  4000,REG,4060BL,ZF20',\n",
              " 'RMX140572 -  4000,PERFORM 01,30ETL,2',\n",
              " '508108. -  5000,REG,65B50BH,Z,3',\n",
              " '3B57CF-1 -  4000,3B57CF-1,5065L,EF15',\n",
              " 'OPATRIL1 -  4000,EXTERIOR-O1,6060L,EF',\n",
              " 'WLP5NH1 -  4000,C-SERIES,50ETM,ZC25,L9',\n",
              " '000000000040652304 -  4000,PERFORM60,65BETH,ZF20G',\n",
              " 'PARK2 -  5000,PARK2,4060L,EF25',\n",
              " '3S52-F1 -  4500,3S52-F1,4065L,EF15',\n",
              " 'H403N1HPD -  4000,WC40,60ETH,ZC15,G9D',\n",
              " '6420150. -  4000,WC45,4060L,Z',\n",
              " '5070073C -  5000,PERFORM66,65B60BA,ZS50,LDC',\n",
              " 'RMX235074 -  3500,PERFORM 01,3065L,2F1',\n",
              " 'EVOL2 -  5000,EVOL2,80ETM,EF20',\n",
              " 'CHRONO48N -  4000,CHRONOLIA48HR,60ETH,EF',\n",
              " '6306220. -  3000,REG,7060BH,ZF15',\n",
              " '000000000099701912 -  3500,PICASSO EX,4060BL,Z,G6',\n",
              " 'VERONA4 -  4000,INT-VERONA,60ETL,EF25',\n",
              " '6411700. -  4000,REG,60ETH,Z',\n",
              " '9561. -  4000,B6.0,7045BH,ZF20L',\n",
              " 'SUPER2IN -  3500,SUPER2IN,50ETM,ZF30',\n",
              " '000000000040672712 -  4000,PERFORM63,4060BL,ZF20,L',\n",
              " '9704G -  B6.0,WD 9704G,4055BM,Z',\n",
              " '9458121. -  4500,PD 2011160,40B60CL,ZF30,',\n",
              " '50TN1H -  5000,STDIN,40ETM,ZC20,G9',\n",
              " '3F52 -  4500,3F52,4065L,EF25',\n",
              " '230612LA -  3000,PEA,5060BL,EF20,3',\n",
              " '000000000030230030 -  3000,LIGHT WTS,4050BM,ZF30,W3',\n",
              " 'RMXEUS9130M -  3500,NG-WH-58-12,9060H,Z',\n",
              " 'RMX95R -  4200,SPECMAT,4050L,ZC1,G',\n",
              " '9628. -  5000,B8.0,70H,ZF20L',\n",
              " 'N90NH-F -  4000,WC45,70ETB,ZC20,G3F',\n",
              " 'OPATRIL4 -  3500,PUMP-O4,50ETL,EF',\n",
              " '9611. -  5500,B7.5,7045BH,ZF20L',\n",
              " 'EDINA457 -  4500,EDINA457,40AEBL,Z,M9',\n",
              " 'RMXUW3067S -  3000,ULTRA WALL,5060A,2T2,G',\n",
              " 'RMXEUS9047M -  3500,W4W-N25-82-7,60ETL,Z',\n",
              " 'TSC2 -  4000,TSC2,5560L,E',\n",
              " 'SUPGFLOW -  25.4 BAG,SLURRY,10BETN,ZF90,',\n",
              " 'RMXEUS8646 -  5000,PERFORM79,65B60B,ZS20L3D3',\n",
              " '9246G -  5000,B6.0,40ETL,ZC20',\n",
              " '9799. -  B9.0,GROUT,7045BH,ZF20,',\n",
              " 'WEIS2 -  4000,WEIS-EXTERIOR,5060L,EF',\n",
              " '604313. -  6000,REG,50ETM,ZF20,X',\n",
              " '4582. -  3500,B5.75,40L,ZF15L',\n",
              " '404530DR -  4000,REG,ZF30',\n",
              " '8515. -  B9.0,GROUT,7070BL,ZF20',\n",
              " 'RMX123S -  4000,STRIP24,8050M,Z',\n",
              " '304123. -  3000,REG,50ETM,Z',\n",
              " 'RMX53M -  4000,PERFORM 01,80ETL,ZC1,B1',\n",
              " '9408721C -  4000,CDOT BZ AE,7065BH,ZF20',\n",
              " '3F32-R -  4500,3F32-R,3065L,EF25',\n",
              " '404304. -  4000,REG,40ETL,2F15',\n",
              " '000000000025500212 -  2500,PERFORM47,40B60BL,ZS50G6',\n",
              " '000000000030602312 -  3000,PERFORM56,460BL,ZF20,G',\n",
              " '454107. -  4500,REG,5050BM,2',\n",
              " '2506181. -  5000,PEA,3065BL,ZF20,3',\n",
              " '000000000030551202 -  3000,PERFORM50,40ETL,ZG6',\n",
              " '6075F704 -  6000,PERFORM70,65BETH,ZF20,L',\n",
              " 'SC305A2M -  3000,SCC,S3CAEBH,ZC25,G3',\n",
              " '000000000050700714 -  5000,PERFORM66,65B60BH,ZS50,L',\n",
              " '6407703. -  4000,REG,40ETH,ZF20',\n",
              " 'WALLICF4 -  4000,WALL,40ETL,EF',\n",
              " 'RMXEUS267H -  HYDROMEDIA,H,Z',\n",
              " '401A2HT-F -  4000,TOPPING,40AEBM,Z,G3F',\n",
              " '000000000060807714 -  6000,PERFORM75,65B60BH,ZS25,L',\n",
              " '6102. -  B6.0,PICASSO EX,4060BL,ZG3',\n",
              " 'EDINA310 -  3000,EDINA310,40ETL,ZC30,G9',\n",
              " '000000000035230725 -  3500,LIGHT WTS,55B50BM,ZF25,W3X',\n",
              " '7775. -  B6.0 ,GROUT,8045BL,ZF20',\n",
              " 'RMXEUS190 -  5000,SHOTCRETE,3070L,Z',\n",
              " '40A25WL -  4000,STDEX,4060L,EF25',\n",
              " '6503101. -  5000,REG,60ETH,ZF10',\n",
              " 'RMX250676 -  5000,STRIP24,3060L,2F2',\n",
              " '000000000030850530 -  3000,EMERALD HS,5050BM,2F30,3',\n",
              " '4065LW92 -  4000,LIGHTWEIGHT,40B65B,ZS35,W2',\n",
              " 'RMX3ORION -  500,3530L,ZF1',\n",
              " '5363. -  4000,B5.75,4045BL,ZF20L',\n",
              " '3A42FA -  3900,3A42,4065BL,EF15',\n",
              " 'RMXC1000003 -  4000,CIH ,4050H,Z',\n",
              " '6406150L -  4000,WC45,4050BL,ZF20',\n",
              " '45N15FT -  4500,FOOTING,40ETL,EF15',\n",
              " '9408721. -  4000,CDOT BZ AE,7065BH,ZF25',\n",
              " 'RMX44 -  3000,6050M,ZT4,B1',\n",
              " '4402780. -  4000,LIGHT WTS,60BAEH,ZF5,W6',\n",
              " '50N25IN -  5000,STDIN,40ETL,EF25',\n",
              " '000000000040453315 -  4000,REG,65BETH,ZF15,U',\n",
              " '000000000040850930 -  4000,EMERALD HS,3050BL,2F30,3',\n",
              " '000000000050692504 -  5000,PERFORM65,65BETH,ZF20,L3',\n",
              " 'RMXC1000002 -  4000,PEA,4050H,Z',\n",
              " '504107. -  5000,REG,5050BM,2',\n",
              " '7626. -  5000,BLEND,40L,ZF20,B12',\n",
              " 'RMX121S -  5000,STRIP24,80ETM,ZC2,L7',\n",
              " '9581. -  4500,B6.57045BH,ZF20L',\n",
              " 'AGSC902C -  9000,AGILIA,D2',\n",
              " 'TRN453N4H -  4500,TRN453N4H,50ETM,ZC15,L9',\n",
              " '401A7H -  4000,STDEX,40AEBL,Z,M9',\n",
              " '254309NF -  2500,NO FINES,3050BN,Z',\n",
              " '000000000055747504 -  5500,PERFORM70,65BETH,ZS25,L3',\n",
              " '304304. -  3000,REG,4030L,2F209',\n",
              " '453A7H -  4500,STDEX,40AEBM,ZC15,M9',\n",
              " '40TA0H -  4000,STDEX,40AEBL,ZC20,G1',\n",
              " '000000000040652712 -  4000,PERFORM61,4060BL,ZF20,L',\n",
              " '404538C4 -  4000,CIH,5060BM,ZF20,D4',\n",
              " '9906SLU -  B0.0,SLURRY SAND,ETN,',\n",
              " '45A35EX -  4500,STDEX,4060L,EF35',\n",
              " '3Y37-F -  4300,3Y37-F,20AEBN,ZC15,G9',\n",
              " '9935SA -  3600,C TXDOT(DS),65ETL,ZF25,LU',\n",
              " '508146. -  5000,SHRINKAGE,55B60BH,Z,3E',\n",
              " '402120. -  4000,LIGHT WTS,4060BM,Z,W3',\n",
              " '5365. -  B5.75,B5.75,4045BL,ZF20L',\n",
              " 'SCHOOL3 -  4000,WALL,4060L,EF20',\n",
              " 'FTP5NH1 -  3500,C-SERIES,40ETM,ZC25,L9',\n",
              " 'RMX510075DP -  3000,NYSDOT CLASS DP,3575L,PS2,G2',\n",
              " 'HE3COF29 -  4000,FASTSET,3060L,EF29',\n",
              " 'MDGROT82 -  3500,SGW-12-25',\n",
              " 'COFH-20 -  4000,COFH-20,4065BL,EF20',\n",
              " 'RMXUP3507 -  3500,ULTRA PATIO,4060M,2T2',\n",
              " 'RMX1AGVERLH -  8000,AGILIA L HEAT,T1ETH,ZT6,B9',\n",
              " 'RMXEUS28 -  5000,PERFORM 01,65ETA,Z',\n",
              " 'MD65235 -  S6W-N35-33-7,35B',\n",
              " '000000000099751202 -  4000,SHOTCRETE,20ETL,Z,G3',\n",
              " 'RMXEUS8478 -  4000,9BGROUTP',\n",
              " 'RMX54CS -  4000,FLOWABLE,8050M,ZF3,G1',\n",
              " '1031. -  B3.0,B3.0,4045BL,Z',\n",
              " 'RMX250574 -  5000,PERFORM 01,3065M,2F1',\n",
              " 'RMXUD4507 -  4500,ULTRA DRIVEWAY,5065M,2T2',\n",
              " '457N1H40 -  4500,STDIN,40ETM,ZS40,G9',\n",
              " '5074F30S -  5000,STRIP24,65BETA,ZF15G',\n",
              " '8554R -  B4.5,RES,60ETM,ZF35,L',\n",
              " '607A4H20D -  6000,STDEX,60AEBM,ZS20,L9D',\n",
              " '402307. -  4000,LIGHT WTS,55B50BM,2F20,W3X',\n",
              " '5045C4KM -  5000,W REPELENT,65B60BH,ZF25,D4',\n",
              " '304106. -  3000,REG,40ETL,2,9',\n",
              " '3A32-3H -  4000,AWALKS,20AEBM,ZC15,G9',\n",
              " 'RMX906995 -  B6.0 ,FINEGROUT,50ETN,2']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "HKgM5dzK49Ex",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}